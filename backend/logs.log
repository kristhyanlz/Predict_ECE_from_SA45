2024-11-04 11:26:29,431:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:26:29,431:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:26:29,432:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:26:29,432:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:26:29,858:INFO:PyCaret ClassificationExperiment
2024-11-04 11:26:29,858:INFO:Logging name: clf-default-name
2024-11-04 11:26:29,858:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:26:29,858:INFO:version 3.3.2
2024-11-04 11:26:29,858:INFO:Initializing setup()
2024-11-04 11:26:29,858:INFO:self.USI: 9003
2024-11-04 11:26:29,858:INFO:self._variable_keys: {'log_plots_param', 'idx', 'memory', 'X', 'n_jobs_param', 'pipeline', 'seed', 'html_param', 'gpu_n_jobs_param', 'USI', 'exp_id', 'is_multiclass', 'data', 'fold_shuffle_param', 'exp_name_log', 'gpu_param', 'logging_param', '_available_plots', 'X_test', 'y', 'fold_generator', 'target_param', 'y_test', 'fold_groups_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'X_train'}
2024-11-04 11:26:29,858:INFO:Checking environment
2024-11-04 11:26:29,858:INFO:python_version: 3.11.2
2024-11-04 11:26:29,858:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:26:29,859:INFO:machine: x86_64
2024-11-04 11:26:29,860:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:26:29,860:INFO:Memory: svmem(total=16721211392, available=10216644608, percent=38.9, used=6040096768, free=4301643776, active=1762340864, inactive=9641615360, buffers=306909184, cached=6072561664, shared=115830784, slab=524374016)
2024-11-04 11:26:29,860:INFO:Physical Core: 4
2024-11-04 11:26:29,860:INFO:Logical Core: 8
2024-11-04 11:26:29,860:INFO:Checking libraries
2024-11-04 11:26:29,860:INFO:System:
2024-11-04 11:26:29,860:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:26:29,860:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:26:29,861:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:26:29,861:INFO:PyCaret required dependencies:
2024-11-04 11:26:29,875:INFO:                 pip: 23.0.1
2024-11-04 11:26:29,875:INFO:          setuptools: 66.1.1
2024-11-04 11:26:29,875:INFO:             pycaret: 3.3.2
2024-11-04 11:26:29,875:INFO:             IPython: 8.29.0
2024-11-04 11:26:29,875:INFO:          ipywidgets: 8.1.5
2024-11-04 11:26:29,875:INFO:                tqdm: 4.66.6
2024-11-04 11:26:29,875:INFO:               numpy: 1.26.4
2024-11-04 11:26:29,875:INFO:              pandas: 2.1.4
2024-11-04 11:26:29,875:INFO:              jinja2: 3.1.4
2024-11-04 11:26:29,875:INFO:               scipy: 1.11.4
2024-11-04 11:26:29,875:INFO:              joblib: 1.3.2
2024-11-04 11:26:29,875:INFO:             sklearn: 1.4.2
2024-11-04 11:26:29,875:INFO:                pyod: 2.0.2
2024-11-04 11:26:29,875:INFO:            imblearn: 0.12.4
2024-11-04 11:26:29,875:INFO:   category_encoders: 2.6.4
2024-11-04 11:26:29,875:INFO:            lightgbm: 4.5.0
2024-11-04 11:26:29,875:INFO:               numba: 0.60.0
2024-11-04 11:26:29,875:INFO:            requests: 2.32.3
2024-11-04 11:26:29,875:INFO:          matplotlib: 3.7.5
2024-11-04 11:26:29,875:INFO:          scikitplot: 0.3.7
2024-11-04 11:26:29,876:INFO:         yellowbrick: 1.5
2024-11-04 11:26:29,876:INFO:              plotly: 5.24.1
2024-11-04 11:26:29,876:INFO:    plotly-resampler: Not installed
2024-11-04 11:26:29,876:INFO:             kaleido: 0.2.1
2024-11-04 11:26:29,876:INFO:           schemdraw: 0.15
2024-11-04 11:26:29,876:INFO:         statsmodels: 0.14.4
2024-11-04 11:26:29,876:INFO:              sktime: 0.26.0
2024-11-04 11:26:29,876:INFO:               tbats: 1.1.3
2024-11-04 11:26:29,876:INFO:            pmdarima: 2.0.4
2024-11-04 11:26:29,876:INFO:              psutil: 6.1.0
2024-11-04 11:26:29,876:INFO:          markupsafe: 3.0.2
2024-11-04 11:26:29,876:INFO:             pickle5: Not installed
2024-11-04 11:26:29,876:INFO:         cloudpickle: 3.1.0
2024-11-04 11:26:29,876:INFO:         deprecation: 2.1.0
2024-11-04 11:26:29,876:INFO:              xxhash: 3.5.0
2024-11-04 11:26:29,876:INFO:           wurlitzer: 3.1.1
2024-11-04 11:26:29,876:INFO:PyCaret optional dependencies:
2024-11-04 11:26:29,889:INFO:                shap: Not installed
2024-11-04 11:26:29,889:INFO:           interpret: Not installed
2024-11-04 11:26:29,889:INFO:                umap: Not installed
2024-11-04 11:26:29,889:INFO:     ydata_profiling: Not installed
2024-11-04 11:26:29,889:INFO:  explainerdashboard: Not installed
2024-11-04 11:26:29,889:INFO:             autoviz: Not installed
2024-11-04 11:26:29,889:INFO:           fairlearn: Not installed
2024-11-04 11:26:29,890:INFO:          deepchecks: Not installed
2024-11-04 11:26:29,890:INFO:             xgboost: Not installed
2024-11-04 11:26:29,890:INFO:            catboost: Not installed
2024-11-04 11:26:29,890:INFO:              kmodes: Not installed
2024-11-04 11:26:29,890:INFO:             mlxtend: Not installed
2024-11-04 11:26:29,890:INFO:       statsforecast: Not installed
2024-11-04 11:26:29,890:INFO:        tune_sklearn: Not installed
2024-11-04 11:26:29,890:INFO:                 ray: Not installed
2024-11-04 11:26:29,890:INFO:            hyperopt: Not installed
2024-11-04 11:26:29,890:INFO:              optuna: Not installed
2024-11-04 11:26:29,890:INFO:               skopt: Not installed
2024-11-04 11:26:29,890:INFO:              mlflow: Not installed
2024-11-04 11:26:29,890:INFO:              gradio: Not installed
2024-11-04 11:26:29,890:INFO:             fastapi: Not installed
2024-11-04 11:26:29,890:INFO:             uvicorn: Not installed
2024-11-04 11:26:29,890:INFO:              m2cgen: Not installed
2024-11-04 11:26:29,890:INFO:           evidently: Not installed
2024-11-04 11:26:29,890:INFO:               fugue: Not installed
2024-11-04 11:26:29,890:INFO:           streamlit: Not installed
2024-11-04 11:26:29,890:INFO:             prophet: Not installed
2024-11-04 11:26:29,890:INFO:None
2024-11-04 11:26:29,890:INFO:Set up data.
2024-11-04 11:26:29,895:INFO:Set up folding strategy.
2024-11-04 11:26:29,895:INFO:Set up train/test split.
2024-11-04 11:26:29,900:INFO:Set up index.
2024-11-04 11:26:29,900:INFO:Assigning column types.
2024-11-04 11:26:29,902:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:26:29,932:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:26:29,934:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:29,964:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:29,964:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:29,995:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:26:29,996:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,015:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,015:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,015:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:26:30,049:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,067:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,116:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,116:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,116:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:26:30,165:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,165:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,215:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,215:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,216:INFO:Preparing preprocessing pipeline...
2024-11-04 11:26:30,217:INFO:Set up simple imputation.
2024-11-04 11:26:30,219:INFO:Set up encoding of categorical features.
2024-11-04 11:26:30,219:INFO:Set up column name cleaning.
2024-11-04 11:26:30,320:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:26:30,325:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:26:30,325:INFO:Creating final display dataframe.
2024-11-04 11:26:30,541:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                 4
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              9003
2024-11-04 11:26:30,633:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,634:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,692:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,692:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,693:INFO:setup() successfully completed in 0.84s...............
2024-11-04 11:26:30,694:INFO:PyCaret ClassificationExperiment
2024-11-04 11:26:30,694:INFO:Logging name: clf-default-name
2024-11-04 11:26:30,694:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:26:30,694:INFO:version 3.3.2
2024-11-04 11:26:30,694:INFO:Initializing setup()
2024-11-04 11:26:30,694:INFO:self.USI: 271a
2024-11-04 11:26:30,694:INFO:self._variable_keys: {'log_plots_param', 'idx', 'memory', 'X', 'n_jobs_param', 'pipeline', 'seed', 'html_param', 'gpu_n_jobs_param', 'USI', 'exp_id', 'is_multiclass', 'data', 'fold_shuffle_param', 'exp_name_log', 'gpu_param', 'logging_param', '_available_plots', 'X_test', 'y', 'fold_generator', 'target_param', 'y_test', 'fold_groups_param', 'fix_imbalance', '_ml_usecase', 'y_train', 'X_train'}
2024-11-04 11:26:30,694:INFO:Checking environment
2024-11-04 11:26:30,694:INFO:python_version: 3.11.2
2024-11-04 11:26:30,694:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:26:30,694:INFO:machine: x86_64
2024-11-04 11:26:30,694:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:26:30,694:INFO:Memory: svmem(total=16721211392, available=10236317696, percent=38.8, used=6018195456, free=4321136640, active=1762361344, inactive=9621966848, buffers=306909184, cached=6074970112, shared=118059008, slab=524435456)
2024-11-04 11:26:30,695:INFO:Physical Core: 4
2024-11-04 11:26:30,695:INFO:Logical Core: 8
2024-11-04 11:26:30,695:INFO:Checking libraries
2024-11-04 11:26:30,695:INFO:System:
2024-11-04 11:26:30,695:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:26:30,695:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:26:30,695:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:26:30,695:INFO:PyCaret required dependencies:
2024-11-04 11:26:30,695:INFO:                 pip: 23.0.1
2024-11-04 11:26:30,695:INFO:          setuptools: 66.1.1
2024-11-04 11:26:30,695:INFO:             pycaret: 3.3.2
2024-11-04 11:26:30,695:INFO:             IPython: 8.29.0
2024-11-04 11:26:30,695:INFO:          ipywidgets: 8.1.5
2024-11-04 11:26:30,695:INFO:                tqdm: 4.66.6
2024-11-04 11:26:30,695:INFO:               numpy: 1.26.4
2024-11-04 11:26:30,695:INFO:              pandas: 2.1.4
2024-11-04 11:26:30,695:INFO:              jinja2: 3.1.4
2024-11-04 11:26:30,695:INFO:               scipy: 1.11.4
2024-11-04 11:26:30,695:INFO:              joblib: 1.3.2
2024-11-04 11:26:30,695:INFO:             sklearn: 1.4.2
2024-11-04 11:26:30,695:INFO:                pyod: 2.0.2
2024-11-04 11:26:30,695:INFO:            imblearn: 0.12.4
2024-11-04 11:26:30,695:INFO:   category_encoders: 2.6.4
2024-11-04 11:26:30,696:INFO:            lightgbm: 4.5.0
2024-11-04 11:26:30,696:INFO:               numba: 0.60.0
2024-11-04 11:26:30,696:INFO:            requests: 2.32.3
2024-11-04 11:26:30,696:INFO:          matplotlib: 3.7.5
2024-11-04 11:26:30,696:INFO:          scikitplot: 0.3.7
2024-11-04 11:26:30,696:INFO:         yellowbrick: 1.5
2024-11-04 11:26:30,696:INFO:              plotly: 5.24.1
2024-11-04 11:26:30,696:INFO:    plotly-resampler: Not installed
2024-11-04 11:26:30,696:INFO:             kaleido: 0.2.1
2024-11-04 11:26:30,696:INFO:           schemdraw: 0.15
2024-11-04 11:26:30,696:INFO:         statsmodels: 0.14.4
2024-11-04 11:26:30,696:INFO:              sktime: 0.26.0
2024-11-04 11:26:30,696:INFO:               tbats: 1.1.3
2024-11-04 11:26:30,696:INFO:            pmdarima: 2.0.4
2024-11-04 11:26:30,696:INFO:              psutil: 6.1.0
2024-11-04 11:26:30,696:INFO:          markupsafe: 3.0.2
2024-11-04 11:26:30,696:INFO:             pickle5: Not installed
2024-11-04 11:26:30,696:INFO:         cloudpickle: 3.1.0
2024-11-04 11:26:30,696:INFO:         deprecation: 2.1.0
2024-11-04 11:26:30,696:INFO:              xxhash: 3.5.0
2024-11-04 11:26:30,696:INFO:           wurlitzer: 3.1.1
2024-11-04 11:26:30,696:INFO:PyCaret optional dependencies:
2024-11-04 11:26:30,696:INFO:                shap: Not installed
2024-11-04 11:26:30,696:INFO:           interpret: Not installed
2024-11-04 11:26:30,696:INFO:                umap: Not installed
2024-11-04 11:26:30,696:INFO:     ydata_profiling: Not installed
2024-11-04 11:26:30,696:INFO:  explainerdashboard: Not installed
2024-11-04 11:26:30,696:INFO:             autoviz: Not installed
2024-11-04 11:26:30,696:INFO:           fairlearn: Not installed
2024-11-04 11:26:30,696:INFO:          deepchecks: Not installed
2024-11-04 11:26:30,696:INFO:             xgboost: Not installed
2024-11-04 11:26:30,696:INFO:            catboost: Not installed
2024-11-04 11:26:30,697:INFO:              kmodes: Not installed
2024-11-04 11:26:30,697:INFO:             mlxtend: Not installed
2024-11-04 11:26:30,697:INFO:       statsforecast: Not installed
2024-11-04 11:26:30,697:INFO:        tune_sklearn: Not installed
2024-11-04 11:26:30,697:INFO:                 ray: Not installed
2024-11-04 11:26:30,697:INFO:            hyperopt: Not installed
2024-11-04 11:26:30,697:INFO:              optuna: Not installed
2024-11-04 11:26:30,697:INFO:               skopt: Not installed
2024-11-04 11:26:30,697:INFO:              mlflow: Not installed
2024-11-04 11:26:30,697:INFO:              gradio: Not installed
2024-11-04 11:26:30,697:INFO:             fastapi: Not installed
2024-11-04 11:26:30,697:INFO:             uvicorn: Not installed
2024-11-04 11:26:30,697:INFO:              m2cgen: Not installed
2024-11-04 11:26:30,697:INFO:           evidently: Not installed
2024-11-04 11:26:30,697:INFO:               fugue: Not installed
2024-11-04 11:26:30,697:INFO:           streamlit: Not installed
2024-11-04 11:26:30,697:INFO:             prophet: Not installed
2024-11-04 11:26:30,697:INFO:None
2024-11-04 11:26:30,697:INFO:Set up data.
2024-11-04 11:26:30,702:INFO:Set up folding strategy.
2024-11-04 11:26:30,702:INFO:Set up train/test split.
2024-11-04 11:26:30,705:INFO:Set up index.
2024-11-04 11:26:30,705:INFO:Assigning column types.
2024-11-04 11:26:30,707:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:26:30,737:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,737:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,756:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,787:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,787:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,806:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,806:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,806:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:26:30,836:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,855:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,886:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:26:30,904:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,905:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,905:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:26:30,953:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:30,953:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,001:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,001:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,002:INFO:Preparing preprocessing pipeline...
2024-11-04 11:26:31,003:INFO:Set up simple imputation.
2024-11-04 11:26:31,005:INFO:Set up encoding of categorical features.
2024-11-04 11:26:31,005:INFO:Set up column name cleaning.
2024-11-04 11:26:31,100:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:26:31,104:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:26:31,104:INFO:Creating final display dataframe.
2024-11-04 11:26:31,332:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              271a
2024-11-04 11:26:31,403:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,403:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,452:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:26:31,453:INFO:setup() successfully completed in 0.76s...............
2024-11-04 11:26:31,453:INFO:Initializing compare_models()
2024-11-04 11:26:31,453:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-04 11:26:31,453:INFO:Checking exceptions
2024-11-04 11:26:31,455:INFO:Preparing display monitor
2024-11-04 11:26:31,457:INFO:Initializing Logistic Regression
2024-11-04 11:26:31,457:INFO:Total runtime is 1.1920928955078125e-06 minutes
2024-11-04 11:26:31,458:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:31,458:INFO:Initializing create_model()
2024-11-04 11:26:31,458:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:31,458:INFO:Checking exceptions
2024-11-04 11:26:31,458:INFO:Importing libraries
2024-11-04 11:26:31,458:INFO:Copying training dataset
2024-11-04 11:26:31,461:INFO:Defining folds
2024-11-04 11:26:31,461:INFO:Declaring metric variables
2024-11-04 11:26:31,461:INFO:Importing untrained model
2024-11-04 11:26:31,461:INFO:Logistic Regression Imported successfully
2024-11-04 11:26:31,461:INFO:Starting cross validation
2024-11-04 11:26:31,463:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:33,134:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:33,177:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:33,324:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:33,348:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:33,360:INFO:Calculating mean and std
2024-11-04 11:26:33,361:INFO:Creating metrics dataframe
2024-11-04 11:26:33,365:INFO:Uploading results into container
2024-11-04 11:26:33,366:INFO:Uploading model into container now
2024-11-04 11:26:33,366:INFO:_master_model_container: 1
2024-11-04 11:26:33,367:INFO:_display_container: 2
2024-11-04 11:26:33,367:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-04 11:26:33,368:INFO:create_model() successfully completed......................................
2024-11-04 11:26:33,471:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:33,471:INFO:Creating metrics dataframe
2024-11-04 11:26:33,473:INFO:Initializing K Neighbors Classifier
2024-11-04 11:26:33,473:INFO:Total runtime is 0.03360196352005005 minutes
2024-11-04 11:26:33,474:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:33,474:INFO:Initializing create_model()
2024-11-04 11:26:33,474:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:33,474:INFO:Checking exceptions
2024-11-04 11:26:33,474:INFO:Importing libraries
2024-11-04 11:26:33,474:INFO:Copying training dataset
2024-11-04 11:26:33,477:INFO:Defining folds
2024-11-04 11:26:33,477:INFO:Declaring metric variables
2024-11-04 11:26:33,477:INFO:Importing untrained model
2024-11-04 11:26:33,477:INFO:K Neighbors Classifier Imported successfully
2024-11-04 11:26:33,477:INFO:Starting cross validation
2024-11-04 11:26:33,479:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:35,152:INFO:Calculating mean and std
2024-11-04 11:26:35,153:INFO:Creating metrics dataframe
2024-11-04 11:26:35,155:INFO:Uploading results into container
2024-11-04 11:26:35,155:INFO:Uploading model into container now
2024-11-04 11:26:35,156:INFO:_master_model_container: 2
2024-11-04 11:26:35,156:INFO:_display_container: 2
2024-11-04 11:26:35,156:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-04 11:26:35,156:INFO:create_model() successfully completed......................................
2024-11-04 11:26:35,257:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:35,258:INFO:Creating metrics dataframe
2024-11-04 11:26:35,260:INFO:Initializing Naive Bayes
2024-11-04 11:26:35,261:INFO:Total runtime is 0.06338812112808227 minutes
2024-11-04 11:26:35,261:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:35,261:INFO:Initializing create_model()
2024-11-04 11:26:35,261:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:35,261:INFO:Checking exceptions
2024-11-04 11:26:35,261:INFO:Importing libraries
2024-11-04 11:26:35,261:INFO:Copying training dataset
2024-11-04 11:26:35,264:INFO:Defining folds
2024-11-04 11:26:35,264:INFO:Declaring metric variables
2024-11-04 11:26:35,264:INFO:Importing untrained model
2024-11-04 11:26:35,264:INFO:Naive Bayes Imported successfully
2024-11-04 11:26:35,264:INFO:Starting cross validation
2024-11-04 11:26:35,266:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:35,388:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:35,392:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:35,400:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:35,413:INFO:Calculating mean and std
2024-11-04 11:26:35,414:INFO:Creating metrics dataframe
2024-11-04 11:26:35,415:INFO:Uploading results into container
2024-11-04 11:26:35,416:INFO:Uploading model into container now
2024-11-04 11:26:35,416:INFO:_master_model_container: 3
2024-11-04 11:26:35,416:INFO:_display_container: 2
2024-11-04 11:26:35,416:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-04 11:26:35,416:INFO:create_model() successfully completed......................................
2024-11-04 11:26:35,492:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:35,492:INFO:Creating metrics dataframe
2024-11-04 11:26:35,494:INFO:Initializing Decision Tree Classifier
2024-11-04 11:26:35,495:INFO:Total runtime is 0.06728856166203817 minutes
2024-11-04 11:26:35,495:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:35,495:INFO:Initializing create_model()
2024-11-04 11:26:35,495:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:35,495:INFO:Checking exceptions
2024-11-04 11:26:35,495:INFO:Importing libraries
2024-11-04 11:26:35,495:INFO:Copying training dataset
2024-11-04 11:26:35,498:INFO:Defining folds
2024-11-04 11:26:35,498:INFO:Declaring metric variables
2024-11-04 11:26:35,498:INFO:Importing untrained model
2024-11-04 11:26:35,499:INFO:Decision Tree Classifier Imported successfully
2024-11-04 11:26:35,499:INFO:Starting cross validation
2024-11-04 11:26:35,500:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:35,709:INFO:Calculating mean and std
2024-11-04 11:26:35,712:INFO:Creating metrics dataframe
2024-11-04 11:26:35,718:INFO:Uploading results into container
2024-11-04 11:26:35,720:INFO:Uploading model into container now
2024-11-04 11:26:35,722:INFO:_master_model_container: 4
2024-11-04 11:26:35,722:INFO:_display_container: 2
2024-11-04 11:26:35,723:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-04 11:26:35,723:INFO:create_model() successfully completed......................................
2024-11-04 11:26:35,843:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:35,843:INFO:Creating metrics dataframe
2024-11-04 11:26:35,847:INFO:Initializing SVM - Linear Kernel
2024-11-04 11:26:35,847:INFO:Total runtime is 0.07315795024236044 minutes
2024-11-04 11:26:35,847:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:35,847:INFO:Initializing create_model()
2024-11-04 11:26:35,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:35,847:INFO:Checking exceptions
2024-11-04 11:26:35,847:INFO:Importing libraries
2024-11-04 11:26:35,847:INFO:Copying training dataset
2024-11-04 11:26:35,851:INFO:Defining folds
2024-11-04 11:26:35,851:INFO:Declaring metric variables
2024-11-04 11:26:35,852:INFO:Importing untrained model
2024-11-04 11:26:35,852:INFO:SVM - Linear Kernel Imported successfully
2024-11-04 11:26:35,852:INFO:Starting cross validation
2024-11-04 11:26:35,854:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:36,010:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,017:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,023:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,025:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,030:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:36,043:INFO:Calculating mean and std
2024-11-04 11:26:36,044:INFO:Creating metrics dataframe
2024-11-04 11:26:36,045:INFO:Uploading results into container
2024-11-04 11:26:36,046:INFO:Uploading model into container now
2024-11-04 11:26:36,046:INFO:_master_model_container: 5
2024-11-04 11:26:36,046:INFO:_display_container: 2
2024-11-04 11:26:36,047:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-04 11:26:36,047:INFO:create_model() successfully completed......................................
2024-11-04 11:26:36,129:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:36,129:INFO:Creating metrics dataframe
2024-11-04 11:26:36,132:INFO:Initializing Ridge Classifier
2024-11-04 11:26:36,132:INFO:Total runtime is 0.07792127927144368 minutes
2024-11-04 11:26:36,133:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:36,133:INFO:Initializing create_model()
2024-11-04 11:26:36,133:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:36,133:INFO:Checking exceptions
2024-11-04 11:26:36,133:INFO:Importing libraries
2024-11-04 11:26:36,133:INFO:Copying training dataset
2024-11-04 11:26:36,135:INFO:Defining folds
2024-11-04 11:26:36,136:INFO:Declaring metric variables
2024-11-04 11:26:36,136:INFO:Importing untrained model
2024-11-04 11:26:36,136:INFO:Ridge Classifier Imported successfully
2024-11-04 11:26:36,136:INFO:Starting cross validation
2024-11-04 11:26:36,138:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:36,280:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,312:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,314:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,316:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:36,318:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:36,321:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:36,324:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:26:36,336:INFO:Calculating mean and std
2024-11-04 11:26:36,336:INFO:Creating metrics dataframe
2024-11-04 11:26:36,338:INFO:Uploading results into container
2024-11-04 11:26:36,339:INFO:Uploading model into container now
2024-11-04 11:26:36,339:INFO:_master_model_container: 6
2024-11-04 11:26:36,339:INFO:_display_container: 2
2024-11-04 11:26:36,339:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-04 11:26:36,339:INFO:create_model() successfully completed......................................
2024-11-04 11:26:36,415:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:36,415:INFO:Creating metrics dataframe
2024-11-04 11:26:36,417:INFO:Initializing Random Forest Classifier
2024-11-04 11:26:36,417:INFO:Total runtime is 0.0826703151067098 minutes
2024-11-04 11:26:36,418:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:36,418:INFO:Initializing create_model()
2024-11-04 11:26:36,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:36,418:INFO:Checking exceptions
2024-11-04 11:26:36,418:INFO:Importing libraries
2024-11-04 11:26:36,418:INFO:Copying training dataset
2024-11-04 11:26:36,420:INFO:Defining folds
2024-11-04 11:26:36,420:INFO:Declaring metric variables
2024-11-04 11:26:36,421:INFO:Importing untrained model
2024-11-04 11:26:36,421:INFO:Random Forest Classifier Imported successfully
2024-11-04 11:26:36,421:INFO:Starting cross validation
2024-11-04 11:26:36,423:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:36,915:INFO:Calculating mean and std
2024-11-04 11:26:36,915:INFO:Creating metrics dataframe
2024-11-04 11:26:36,917:INFO:Uploading results into container
2024-11-04 11:26:36,917:INFO:Uploading model into container now
2024-11-04 11:26:36,918:INFO:_master_model_container: 7
2024-11-04 11:26:36,918:INFO:_display_container: 2
2024-11-04 11:26:36,918:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-04 11:26:36,918:INFO:create_model() successfully completed......................................
2024-11-04 11:26:36,991:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:36,991:INFO:Creating metrics dataframe
2024-11-04 11:26:36,993:INFO:Initializing Quadratic Discriminant Analysis
2024-11-04 11:26:36,993:INFO:Total runtime is 0.09225944677988689 minutes
2024-11-04 11:26:36,993:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:36,993:INFO:Initializing create_model()
2024-11-04 11:26:36,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:36,993:INFO:Checking exceptions
2024-11-04 11:26:36,993:INFO:Importing libraries
2024-11-04 11:26:36,993:INFO:Copying training dataset
2024-11-04 11:26:36,996:INFO:Defining folds
2024-11-04 11:26:36,996:INFO:Declaring metric variables
2024-11-04 11:26:36,996:INFO:Importing untrained model
2024-11-04 11:26:36,996:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-04 11:26:36,996:INFO:Starting cross validation
2024-11-04 11:26:36,998:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:37,110:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:26:37,112:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:26:37,122:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:26:37,124:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:26:37,137:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,139:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,161:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,162:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,175:INFO:Calculating mean and std
2024-11-04 11:26:37,175:INFO:Creating metrics dataframe
2024-11-04 11:26:37,177:INFO:Uploading results into container
2024-11-04 11:26:37,178:INFO:Uploading model into container now
2024-11-04 11:26:37,178:INFO:_master_model_container: 8
2024-11-04 11:26:37,178:INFO:_display_container: 2
2024-11-04 11:26:37,178:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-04 11:26:37,178:INFO:create_model() successfully completed......................................
2024-11-04 11:26:37,244:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:37,244:INFO:Creating metrics dataframe
2024-11-04 11:26:37,246:INFO:Initializing Ada Boost Classifier
2024-11-04 11:26:37,246:INFO:Total runtime is 0.0964775323867798 minutes
2024-11-04 11:26:37,246:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:37,246:INFO:Initializing create_model()
2024-11-04 11:26:37,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:37,246:INFO:Checking exceptions
2024-11-04 11:26:37,246:INFO:Importing libraries
2024-11-04 11:26:37,246:INFO:Copying training dataset
2024-11-04 11:26:37,249:INFO:Defining folds
2024-11-04 11:26:37,249:INFO:Declaring metric variables
2024-11-04 11:26:37,249:INFO:Importing untrained model
2024-11-04 11:26:37,249:INFO:Ada Boost Classifier Imported successfully
2024-11-04 11:26:37,249:INFO:Starting cross validation
2024-11-04 11:26:37,251:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:37,342:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:26:37,353:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:26:37,355:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:26:37,356:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:26:37,439:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,448:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,451:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,452:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:37,468:INFO:Calculating mean and std
2024-11-04 11:26:37,468:INFO:Creating metrics dataframe
2024-11-04 11:26:37,470:INFO:Uploading results into container
2024-11-04 11:26:37,470:INFO:Uploading model into container now
2024-11-04 11:26:37,471:INFO:_master_model_container: 9
2024-11-04 11:26:37,471:INFO:_display_container: 2
2024-11-04 11:26:37,471:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-04 11:26:37,471:INFO:create_model() successfully completed......................................
2024-11-04 11:26:37,542:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:37,542:INFO:Creating metrics dataframe
2024-11-04 11:26:37,544:INFO:Initializing Gradient Boosting Classifier
2024-11-04 11:26:37,544:INFO:Total runtime is 0.1014468510945638 minutes
2024-11-04 11:26:37,544:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:37,544:INFO:Initializing create_model()
2024-11-04 11:26:37,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:37,545:INFO:Checking exceptions
2024-11-04 11:26:37,545:INFO:Importing libraries
2024-11-04 11:26:37,545:INFO:Copying training dataset
2024-11-04 11:26:37,547:INFO:Defining folds
2024-11-04 11:26:37,547:INFO:Declaring metric variables
2024-11-04 11:26:37,548:INFO:Importing untrained model
2024-11-04 11:26:37,548:INFO:Gradient Boosting Classifier Imported successfully
2024-11-04 11:26:37,548:INFO:Starting cross validation
2024-11-04 11:26:37,550:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:38,006:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,015:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,090:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,102:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,109:INFO:Calculating mean and std
2024-11-04 11:26:38,110:INFO:Creating metrics dataframe
2024-11-04 11:26:38,112:INFO:Uploading results into container
2024-11-04 11:26:38,112:INFO:Uploading model into container now
2024-11-04 11:26:38,112:INFO:_master_model_container: 10
2024-11-04 11:26:38,112:INFO:_display_container: 2
2024-11-04 11:26:38,113:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-04 11:26:38,113:INFO:create_model() successfully completed......................................
2024-11-04 11:26:38,178:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:38,178:INFO:Creating metrics dataframe
2024-11-04 11:26:38,180:INFO:Initializing Linear Discriminant Analysis
2024-11-04 11:26:38,180:INFO:Total runtime is 0.11204785903294881 minutes
2024-11-04 11:26:38,180:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:38,180:INFO:Initializing create_model()
2024-11-04 11:26:38,180:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:38,180:INFO:Checking exceptions
2024-11-04 11:26:38,180:INFO:Importing libraries
2024-11-04 11:26:38,180:INFO:Copying training dataset
2024-11-04 11:26:38,183:INFO:Defining folds
2024-11-04 11:26:38,183:INFO:Declaring metric variables
2024-11-04 11:26:38,183:INFO:Importing untrained model
2024-11-04 11:26:38,183:INFO:Linear Discriminant Analysis Imported successfully
2024-11-04 11:26:38,183:INFO:Starting cross validation
2024-11-04 11:26:38,185:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:38,293:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,293:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,313:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,313:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:26:38,331:INFO:Calculating mean and std
2024-11-04 11:26:38,331:INFO:Creating metrics dataframe
2024-11-04 11:26:38,333:INFO:Uploading results into container
2024-11-04 11:26:38,333:INFO:Uploading model into container now
2024-11-04 11:26:38,334:INFO:_master_model_container: 11
2024-11-04 11:26:38,334:INFO:_display_container: 2
2024-11-04 11:26:38,334:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-04 11:26:38,334:INFO:create_model() successfully completed......................................
2024-11-04 11:26:38,400:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:38,400:INFO:Creating metrics dataframe
2024-11-04 11:26:38,402:INFO:Initializing Extra Trees Classifier
2024-11-04 11:26:38,402:INFO:Total runtime is 0.11575011014938354 minutes
2024-11-04 11:26:38,402:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:38,402:INFO:Initializing create_model()
2024-11-04 11:26:38,403:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:38,403:INFO:Checking exceptions
2024-11-04 11:26:38,403:INFO:Importing libraries
2024-11-04 11:26:38,403:INFO:Copying training dataset
2024-11-04 11:26:38,405:INFO:Defining folds
2024-11-04 11:26:38,405:INFO:Declaring metric variables
2024-11-04 11:26:38,405:INFO:Importing untrained model
2024-11-04 11:26:38,405:INFO:Extra Trees Classifier Imported successfully
2024-11-04 11:26:38,405:INFO:Starting cross validation
2024-11-04 11:26:38,407:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:26:38,725:INFO:Calculating mean and std
2024-11-04 11:26:38,726:INFO:Creating metrics dataframe
2024-11-04 11:26:38,727:INFO:Uploading results into container
2024-11-04 11:26:38,727:INFO:Uploading model into container now
2024-11-04 11:26:38,727:INFO:_master_model_container: 12
2024-11-04 11:26:38,727:INFO:_display_container: 2
2024-11-04 11:26:38,728:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-04 11:26:38,728:INFO:create_model() successfully completed......................................
2024-11-04 11:26:38,793:INFO:SubProcess create_model() end ==================================
2024-11-04 11:26:38,793:INFO:Creating metrics dataframe
2024-11-04 11:26:38,795:INFO:Initializing Light Gradient Boosting Machine
2024-11-04 11:26:38,795:INFO:Total runtime is 0.12229493856430053 minutes
2024-11-04 11:26:38,795:INFO:SubProcess create_model() called ==================================
2024-11-04 11:26:38,795:INFO:Initializing create_model()
2024-11-04 11:26:38,795:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f983ec72910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f981db54990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:26:38,795:INFO:Checking exceptions
2024-11-04 11:26:38,795:INFO:Importing libraries
2024-11-04 11:26:38,795:INFO:Copying training dataset
2024-11-04 11:26:38,798:INFO:Defining folds
2024-11-04 11:26:38,798:INFO:Declaring metric variables
2024-11-04 11:26:38,798:INFO:Importing untrained model
2024-11-04 11:26:38,798:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-04 11:26:38,798:INFO:Starting cross validation
2024-11-04 11:26:38,800:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:28:59,573:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:28:59,573:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:28:59,574:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:28:59,574:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:29:00,086:INFO:PyCaret ClassificationExperiment
2024-11-04 11:29:00,086:INFO:Logging name: clf-default-name
2024-11-04 11:29:00,086:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:29:00,086:INFO:version 3.3.2
2024-11-04 11:29:00,086:INFO:Initializing setup()
2024-11-04 11:29:00,086:INFO:self.USI: 0a2e
2024-11-04 11:29:00,086:INFO:self._variable_keys: {'X_train', 'is_multiclass', 'html_param', 'exp_id', 'target_param', '_ml_usecase', 'X', 'data', 'y_test', 'log_plots_param', 'memory', 'fold_shuffle_param', 'y_train', 'fold_generator', 'fix_imbalance', '_available_plots', 'y', 'n_jobs_param', 'idx', 'fold_groups_param', 'seed', 'X_test', 'gpu_n_jobs_param', 'logging_param', 'gpu_param', 'pipeline', 'USI', 'exp_name_log'}
2024-11-04 11:29:00,086:INFO:Checking environment
2024-11-04 11:29:00,086:INFO:python_version: 3.11.2
2024-11-04 11:29:00,086:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:29:00,086:INFO:machine: x86_64
2024-11-04 11:29:00,087:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:29:00,087:INFO:Memory: svmem(total=16721211392, available=10263887872, percent=38.6, used=5953306624, free=4342157312, active=1881501696, inactive=9483411456, buffers=308264960, cached=6117482496, shared=155377664, slab=526561280)
2024-11-04 11:29:00,088:INFO:Physical Core: 4
2024-11-04 11:29:00,088:INFO:Logical Core: 8
2024-11-04 11:29:00,088:INFO:Checking libraries
2024-11-04 11:29:00,088:INFO:System:
2024-11-04 11:29:00,088:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:29:00,088:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:29:00,088:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:29:00,088:INFO:PyCaret required dependencies:
2024-11-04 11:29:00,106:INFO:                 pip: 23.0.1
2024-11-04 11:29:00,106:INFO:          setuptools: 66.1.1
2024-11-04 11:29:00,106:INFO:             pycaret: 3.3.2
2024-11-04 11:29:00,106:INFO:             IPython: 8.29.0
2024-11-04 11:29:00,106:INFO:          ipywidgets: 8.1.5
2024-11-04 11:29:00,106:INFO:                tqdm: 4.66.6
2024-11-04 11:29:00,106:INFO:               numpy: 1.26.4
2024-11-04 11:29:00,106:INFO:              pandas: 2.1.4
2024-11-04 11:29:00,106:INFO:              jinja2: 3.1.4
2024-11-04 11:29:00,106:INFO:               scipy: 1.11.4
2024-11-04 11:29:00,106:INFO:              joblib: 1.3.2
2024-11-04 11:29:00,106:INFO:             sklearn: 1.4.2
2024-11-04 11:29:00,106:INFO:                pyod: 2.0.2
2024-11-04 11:29:00,106:INFO:            imblearn: 0.12.4
2024-11-04 11:29:00,107:INFO:   category_encoders: 2.6.4
2024-11-04 11:29:00,107:INFO:            lightgbm: 4.5.0
2024-11-04 11:29:00,107:INFO:               numba: 0.60.0
2024-11-04 11:29:00,107:INFO:            requests: 2.32.3
2024-11-04 11:29:00,107:INFO:          matplotlib: 3.7.5
2024-11-04 11:29:00,107:INFO:          scikitplot: 0.3.7
2024-11-04 11:29:00,107:INFO:         yellowbrick: 1.5
2024-11-04 11:29:00,107:INFO:              plotly: 5.24.1
2024-11-04 11:29:00,107:INFO:    plotly-resampler: Not installed
2024-11-04 11:29:00,107:INFO:             kaleido: 0.2.1
2024-11-04 11:29:00,107:INFO:           schemdraw: 0.15
2024-11-04 11:29:00,107:INFO:         statsmodels: 0.14.4
2024-11-04 11:29:00,107:INFO:              sktime: 0.26.0
2024-11-04 11:29:00,107:INFO:               tbats: 1.1.3
2024-11-04 11:29:00,107:INFO:            pmdarima: 2.0.4
2024-11-04 11:29:00,107:INFO:              psutil: 6.1.0
2024-11-04 11:29:00,107:INFO:          markupsafe: 3.0.2
2024-11-04 11:29:00,107:INFO:             pickle5: Not installed
2024-11-04 11:29:00,107:INFO:         cloudpickle: 3.1.0
2024-11-04 11:29:00,107:INFO:         deprecation: 2.1.0
2024-11-04 11:29:00,107:INFO:              xxhash: 3.5.0
2024-11-04 11:29:00,107:INFO:           wurlitzer: 3.1.1
2024-11-04 11:29:00,107:INFO:PyCaret optional dependencies:
2024-11-04 11:29:00,125:INFO:                shap: Not installed
2024-11-04 11:29:00,125:INFO:           interpret: Not installed
2024-11-04 11:29:00,125:INFO:                umap: Not installed
2024-11-04 11:29:00,125:INFO:     ydata_profiling: Not installed
2024-11-04 11:29:00,125:INFO:  explainerdashboard: Not installed
2024-11-04 11:29:00,125:INFO:             autoviz: Not installed
2024-11-04 11:29:00,125:INFO:           fairlearn: Not installed
2024-11-04 11:29:00,125:INFO:          deepchecks: Not installed
2024-11-04 11:29:00,126:INFO:             xgboost: Not installed
2024-11-04 11:29:00,126:INFO:            catboost: Not installed
2024-11-04 11:29:00,126:INFO:              kmodes: Not installed
2024-11-04 11:29:00,126:INFO:             mlxtend: Not installed
2024-11-04 11:29:00,126:INFO:       statsforecast: Not installed
2024-11-04 11:29:00,126:INFO:        tune_sklearn: Not installed
2024-11-04 11:29:00,126:INFO:                 ray: Not installed
2024-11-04 11:29:00,126:INFO:            hyperopt: Not installed
2024-11-04 11:29:00,126:INFO:              optuna: Not installed
2024-11-04 11:29:00,126:INFO:               skopt: Not installed
2024-11-04 11:29:00,126:INFO:              mlflow: Not installed
2024-11-04 11:29:00,126:INFO:              gradio: Not installed
2024-11-04 11:29:00,126:INFO:             fastapi: Not installed
2024-11-04 11:29:00,126:INFO:             uvicorn: Not installed
2024-11-04 11:29:00,126:INFO:              m2cgen: Not installed
2024-11-04 11:29:00,126:INFO:           evidently: Not installed
2024-11-04 11:29:00,126:INFO:               fugue: Not installed
2024-11-04 11:29:00,126:INFO:           streamlit: Not installed
2024-11-04 11:29:00,126:INFO:             prophet: Not installed
2024-11-04 11:29:00,126:INFO:None
2024-11-04 11:29:00,126:INFO:Set up data.
2024-11-04 11:29:00,134:INFO:Set up folding strategy.
2024-11-04 11:29:00,134:INFO:Set up train/test split.
2024-11-04 11:29:00,141:INFO:Set up index.
2024-11-04 11:29:00,141:INFO:Assigning column types.
2024-11-04 11:29:00,144:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:29:00,186:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,189:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,218:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,218:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,277:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,278:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,300:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,300:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,301:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:29:00,338:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,360:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,360:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,397:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:00,418:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,418:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,419:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:29:00,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,542:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,542:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:00,543:INFO:Preparing preprocessing pipeline...
2024-11-04 11:29:00,544:INFO:Set up simple imputation.
2024-11-04 11:29:00,548:INFO:Set up encoding of categorical features.
2024-11-04 11:29:00,549:INFO:Set up column name cleaning.
2024-11-04 11:29:00,680:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:29:00,686:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:29:00,686:INFO:Creating final display dataframe.
2024-11-04 11:29:00,947:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                 4
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0a2e
2024-11-04 11:29:01,012:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,013:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,078:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,078:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,079:INFO:setup() successfully completed in 0.99s...............
2024-11-04 11:29:01,080:INFO:PyCaret ClassificationExperiment
2024-11-04 11:29:01,080:INFO:Logging name: clf-default-name
2024-11-04 11:29:01,080:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:29:01,080:INFO:version 3.3.2
2024-11-04 11:29:01,080:INFO:Initializing setup()
2024-11-04 11:29:01,080:INFO:self.USI: 28cb
2024-11-04 11:29:01,080:INFO:self._variable_keys: {'X_train', 'is_multiclass', 'html_param', 'exp_id', 'target_param', '_ml_usecase', 'X', 'data', 'y_test', 'log_plots_param', 'memory', 'fold_shuffle_param', 'y_train', 'fold_generator', 'fix_imbalance', '_available_plots', 'y', 'n_jobs_param', 'idx', 'fold_groups_param', 'seed', 'X_test', 'gpu_n_jobs_param', 'logging_param', 'gpu_param', 'pipeline', 'USI', 'exp_name_log'}
2024-11-04 11:29:01,080:INFO:Checking environment
2024-11-04 11:29:01,080:INFO:python_version: 3.11.2
2024-11-04 11:29:01,080:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:29:01,080:INFO:machine: x86_64
2024-11-04 11:29:01,080:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:29:01,080:INFO:Memory: svmem(total=16721211392, available=10245439488, percent=38.7, used=5971828736, free=4323667968, active=1881759744, inactive=9505173504, buffers=308264960, cached=6117449728, shared=155303936, slab=526553088)
2024-11-04 11:29:01,081:INFO:Physical Core: 4
2024-11-04 11:29:01,081:INFO:Logical Core: 8
2024-11-04 11:29:01,081:INFO:Checking libraries
2024-11-04 11:29:01,081:INFO:System:
2024-11-04 11:29:01,081:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:29:01,081:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:29:01,081:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:29:01,081:INFO:PyCaret required dependencies:
2024-11-04 11:29:01,081:INFO:                 pip: 23.0.1
2024-11-04 11:29:01,081:INFO:          setuptools: 66.1.1
2024-11-04 11:29:01,081:INFO:             pycaret: 3.3.2
2024-11-04 11:29:01,081:INFO:             IPython: 8.29.0
2024-11-04 11:29:01,081:INFO:          ipywidgets: 8.1.5
2024-11-04 11:29:01,081:INFO:                tqdm: 4.66.6
2024-11-04 11:29:01,081:INFO:               numpy: 1.26.4
2024-11-04 11:29:01,081:INFO:              pandas: 2.1.4
2024-11-04 11:29:01,081:INFO:              jinja2: 3.1.4
2024-11-04 11:29:01,081:INFO:               scipy: 1.11.4
2024-11-04 11:29:01,081:INFO:              joblib: 1.3.2
2024-11-04 11:29:01,081:INFO:             sklearn: 1.4.2
2024-11-04 11:29:01,081:INFO:                pyod: 2.0.2
2024-11-04 11:29:01,081:INFO:            imblearn: 0.12.4
2024-11-04 11:29:01,081:INFO:   category_encoders: 2.6.4
2024-11-04 11:29:01,081:INFO:            lightgbm: 4.5.0
2024-11-04 11:29:01,081:INFO:               numba: 0.60.0
2024-11-04 11:29:01,081:INFO:            requests: 2.32.3
2024-11-04 11:29:01,081:INFO:          matplotlib: 3.7.5
2024-11-04 11:29:01,081:INFO:          scikitplot: 0.3.7
2024-11-04 11:29:01,081:INFO:         yellowbrick: 1.5
2024-11-04 11:29:01,081:INFO:              plotly: 5.24.1
2024-11-04 11:29:01,081:INFO:    plotly-resampler: Not installed
2024-11-04 11:29:01,081:INFO:             kaleido: 0.2.1
2024-11-04 11:29:01,081:INFO:           schemdraw: 0.15
2024-11-04 11:29:01,081:INFO:         statsmodels: 0.14.4
2024-11-04 11:29:01,081:INFO:              sktime: 0.26.0
2024-11-04 11:29:01,081:INFO:               tbats: 1.1.3
2024-11-04 11:29:01,081:INFO:            pmdarima: 2.0.4
2024-11-04 11:29:01,081:INFO:              psutil: 6.1.0
2024-11-04 11:29:01,081:INFO:          markupsafe: 3.0.2
2024-11-04 11:29:01,082:INFO:             pickle5: Not installed
2024-11-04 11:29:01,082:INFO:         cloudpickle: 3.1.0
2024-11-04 11:29:01,082:INFO:         deprecation: 2.1.0
2024-11-04 11:29:01,082:INFO:              xxhash: 3.5.0
2024-11-04 11:29:01,082:INFO:           wurlitzer: 3.1.1
2024-11-04 11:29:01,082:INFO:PyCaret optional dependencies:
2024-11-04 11:29:01,082:INFO:                shap: Not installed
2024-11-04 11:29:01,082:INFO:           interpret: Not installed
2024-11-04 11:29:01,082:INFO:                umap: Not installed
2024-11-04 11:29:01,082:INFO:     ydata_profiling: Not installed
2024-11-04 11:29:01,082:INFO:  explainerdashboard: Not installed
2024-11-04 11:29:01,082:INFO:             autoviz: Not installed
2024-11-04 11:29:01,082:INFO:           fairlearn: Not installed
2024-11-04 11:29:01,082:INFO:          deepchecks: Not installed
2024-11-04 11:29:01,082:INFO:             xgboost: Not installed
2024-11-04 11:29:01,082:INFO:            catboost: Not installed
2024-11-04 11:29:01,082:INFO:              kmodes: Not installed
2024-11-04 11:29:01,082:INFO:             mlxtend: Not installed
2024-11-04 11:29:01,082:INFO:       statsforecast: Not installed
2024-11-04 11:29:01,082:INFO:        tune_sklearn: Not installed
2024-11-04 11:29:01,082:INFO:                 ray: Not installed
2024-11-04 11:29:01,082:INFO:            hyperopt: Not installed
2024-11-04 11:29:01,082:INFO:              optuna: Not installed
2024-11-04 11:29:01,082:INFO:               skopt: Not installed
2024-11-04 11:29:01,082:INFO:              mlflow: Not installed
2024-11-04 11:29:01,082:INFO:              gradio: Not installed
2024-11-04 11:29:01,082:INFO:             fastapi: Not installed
2024-11-04 11:29:01,082:INFO:             uvicorn: Not installed
2024-11-04 11:29:01,082:INFO:              m2cgen: Not installed
2024-11-04 11:29:01,082:INFO:           evidently: Not installed
2024-11-04 11:29:01,082:INFO:               fugue: Not installed
2024-11-04 11:29:01,082:INFO:           streamlit: Not installed
2024-11-04 11:29:01,082:INFO:             prophet: Not installed
2024-11-04 11:29:01,082:INFO:None
2024-11-04 11:29:01,082:INFO:Set up data.
2024-11-04 11:29:01,087:INFO:Set up folding strategy.
2024-11-04 11:29:01,088:INFO:Set up train/test split.
2024-11-04 11:29:01,092:INFO:Set up index.
2024-11-04 11:29:01,092:INFO:Assigning column types.
2024-11-04 11:29:01,094:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:29:01,130:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,131:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,154:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,155:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,189:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,190:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,211:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,211:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,212:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:29:01,246:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,272:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,272:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,315:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:29:01,341:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,341:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,341:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:29:01,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,463:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,464:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,464:INFO:Preparing preprocessing pipeline...
2024-11-04 11:29:01,465:INFO:Set up simple imputation.
2024-11-04 11:29:01,467:INFO:Set up encoding of categorical features.
2024-11-04 11:29:01,467:INFO:Set up column name cleaning.
2024-11-04 11:29:01,578:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:29:01,583:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:29:01,583:INFO:Creating final display dataframe.
2024-11-04 11:29:01,836:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              28cb
2024-11-04 11:29:01,903:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,963:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,963:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:29:01,964:INFO:setup() successfully completed in 0.88s...............
2024-11-04 11:29:01,964:INFO:Initializing compare_models()
2024-11-04 11:29:01,964:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-04 11:29:01,964:INFO:Checking exceptions
2024-11-04 11:29:01,966:INFO:Preparing display monitor
2024-11-04 11:29:01,970:INFO:Initializing Logistic Regression
2024-11-04 11:29:01,970:INFO:Total runtime is 1.5695889790852864e-06 minutes
2024-11-04 11:29:01,970:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:01,970:INFO:Initializing create_model()
2024-11-04 11:29:01,970:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:01,970:INFO:Checking exceptions
2024-11-04 11:29:01,970:INFO:Importing libraries
2024-11-04 11:29:01,970:INFO:Copying training dataset
2024-11-04 11:29:01,973:INFO:Defining folds
2024-11-04 11:29:01,973:INFO:Declaring metric variables
2024-11-04 11:29:01,973:INFO:Importing untrained model
2024-11-04 11:29:01,974:INFO:Logistic Regression Imported successfully
2024-11-04 11:29:01,974:INFO:Starting cross validation
2024-11-04 11:29:01,976:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:03,933:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:03,960:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:04,258:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:04,316:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:04,336:INFO:Calculating mean and std
2024-11-04 11:29:04,337:INFO:Creating metrics dataframe
2024-11-04 11:29:04,341:INFO:Uploading results into container
2024-11-04 11:29:04,342:INFO:Uploading model into container now
2024-11-04 11:29:04,342:INFO:_master_model_container: 1
2024-11-04 11:29:04,343:INFO:_display_container: 2
2024-11-04 11:29:04,343:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-04 11:29:04,343:INFO:create_model() successfully completed......................................
2024-11-04 11:29:04,454:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:04,454:INFO:Creating metrics dataframe
2024-11-04 11:29:04,456:INFO:Initializing K Neighbors Classifier
2024-11-04 11:29:04,456:INFO:Total runtime is 0.04144270420074463 minutes
2024-11-04 11:29:04,456:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:04,457:INFO:Initializing create_model()
2024-11-04 11:29:04,457:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:04,457:INFO:Checking exceptions
2024-11-04 11:29:04,457:INFO:Importing libraries
2024-11-04 11:29:04,457:INFO:Copying training dataset
2024-11-04 11:29:04,460:INFO:Defining folds
2024-11-04 11:29:04,460:INFO:Declaring metric variables
2024-11-04 11:29:04,460:INFO:Importing untrained model
2024-11-04 11:29:04,461:INFO:K Neighbors Classifier Imported successfully
2024-11-04 11:29:04,461:INFO:Starting cross validation
2024-11-04 11:29:04,462:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:06,143:INFO:Calculating mean and std
2024-11-04 11:29:06,144:INFO:Creating metrics dataframe
2024-11-04 11:29:06,147:INFO:Uploading results into container
2024-11-04 11:29:06,147:INFO:Uploading model into container now
2024-11-04 11:29:06,148:INFO:_master_model_container: 2
2024-11-04 11:29:06,148:INFO:_display_container: 2
2024-11-04 11:29:06,148:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-04 11:29:06,148:INFO:create_model() successfully completed......................................
2024-11-04 11:29:06,248:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:06,248:INFO:Creating metrics dataframe
2024-11-04 11:29:06,250:INFO:Initializing Naive Bayes
2024-11-04 11:29:06,250:INFO:Total runtime is 0.07133907079696655 minutes
2024-11-04 11:29:06,250:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:06,250:INFO:Initializing create_model()
2024-11-04 11:29:06,250:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:06,251:INFO:Checking exceptions
2024-11-04 11:29:06,251:INFO:Importing libraries
2024-11-04 11:29:06,251:INFO:Copying training dataset
2024-11-04 11:29:06,254:INFO:Defining folds
2024-11-04 11:29:06,255:INFO:Declaring metric variables
2024-11-04 11:29:06,255:INFO:Importing untrained model
2024-11-04 11:29:06,255:INFO:Naive Bayes Imported successfully
2024-11-04 11:29:06,255:INFO:Starting cross validation
2024-11-04 11:29:06,258:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:06,396:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:06,437:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:06,439:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:06,455:INFO:Calculating mean and std
2024-11-04 11:29:06,456:INFO:Creating metrics dataframe
2024-11-04 11:29:06,457:INFO:Uploading results into container
2024-11-04 11:29:06,458:INFO:Uploading model into container now
2024-11-04 11:29:06,458:INFO:_master_model_container: 3
2024-11-04 11:29:06,458:INFO:_display_container: 2
2024-11-04 11:29:06,458:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-04 11:29:06,458:INFO:create_model() successfully completed......................................
2024-11-04 11:29:06,530:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:06,530:INFO:Creating metrics dataframe
2024-11-04 11:29:06,532:INFO:Initializing Decision Tree Classifier
2024-11-04 11:29:06,532:INFO:Total runtime is 0.0760403593381246 minutes
2024-11-04 11:29:06,532:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:06,532:INFO:Initializing create_model()
2024-11-04 11:29:06,532:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:06,532:INFO:Checking exceptions
2024-11-04 11:29:06,532:INFO:Importing libraries
2024-11-04 11:29:06,532:INFO:Copying training dataset
2024-11-04 11:29:06,535:INFO:Defining folds
2024-11-04 11:29:06,535:INFO:Declaring metric variables
2024-11-04 11:29:06,535:INFO:Importing untrained model
2024-11-04 11:29:06,535:INFO:Decision Tree Classifier Imported successfully
2024-11-04 11:29:06,536:INFO:Starting cross validation
2024-11-04 11:29:06,538:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:06,684:INFO:Calculating mean and std
2024-11-04 11:29:06,685:INFO:Creating metrics dataframe
2024-11-04 11:29:06,687:INFO:Uploading results into container
2024-11-04 11:29:06,687:INFO:Uploading model into container now
2024-11-04 11:29:06,687:INFO:_master_model_container: 4
2024-11-04 11:29:06,687:INFO:_display_container: 2
2024-11-04 11:29:06,688:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-04 11:29:06,688:INFO:create_model() successfully completed......................................
2024-11-04 11:29:06,763:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:06,763:INFO:Creating metrics dataframe
2024-11-04 11:29:06,766:INFO:Initializing SVM - Linear Kernel
2024-11-04 11:29:06,766:INFO:Total runtime is 0.07994455099105835 minutes
2024-11-04 11:29:06,767:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:06,767:INFO:Initializing create_model()
2024-11-04 11:29:06,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:06,767:INFO:Checking exceptions
2024-11-04 11:29:06,767:INFO:Importing libraries
2024-11-04 11:29:06,767:INFO:Copying training dataset
2024-11-04 11:29:06,771:INFO:Defining folds
2024-11-04 11:29:06,771:INFO:Declaring metric variables
2024-11-04 11:29:06,771:INFO:Importing untrained model
2024-11-04 11:29:06,772:INFO:SVM - Linear Kernel Imported successfully
2024-11-04 11:29:06,772:INFO:Starting cross validation
2024-11-04 11:29:06,773:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:06,922:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:06,930:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:06,951:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:06,955:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:06,956:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:06,970:INFO:Calculating mean and std
2024-11-04 11:29:06,971:INFO:Creating metrics dataframe
2024-11-04 11:29:06,972:INFO:Uploading results into container
2024-11-04 11:29:06,973:INFO:Uploading model into container now
2024-11-04 11:29:06,973:INFO:_master_model_container: 5
2024-11-04 11:29:06,973:INFO:_display_container: 2
2024-11-04 11:29:06,974:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-04 11:29:06,974:INFO:create_model() successfully completed......................................
2024-11-04 11:29:07,044:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:07,044:INFO:Creating metrics dataframe
2024-11-04 11:29:07,046:INFO:Initializing Ridge Classifier
2024-11-04 11:29:07,046:INFO:Total runtime is 0.08460923035939534 minutes
2024-11-04 11:29:07,046:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:07,047:INFO:Initializing create_model()
2024-11-04 11:29:07,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:07,047:INFO:Checking exceptions
2024-11-04 11:29:07,047:INFO:Importing libraries
2024-11-04 11:29:07,047:INFO:Copying training dataset
2024-11-04 11:29:07,049:INFO:Defining folds
2024-11-04 11:29:07,049:INFO:Declaring metric variables
2024-11-04 11:29:07,049:INFO:Importing untrained model
2024-11-04 11:29:07,050:INFO:Ridge Classifier Imported successfully
2024-11-04 11:29:07,050:INFO:Starting cross validation
2024-11-04 11:29:07,052:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:07,186:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:07,188:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:07,190:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:07,224:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:07,231:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:07,234:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:07,240:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:29:07,250:INFO:Calculating mean and std
2024-11-04 11:29:07,252:INFO:Creating metrics dataframe
2024-11-04 11:29:07,254:INFO:Uploading results into container
2024-11-04 11:29:07,254:INFO:Uploading model into container now
2024-11-04 11:29:07,255:INFO:_master_model_container: 6
2024-11-04 11:29:07,255:INFO:_display_container: 2
2024-11-04 11:29:07,255:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-04 11:29:07,255:INFO:create_model() successfully completed......................................
2024-11-04 11:29:07,328:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:07,328:INFO:Creating metrics dataframe
2024-11-04 11:29:07,330:INFO:Initializing Random Forest Classifier
2024-11-04 11:29:07,330:INFO:Total runtime is 0.08934191465377807 minutes
2024-11-04 11:29:07,330:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:07,330:INFO:Initializing create_model()
2024-11-04 11:29:07,330:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:07,331:INFO:Checking exceptions
2024-11-04 11:29:07,331:INFO:Importing libraries
2024-11-04 11:29:07,331:INFO:Copying training dataset
2024-11-04 11:29:07,333:INFO:Defining folds
2024-11-04 11:29:07,333:INFO:Declaring metric variables
2024-11-04 11:29:07,334:INFO:Importing untrained model
2024-11-04 11:29:07,334:INFO:Random Forest Classifier Imported successfully
2024-11-04 11:29:07,334:INFO:Starting cross validation
2024-11-04 11:29:07,336:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:07,816:INFO:Calculating mean and std
2024-11-04 11:29:07,816:INFO:Creating metrics dataframe
2024-11-04 11:29:07,817:INFO:Uploading results into container
2024-11-04 11:29:07,818:INFO:Uploading model into container now
2024-11-04 11:29:07,818:INFO:_master_model_container: 7
2024-11-04 11:29:07,818:INFO:_display_container: 2
2024-11-04 11:29:07,819:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-04 11:29:07,819:INFO:create_model() successfully completed......................................
2024-11-04 11:29:07,886:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:07,886:INFO:Creating metrics dataframe
2024-11-04 11:29:07,888:INFO:Initializing Quadratic Discriminant Analysis
2024-11-04 11:29:07,888:INFO:Total runtime is 0.09863728682200112 minutes
2024-11-04 11:29:07,888:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:07,888:INFO:Initializing create_model()
2024-11-04 11:29:07,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:07,888:INFO:Checking exceptions
2024-11-04 11:29:07,888:INFO:Importing libraries
2024-11-04 11:29:07,888:INFO:Copying training dataset
2024-11-04 11:29:07,891:INFO:Defining folds
2024-11-04 11:29:07,891:INFO:Declaring metric variables
2024-11-04 11:29:07,891:INFO:Importing untrained model
2024-11-04 11:29:07,891:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-04 11:29:07,891:INFO:Starting cross validation
2024-11-04 11:29:07,893:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:07,994:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:29:07,996:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:29:08,018:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:29:08,023:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:29:08,026:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,027:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,059:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,066:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,081:INFO:Calculating mean and std
2024-11-04 11:29:08,082:INFO:Creating metrics dataframe
2024-11-04 11:29:08,083:INFO:Uploading results into container
2024-11-04 11:29:08,084:INFO:Uploading model into container now
2024-11-04 11:29:08,084:INFO:_master_model_container: 8
2024-11-04 11:29:08,084:INFO:_display_container: 2
2024-11-04 11:29:08,084:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-04 11:29:08,084:INFO:create_model() successfully completed......................................
2024-11-04 11:29:08,153:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:08,153:INFO:Creating metrics dataframe
2024-11-04 11:29:08,155:INFO:Initializing Ada Boost Classifier
2024-11-04 11:29:08,155:INFO:Total runtime is 0.10308752059936523 minutes
2024-11-04 11:29:08,155:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:08,155:INFO:Initializing create_model()
2024-11-04 11:29:08,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:08,155:INFO:Checking exceptions
2024-11-04 11:29:08,156:INFO:Importing libraries
2024-11-04 11:29:08,156:INFO:Copying training dataset
2024-11-04 11:29:08,159:INFO:Defining folds
2024-11-04 11:29:08,159:INFO:Declaring metric variables
2024-11-04 11:29:08,159:INFO:Importing untrained model
2024-11-04 11:29:08,159:INFO:Ada Boost Classifier Imported successfully
2024-11-04 11:29:08,160:INFO:Starting cross validation
2024-11-04 11:29:08,162:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:08,259:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:29:08,265:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:29:08,292:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:29:08,297:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:29:08,366:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,378:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,435:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,439:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:08,452:INFO:Calculating mean and std
2024-11-04 11:29:08,452:INFO:Creating metrics dataframe
2024-11-04 11:29:08,454:INFO:Uploading results into container
2024-11-04 11:29:08,454:INFO:Uploading model into container now
2024-11-04 11:29:08,454:INFO:_master_model_container: 9
2024-11-04 11:29:08,454:INFO:_display_container: 2
2024-11-04 11:29:08,455:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-04 11:29:08,455:INFO:create_model() successfully completed......................................
2024-11-04 11:29:08,527:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:08,527:INFO:Creating metrics dataframe
2024-11-04 11:29:08,529:INFO:Initializing Gradient Boosting Classifier
2024-11-04 11:29:08,529:INFO:Total runtime is 0.10932273864746093 minutes
2024-11-04 11:29:08,529:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:08,529:INFO:Initializing create_model()
2024-11-04 11:29:08,529:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:08,529:INFO:Checking exceptions
2024-11-04 11:29:08,529:INFO:Importing libraries
2024-11-04 11:29:08,529:INFO:Copying training dataset
2024-11-04 11:29:08,532:INFO:Defining folds
2024-11-04 11:29:08,532:INFO:Declaring metric variables
2024-11-04 11:29:08,532:INFO:Importing untrained model
2024-11-04 11:29:08,532:INFO:Gradient Boosting Classifier Imported successfully
2024-11-04 11:29:08,532:INFO:Starting cross validation
2024-11-04 11:29:08,534:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:09,015:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,015:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,032:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,042:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,054:INFO:Calculating mean and std
2024-11-04 11:29:09,054:INFO:Creating metrics dataframe
2024-11-04 11:29:09,056:INFO:Uploading results into container
2024-11-04 11:29:09,056:INFO:Uploading model into container now
2024-11-04 11:29:09,056:INFO:_master_model_container: 10
2024-11-04 11:29:09,056:INFO:_display_container: 2
2024-11-04 11:29:09,057:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-04 11:29:09,057:INFO:create_model() successfully completed......................................
2024-11-04 11:29:09,127:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:09,127:INFO:Creating metrics dataframe
2024-11-04 11:29:09,130:INFO:Initializing Linear Discriminant Analysis
2024-11-04 11:29:09,130:INFO:Total runtime is 0.11933446327845255 minutes
2024-11-04 11:29:09,130:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:09,130:INFO:Initializing create_model()
2024-11-04 11:29:09,130:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:09,130:INFO:Checking exceptions
2024-11-04 11:29:09,130:INFO:Importing libraries
2024-11-04 11:29:09,130:INFO:Copying training dataset
2024-11-04 11:29:09,133:INFO:Defining folds
2024-11-04 11:29:09,133:INFO:Declaring metric variables
2024-11-04 11:29:09,133:INFO:Importing untrained model
2024-11-04 11:29:09,133:INFO:Linear Discriminant Analysis Imported successfully
2024-11-04 11:29:09,134:INFO:Starting cross validation
2024-11-04 11:29:09,135:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:09,307:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,307:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,309:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,309:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:29:09,322:INFO:Calculating mean and std
2024-11-04 11:29:09,323:INFO:Creating metrics dataframe
2024-11-04 11:29:09,324:INFO:Uploading results into container
2024-11-04 11:29:09,325:INFO:Uploading model into container now
2024-11-04 11:29:09,325:INFO:_master_model_container: 11
2024-11-04 11:29:09,326:INFO:_display_container: 2
2024-11-04 11:29:09,326:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-04 11:29:09,326:INFO:create_model() successfully completed......................................
2024-11-04 11:29:09,394:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:09,395:INFO:Creating metrics dataframe
2024-11-04 11:29:09,397:INFO:Initializing Extra Trees Classifier
2024-11-04 11:29:09,397:INFO:Total runtime is 0.12378817399342855 minutes
2024-11-04 11:29:09,397:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:09,397:INFO:Initializing create_model()
2024-11-04 11:29:09,398:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:09,398:INFO:Checking exceptions
2024-11-04 11:29:09,398:INFO:Importing libraries
2024-11-04 11:29:09,398:INFO:Copying training dataset
2024-11-04 11:29:09,401:INFO:Defining folds
2024-11-04 11:29:09,401:INFO:Declaring metric variables
2024-11-04 11:29:09,401:INFO:Importing untrained model
2024-11-04 11:29:09,402:INFO:Extra Trees Classifier Imported successfully
2024-11-04 11:29:09,402:INFO:Starting cross validation
2024-11-04 11:29:09,404:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:29:09,753:INFO:Calculating mean and std
2024-11-04 11:29:09,754:INFO:Creating metrics dataframe
2024-11-04 11:29:09,756:INFO:Uploading results into container
2024-11-04 11:29:09,756:INFO:Uploading model into container now
2024-11-04 11:29:09,757:INFO:_master_model_container: 12
2024-11-04 11:29:09,757:INFO:_display_container: 2
2024-11-04 11:29:09,757:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-04 11:29:09,757:INFO:create_model() successfully completed......................................
2024-11-04 11:29:09,824:INFO:SubProcess create_model() end ==================================
2024-11-04 11:29:09,824:INFO:Creating metrics dataframe
2024-11-04 11:29:09,826:INFO:Initializing Light Gradient Boosting Machine
2024-11-04 11:29:09,827:INFO:Total runtime is 0.1309473435084025 minutes
2024-11-04 11:29:09,827:INFO:SubProcess create_model() called ==================================
2024-11-04 11:29:09,827:INFO:Initializing create_model()
2024-11-04 11:29:09,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb465202a90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fb443c344d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:29:09,827:INFO:Checking exceptions
2024-11-04 11:29:09,827:INFO:Importing libraries
2024-11-04 11:29:09,827:INFO:Copying training dataset
2024-11-04 11:29:09,829:INFO:Defining folds
2024-11-04 11:29:09,829:INFO:Declaring metric variables
2024-11-04 11:29:09,829:INFO:Importing untrained model
2024-11-04 11:29:09,830:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-04 11:29:09,830:INFO:Starting cross validation
2024-11-04 11:29:09,831:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:37:47,821:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:37:47,822:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:37:47,822:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:37:47,822:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:37:51,058:INFO:PyCaret ClassificationExperiment
2024-11-04 11:37:51,058:INFO:Logging name: clf-default-name
2024-11-04 11:37:51,059:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:37:51,059:INFO:version 3.3.2
2024-11-04 11:37:51,059:INFO:Initializing setup()
2024-11-04 11:37:51,059:INFO:self.USI: 5cbf
2024-11-04 11:37:51,059:INFO:self._variable_keys: {'X_test', 'logging_param', 'seed', 'X', 'exp_name_log', 'target_param', 'n_jobs_param', 'X_train', 'y', 'y_train', 'fold_shuffle_param', '_ml_usecase', 'memory', 'fold_generator', 'fold_groups_param', 'fix_imbalance', 'log_plots_param', 'USI', 'idx', 'y_test', 'gpu_n_jobs_param', 'is_multiclass', 'pipeline', '_available_plots', 'exp_id', 'gpu_param', 'data', 'html_param'}
2024-11-04 11:37:51,059:INFO:Checking environment
2024-11-04 11:37:51,059:INFO:python_version: 3.11.2
2024-11-04 11:37:51,059:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:37:51,060:INFO:machine: x86_64
2024-11-04 11:37:51,060:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:37:51,060:INFO:Memory: svmem(total=16721211392, available=10133319680, percent=39.4, used=6124294144, free=3938074624, active=1965309952, inactive=9786236928, buffers=319991808, cached=6338850816, shared=114958336, slab=546045952)
2024-11-04 11:37:51,062:INFO:Physical Core: 4
2024-11-04 11:37:51,062:INFO:Logical Core: 8
2024-11-04 11:37:51,062:INFO:Checking libraries
2024-11-04 11:37:51,063:INFO:System:
2024-11-04 11:37:51,063:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:37:51,063:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:37:51,063:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:37:51,063:INFO:PyCaret required dependencies:
2024-11-04 11:37:51,088:INFO:                 pip: 23.0.1
2024-11-04 11:37:51,089:INFO:          setuptools: 66.1.1
2024-11-04 11:37:51,089:INFO:             pycaret: 3.3.2
2024-11-04 11:37:51,089:INFO:             IPython: 8.29.0
2024-11-04 11:37:51,089:INFO:          ipywidgets: 8.1.5
2024-11-04 11:37:51,089:INFO:                tqdm: 4.66.6
2024-11-04 11:37:51,089:INFO:               numpy: 1.26.4
2024-11-04 11:37:51,089:INFO:              pandas: 2.1.4
2024-11-04 11:37:51,089:INFO:              jinja2: 3.1.4
2024-11-04 11:37:51,089:INFO:               scipy: 1.11.4
2024-11-04 11:37:51,089:INFO:              joblib: 1.3.2
2024-11-04 11:37:51,089:INFO:             sklearn: 1.4.2
2024-11-04 11:37:51,089:INFO:                pyod: 2.0.2
2024-11-04 11:37:51,089:INFO:            imblearn: 0.12.4
2024-11-04 11:37:51,089:INFO:   category_encoders: 2.6.4
2024-11-04 11:37:51,089:INFO:            lightgbm: 4.5.0
2024-11-04 11:37:51,089:INFO:               numba: 0.60.0
2024-11-04 11:37:51,089:INFO:            requests: 2.32.3
2024-11-04 11:37:51,089:INFO:          matplotlib: 3.7.5
2024-11-04 11:37:51,089:INFO:          scikitplot: 0.3.7
2024-11-04 11:37:51,089:INFO:         yellowbrick: 1.5
2024-11-04 11:37:51,089:INFO:              plotly: 5.24.1
2024-11-04 11:37:51,089:INFO:    plotly-resampler: Not installed
2024-11-04 11:37:51,089:INFO:             kaleido: 0.2.1
2024-11-04 11:37:51,089:INFO:           schemdraw: 0.15
2024-11-04 11:37:51,089:INFO:         statsmodels: 0.14.4
2024-11-04 11:37:51,089:INFO:              sktime: 0.26.0
2024-11-04 11:37:51,089:INFO:               tbats: 1.1.3
2024-11-04 11:37:51,090:INFO:            pmdarima: 2.0.4
2024-11-04 11:37:51,090:INFO:              psutil: 6.1.0
2024-11-04 11:37:51,090:INFO:          markupsafe: 3.0.2
2024-11-04 11:37:51,090:INFO:             pickle5: Not installed
2024-11-04 11:37:51,090:INFO:         cloudpickle: 3.1.0
2024-11-04 11:37:51,090:INFO:         deprecation: 2.1.0
2024-11-04 11:37:51,090:INFO:              xxhash: 3.5.0
2024-11-04 11:37:51,090:INFO:           wurlitzer: 3.1.1
2024-11-04 11:37:51,090:INFO:PyCaret optional dependencies:
2024-11-04 11:37:51,107:INFO:                shap: Not installed
2024-11-04 11:37:51,107:INFO:           interpret: Not installed
2024-11-04 11:37:51,107:INFO:                umap: Not installed
2024-11-04 11:37:51,107:INFO:     ydata_profiling: Not installed
2024-11-04 11:37:51,107:INFO:  explainerdashboard: Not installed
2024-11-04 11:37:51,107:INFO:             autoviz: Not installed
2024-11-04 11:37:51,107:INFO:           fairlearn: Not installed
2024-11-04 11:37:51,108:INFO:          deepchecks: Not installed
2024-11-04 11:37:51,108:INFO:             xgboost: Not installed
2024-11-04 11:37:51,108:INFO:            catboost: Not installed
2024-11-04 11:37:51,108:INFO:              kmodes: Not installed
2024-11-04 11:37:51,108:INFO:             mlxtend: Not installed
2024-11-04 11:37:51,108:INFO:       statsforecast: Not installed
2024-11-04 11:37:51,108:INFO:        tune_sklearn: Not installed
2024-11-04 11:37:51,108:INFO:                 ray: Not installed
2024-11-04 11:37:51,108:INFO:            hyperopt: Not installed
2024-11-04 11:37:51,108:INFO:              optuna: Not installed
2024-11-04 11:37:51,108:INFO:               skopt: Not installed
2024-11-04 11:37:51,108:INFO:              mlflow: Not installed
2024-11-04 11:37:51,108:INFO:              gradio: Not installed
2024-11-04 11:37:51,108:INFO:             fastapi: Not installed
2024-11-04 11:37:51,108:INFO:             uvicorn: Not installed
2024-11-04 11:37:51,108:INFO:              m2cgen: Not installed
2024-11-04 11:37:51,108:INFO:           evidently: Not installed
2024-11-04 11:37:51,108:INFO:               fugue: Not installed
2024-11-04 11:37:51,108:INFO:           streamlit: Not installed
2024-11-04 11:37:51,108:INFO:             prophet: Not installed
2024-11-04 11:37:51,108:INFO:None
2024-11-04 11:37:51,108:INFO:Set up data.
2024-11-04 11:37:51,117:INFO:Set up folding strategy.
2024-11-04 11:37:51,118:INFO:Set up train/test split.
2024-11-04 11:37:51,124:INFO:Set up index.
2024-11-04 11:37:51,124:INFO:Assigning column types.
2024-11-04 11:37:51,128:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:37:51,170:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,172:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,199:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,200:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,246:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,247:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,269:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,270:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,270:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:37:51,306:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,326:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,326:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,362:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:37:51,384:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,384:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:37:51,443:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,443:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,501:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,501:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,503:INFO:Preparing preprocessing pipeline...
2024-11-04 11:37:51,503:INFO:Set up simple imputation.
2024-11-04 11:37:51,506:INFO:Set up encoding of categorical features.
2024-11-04 11:37:51,506:INFO:Set up column name cleaning.
2024-11-04 11:37:51,622:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:37:51,627:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:37:51,627:INFO:Creating final display dataframe.
2024-11-04 11:37:51,878:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                 4
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              5cbf
2024-11-04 11:37:51,948:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:51,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:52,012:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:52,012:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:37:52,013:INFO:setup() successfully completed in 0.96s...............
2024-11-04 11:38:18,929:INFO:PyCaret ClassificationExperiment
2024-11-04 11:38:18,929:INFO:Logging name: clf-default-name
2024-11-04 11:38:18,930:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-04 11:38:18,930:INFO:version 3.3.2
2024-11-04 11:38:18,930:INFO:Initializing setup()
2024-11-04 11:38:18,930:INFO:self.USI: 0d1f
2024-11-04 11:38:18,930:INFO:self._variable_keys: {'X_test', 'logging_param', 'seed', 'X', 'exp_name_log', 'target_param', 'n_jobs_param', 'X_train', 'y', 'y_train', 'fold_shuffle_param', '_ml_usecase', 'memory', 'fold_generator', 'fold_groups_param', 'fix_imbalance', 'log_plots_param', 'USI', 'idx', 'y_test', 'gpu_n_jobs_param', 'is_multiclass', 'pipeline', '_available_plots', 'exp_id', 'gpu_param', 'data', 'html_param'}
2024-11-04 11:38:18,931:INFO:Checking environment
2024-11-04 11:38:18,931:INFO:python_version: 3.11.2
2024-11-04 11:38:18,931:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-04 11:38:18,931:INFO:machine: x86_64
2024-11-04 11:38:18,931:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:38:18,932:INFO:Memory: svmem(total=16721211392, available=10153107456, percent=39.3, used=6105149440, free=3957821440, active=1965404160, inactive=9766260736, buffers=320090112, cached=6338150400, shared=114315264, slab=545935360)
2024-11-04 11:38:18,934:INFO:Physical Core: 4
2024-11-04 11:38:18,934:INFO:Logical Core: 8
2024-11-04 11:38:18,934:INFO:Checking libraries
2024-11-04 11:38:18,934:INFO:System:
2024-11-04 11:38:18,935:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-04 11:38:18,935:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-04 11:38:18,935:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-04 11:38:18,935:INFO:PyCaret required dependencies:
2024-11-04 11:38:18,935:INFO:                 pip: 23.0.1
2024-11-04 11:38:18,935:INFO:          setuptools: 66.1.1
2024-11-04 11:38:18,935:INFO:             pycaret: 3.3.2
2024-11-04 11:38:18,935:INFO:             IPython: 8.29.0
2024-11-04 11:38:18,936:INFO:          ipywidgets: 8.1.5
2024-11-04 11:38:18,936:INFO:                tqdm: 4.66.6
2024-11-04 11:38:18,936:INFO:               numpy: 1.26.4
2024-11-04 11:38:18,936:INFO:              pandas: 2.1.4
2024-11-04 11:38:18,936:INFO:              jinja2: 3.1.4
2024-11-04 11:38:18,936:INFO:               scipy: 1.11.4
2024-11-04 11:38:18,936:INFO:              joblib: 1.3.2
2024-11-04 11:38:18,936:INFO:             sklearn: 1.4.2
2024-11-04 11:38:18,936:INFO:                pyod: 2.0.2
2024-11-04 11:38:18,936:INFO:            imblearn: 0.12.4
2024-11-04 11:38:18,937:INFO:   category_encoders: 2.6.4
2024-11-04 11:38:18,937:INFO:            lightgbm: 4.5.0
2024-11-04 11:38:18,937:INFO:               numba: 0.60.0
2024-11-04 11:38:18,937:INFO:            requests: 2.32.3
2024-11-04 11:38:18,937:INFO:          matplotlib: 3.7.5
2024-11-04 11:38:18,937:INFO:          scikitplot: 0.3.7
2024-11-04 11:38:18,937:INFO:         yellowbrick: 1.5
2024-11-04 11:38:18,937:INFO:              plotly: 5.24.1
2024-11-04 11:38:18,937:INFO:    plotly-resampler: Not installed
2024-11-04 11:38:18,938:INFO:             kaleido: 0.2.1
2024-11-04 11:38:18,938:INFO:           schemdraw: 0.15
2024-11-04 11:38:18,938:INFO:         statsmodels: 0.14.4
2024-11-04 11:38:18,938:INFO:              sktime: 0.26.0
2024-11-04 11:38:18,938:INFO:               tbats: 1.1.3
2024-11-04 11:38:18,938:INFO:            pmdarima: 2.0.4
2024-11-04 11:38:18,938:INFO:              psutil: 6.1.0
2024-11-04 11:38:18,938:INFO:          markupsafe: 3.0.2
2024-11-04 11:38:18,938:INFO:             pickle5: Not installed
2024-11-04 11:38:18,938:INFO:         cloudpickle: 3.1.0
2024-11-04 11:38:18,939:INFO:         deprecation: 2.1.0
2024-11-04 11:38:18,939:INFO:              xxhash: 3.5.0
2024-11-04 11:38:18,939:INFO:           wurlitzer: 3.1.1
2024-11-04 11:38:18,939:INFO:PyCaret optional dependencies:
2024-11-04 11:38:18,939:INFO:                shap: Not installed
2024-11-04 11:38:18,939:INFO:           interpret: Not installed
2024-11-04 11:38:18,939:INFO:                umap: Not installed
2024-11-04 11:38:18,939:INFO:     ydata_profiling: Not installed
2024-11-04 11:38:18,939:INFO:  explainerdashboard: Not installed
2024-11-04 11:38:18,940:INFO:             autoviz: Not installed
2024-11-04 11:38:18,940:INFO:           fairlearn: Not installed
2024-11-04 11:38:18,940:INFO:          deepchecks: Not installed
2024-11-04 11:38:18,940:INFO:             xgboost: Not installed
2024-11-04 11:38:18,940:INFO:            catboost: Not installed
2024-11-04 11:38:18,940:INFO:              kmodes: Not installed
2024-11-04 11:38:18,940:INFO:             mlxtend: Not installed
2024-11-04 11:38:18,941:INFO:       statsforecast: Not installed
2024-11-04 11:38:18,941:INFO:        tune_sklearn: Not installed
2024-11-04 11:38:18,941:INFO:                 ray: Not installed
2024-11-04 11:38:18,941:INFO:            hyperopt: Not installed
2024-11-04 11:38:18,941:INFO:              optuna: Not installed
2024-11-04 11:38:18,941:INFO:               skopt: Not installed
2024-11-04 11:38:18,941:INFO:              mlflow: Not installed
2024-11-04 11:38:18,942:INFO:              gradio: Not installed
2024-11-04 11:38:18,942:INFO:             fastapi: Not installed
2024-11-04 11:38:18,942:INFO:             uvicorn: Not installed
2024-11-04 11:38:18,942:INFO:              m2cgen: Not installed
2024-11-04 11:38:18,942:INFO:           evidently: Not installed
2024-11-04 11:38:18,942:INFO:               fugue: Not installed
2024-11-04 11:38:18,942:INFO:           streamlit: Not installed
2024-11-04 11:38:18,942:INFO:             prophet: Not installed
2024-11-04 11:38:18,942:INFO:None
2024-11-04 11:38:18,942:INFO:Set up data.
2024-11-04 11:38:18,958:INFO:Set up folding strategy.
2024-11-04 11:38:18,958:INFO:Set up train/test split.
2024-11-04 11:38:18,964:INFO:Set up index.
2024-11-04 11:38:18,965:INFO:Assigning column types.
2024-11-04 11:38:18,970:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-04 11:38:19,033:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,034:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,059:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,059:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,100:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,101:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,129:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,129:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,130:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-04 11:38:19,164:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,186:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,186:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-04 11:38:19,239:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,239:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,239:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-04 11:38:19,293:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,293:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,347:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,348:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,348:INFO:Preparing preprocessing pipeline...
2024-11-04 11:38:19,349:INFO:Set up simple imputation.
2024-11-04 11:38:19,351:INFO:Set up encoding of categorical features.
2024-11-04 11:38:19,352:INFO:Set up column name cleaning.
2024-11-04 11:38:19,466:INFO:Finished creating preprocessing pipeline.
2024-11-04 11:38:19,472:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-04 11:38:19,472:INFO:Creating final display dataframe.
2024-11-04 11:38:19,726:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 10)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7          Categorical features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12     Maximum one-hot encoding                25
13              Encoding method              None
14               Fold Generator   StratifiedKFold
15                  Fold Number                10
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0d1f
2024-11-04 11:38:19,797:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,798:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,854:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,854:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-04 11:38:19,855:INFO:setup() successfully completed in 0.93s...............
2024-11-04 11:38:26,957:INFO:Initializing compare_models()
2024-11-04 11:38:26,958:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-04 11:38:26,958:INFO:Checking exceptions
2024-11-04 11:38:26,968:INFO:Preparing display monitor
2024-11-04 11:38:26,999:INFO:Initializing Logistic Regression
2024-11-04 11:38:26,999:INFO:Total runtime is 2.86102294921875e-06 minutes
2024-11-04 11:38:27,002:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:27,003:INFO:Initializing create_model()
2024-11-04 11:38:27,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:27,003:INFO:Checking exceptions
2024-11-04 11:38:27,003:INFO:Importing libraries
2024-11-04 11:38:27,003:INFO:Copying training dataset
2024-11-04 11:38:27,008:INFO:Defining folds
2024-11-04 11:38:27,008:INFO:Declaring metric variables
2024-11-04 11:38:27,011:INFO:Importing untrained model
2024-11-04 11:38:27,015:INFO:Logistic Regression Imported successfully
2024-11-04 11:38:27,024:INFO:Starting cross validation
2024-11-04 11:38:27,028:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:29,061:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:29,199:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:29,262:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:29,375:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:29,386:INFO:Calculating mean and std
2024-11-04 11:38:29,387:INFO:Creating metrics dataframe
2024-11-04 11:38:29,391:INFO:Uploading results into container
2024-11-04 11:38:29,392:INFO:Uploading model into container now
2024-11-04 11:38:29,392:INFO:_master_model_container: 1
2024-11-04 11:38:29,392:INFO:_display_container: 2
2024-11-04 11:38:29,393:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-04 11:38:29,393:INFO:create_model() successfully completed......................................
2024-11-04 11:38:29,508:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:29,508:INFO:Creating metrics dataframe
2024-11-04 11:38:29,515:INFO:Initializing K Neighbors Classifier
2024-11-04 11:38:29,515:INFO:Total runtime is 0.041946160793304446 minutes
2024-11-04 11:38:29,518:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:29,519:INFO:Initializing create_model()
2024-11-04 11:38:29,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:29,519:INFO:Checking exceptions
2024-11-04 11:38:29,519:INFO:Importing libraries
2024-11-04 11:38:29,519:INFO:Copying training dataset
2024-11-04 11:38:29,525:INFO:Defining folds
2024-11-04 11:38:29,525:INFO:Declaring metric variables
2024-11-04 11:38:29,529:INFO:Importing untrained model
2024-11-04 11:38:29,532:INFO:K Neighbors Classifier Imported successfully
2024-11-04 11:38:29,541:INFO:Starting cross validation
2024-11-04 11:38:29,544:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:31,347:INFO:Calculating mean and std
2024-11-04 11:38:31,348:INFO:Creating metrics dataframe
2024-11-04 11:38:31,350:INFO:Uploading results into container
2024-11-04 11:38:31,351:INFO:Uploading model into container now
2024-11-04 11:38:31,351:INFO:_master_model_container: 2
2024-11-04 11:38:31,351:INFO:_display_container: 2
2024-11-04 11:38:31,351:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-04 11:38:31,352:INFO:create_model() successfully completed......................................
2024-11-04 11:38:31,440:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:31,440:INFO:Creating metrics dataframe
2024-11-04 11:38:31,446:INFO:Initializing Naive Bayes
2024-11-04 11:38:31,446:INFO:Total runtime is 0.07412230173746745 minutes
2024-11-04 11:38:31,450:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:31,450:INFO:Initializing create_model()
2024-11-04 11:38:31,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:31,450:INFO:Checking exceptions
2024-11-04 11:38:31,450:INFO:Importing libraries
2024-11-04 11:38:31,450:INFO:Copying training dataset
2024-11-04 11:38:31,455:INFO:Defining folds
2024-11-04 11:38:31,455:INFO:Declaring metric variables
2024-11-04 11:38:31,459:INFO:Importing untrained model
2024-11-04 11:38:31,462:INFO:Naive Bayes Imported successfully
2024-11-04 11:38:31,470:INFO:Starting cross validation
2024-11-04 11:38:31,474:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:31,617:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:31,620:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:31,660:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:31,673:INFO:Calculating mean and std
2024-11-04 11:38:31,674:INFO:Creating metrics dataframe
2024-11-04 11:38:31,677:INFO:Uploading results into container
2024-11-04 11:38:31,678:INFO:Uploading model into container now
2024-11-04 11:38:31,679:INFO:_master_model_container: 3
2024-11-04 11:38:31,679:INFO:_display_container: 2
2024-11-04 11:38:31,679:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-04 11:38:31,679:INFO:create_model() successfully completed......................................
2024-11-04 11:38:31,755:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:31,755:INFO:Creating metrics dataframe
2024-11-04 11:38:31,763:INFO:Initializing Decision Tree Classifier
2024-11-04 11:38:31,764:INFO:Total runtime is 0.07941530148188274 minutes
2024-11-04 11:38:31,766:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:31,767:INFO:Initializing create_model()
2024-11-04 11:38:31,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:31,767:INFO:Checking exceptions
2024-11-04 11:38:31,767:INFO:Importing libraries
2024-11-04 11:38:31,767:INFO:Copying training dataset
2024-11-04 11:38:31,770:INFO:Defining folds
2024-11-04 11:38:31,770:INFO:Declaring metric variables
2024-11-04 11:38:31,774:INFO:Importing untrained model
2024-11-04 11:38:31,778:INFO:Decision Tree Classifier Imported successfully
2024-11-04 11:38:31,784:INFO:Starting cross validation
2024-11-04 11:38:31,787:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:31,977:INFO:Calculating mean and std
2024-11-04 11:38:31,977:INFO:Creating metrics dataframe
2024-11-04 11:38:31,979:INFO:Uploading results into container
2024-11-04 11:38:31,979:INFO:Uploading model into container now
2024-11-04 11:38:31,980:INFO:_master_model_container: 4
2024-11-04 11:38:31,980:INFO:_display_container: 2
2024-11-04 11:38:31,980:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-04 11:38:31,980:INFO:create_model() successfully completed......................................
2024-11-04 11:38:32,055:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:32,055:INFO:Creating metrics dataframe
2024-11-04 11:38:32,061:INFO:Initializing SVM - Linear Kernel
2024-11-04 11:38:32,061:INFO:Total runtime is 0.08437683979670207 minutes
2024-11-04 11:38:32,065:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:32,065:INFO:Initializing create_model()
2024-11-04 11:38:32,065:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:32,066:INFO:Checking exceptions
2024-11-04 11:38:32,066:INFO:Importing libraries
2024-11-04 11:38:32,066:INFO:Copying training dataset
2024-11-04 11:38:32,070:INFO:Defining folds
2024-11-04 11:38:32,070:INFO:Declaring metric variables
2024-11-04 11:38:32,073:INFO:Importing untrained model
2024-11-04 11:38:32,077:INFO:SVM - Linear Kernel Imported successfully
2024-11-04 11:38:32,085:INFO:Starting cross validation
2024-11-04 11:38:32,087:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:32,238:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,241:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,244:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,245:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:32,255:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,266:INFO:Calculating mean and std
2024-11-04 11:38:32,268:INFO:Creating metrics dataframe
2024-11-04 11:38:32,270:INFO:Uploading results into container
2024-11-04 11:38:32,271:INFO:Uploading model into container now
2024-11-04 11:38:32,271:INFO:_master_model_container: 5
2024-11-04 11:38:32,271:INFO:_display_container: 2
2024-11-04 11:38:32,272:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-04 11:38:32,272:INFO:create_model() successfully completed......................................
2024-11-04 11:38:32,348:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:32,349:INFO:Creating metrics dataframe
2024-11-04 11:38:32,355:INFO:Initializing Ridge Classifier
2024-11-04 11:38:32,355:INFO:Total runtime is 0.08928008476893108 minutes
2024-11-04 11:38:32,358:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:32,359:INFO:Initializing create_model()
2024-11-04 11:38:32,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:32,359:INFO:Checking exceptions
2024-11-04 11:38:32,359:INFO:Importing libraries
2024-11-04 11:38:32,359:INFO:Copying training dataset
2024-11-04 11:38:32,363:INFO:Defining folds
2024-11-04 11:38:32,363:INFO:Declaring metric variables
2024-11-04 11:38:32,366:INFO:Importing untrained model
2024-11-04 11:38:32,369:INFO:Ridge Classifier Imported successfully
2024-11-04 11:38:32,377:INFO:Starting cross validation
2024-11-04 11:38:32,379:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:32,511:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,512:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,517:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:32,549:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,552:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:32,556:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:32,558:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:38:32,566:INFO:Calculating mean and std
2024-11-04 11:38:32,567:INFO:Creating metrics dataframe
2024-11-04 11:38:32,570:INFO:Uploading results into container
2024-11-04 11:38:32,570:INFO:Uploading model into container now
2024-11-04 11:38:32,570:INFO:_master_model_container: 6
2024-11-04 11:38:32,570:INFO:_display_container: 2
2024-11-04 11:38:32,570:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-04 11:38:32,571:INFO:create_model() successfully completed......................................
2024-11-04 11:38:32,643:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:32,644:INFO:Creating metrics dataframe
2024-11-04 11:38:32,651:INFO:Initializing Random Forest Classifier
2024-11-04 11:38:32,651:INFO:Total runtime is 0.09420786301294963 minutes
2024-11-04 11:38:32,654:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:32,655:INFO:Initializing create_model()
2024-11-04 11:38:32,655:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:32,655:INFO:Checking exceptions
2024-11-04 11:38:32,655:INFO:Importing libraries
2024-11-04 11:38:32,655:INFO:Copying training dataset
2024-11-04 11:38:32,658:INFO:Defining folds
2024-11-04 11:38:32,658:INFO:Declaring metric variables
2024-11-04 11:38:32,661:INFO:Importing untrained model
2024-11-04 11:38:32,664:INFO:Random Forest Classifier Imported successfully
2024-11-04 11:38:32,671:INFO:Starting cross validation
2024-11-04 11:38:32,675:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:33,126:INFO:Calculating mean and std
2024-11-04 11:38:33,127:INFO:Creating metrics dataframe
2024-11-04 11:38:33,129:INFO:Uploading results into container
2024-11-04 11:38:33,129:INFO:Uploading model into container now
2024-11-04 11:38:33,129:INFO:_master_model_container: 7
2024-11-04 11:38:33,129:INFO:_display_container: 2
2024-11-04 11:38:33,130:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-04 11:38:33,130:INFO:create_model() successfully completed......................................
2024-11-04 11:38:33,204:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:33,205:INFO:Creating metrics dataframe
2024-11-04 11:38:33,213:INFO:Initializing Quadratic Discriminant Analysis
2024-11-04 11:38:33,213:INFO:Total runtime is 0.10356875658035279 minutes
2024-11-04 11:38:33,215:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:33,216:INFO:Initializing create_model()
2024-11-04 11:38:33,216:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:33,216:INFO:Checking exceptions
2024-11-04 11:38:33,216:INFO:Importing libraries
2024-11-04 11:38:33,216:INFO:Copying training dataset
2024-11-04 11:38:33,219:INFO:Defining folds
2024-11-04 11:38:33,219:INFO:Declaring metric variables
2024-11-04 11:38:33,222:INFO:Importing untrained model
2024-11-04 11:38:33,226:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-04 11:38:33,233:INFO:Starting cross validation
2024-11-04 11:38:33,237:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:33,351:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:38:33,356:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:38:33,376:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:38:33,379:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-04 11:38:33,389:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,396:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,420:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,422:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,438:INFO:Calculating mean and std
2024-11-04 11:38:33,438:INFO:Creating metrics dataframe
2024-11-04 11:38:33,441:INFO:Uploading results into container
2024-11-04 11:38:33,442:INFO:Uploading model into container now
2024-11-04 11:38:33,442:INFO:_master_model_container: 8
2024-11-04 11:38:33,442:INFO:_display_container: 2
2024-11-04 11:38:33,442:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-04 11:38:33,442:INFO:create_model() successfully completed......................................
2024-11-04 11:38:33,531:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:33,531:INFO:Creating metrics dataframe
2024-11-04 11:38:33,539:INFO:Initializing Ada Boost Classifier
2024-11-04 11:38:33,539:INFO:Total runtime is 0.10901459058125815 minutes
2024-11-04 11:38:33,542:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:33,543:INFO:Initializing create_model()
2024-11-04 11:38:33,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:33,543:INFO:Checking exceptions
2024-11-04 11:38:33,543:INFO:Importing libraries
2024-11-04 11:38:33,543:INFO:Copying training dataset
2024-11-04 11:38:33,546:INFO:Defining folds
2024-11-04 11:38:33,546:INFO:Declaring metric variables
2024-11-04 11:38:33,551:INFO:Importing untrained model
2024-11-04 11:38:33,555:INFO:Ada Boost Classifier Imported successfully
2024-11-04 11:38:33,562:INFO:Starting cross validation
2024-11-04 11:38:33,565:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:33,687:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:38:33,696:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:38:33,697:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:38:33,699:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-04 11:38:33,794:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,809:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,811:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,811:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:33,825:INFO:Calculating mean and std
2024-11-04 11:38:33,826:INFO:Creating metrics dataframe
2024-11-04 11:38:33,828:INFO:Uploading results into container
2024-11-04 11:38:33,828:INFO:Uploading model into container now
2024-11-04 11:38:33,829:INFO:_master_model_container: 9
2024-11-04 11:38:33,829:INFO:_display_container: 2
2024-11-04 11:38:33,829:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-04 11:38:33,829:INFO:create_model() successfully completed......................................
2024-11-04 11:38:33,909:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:33,909:INFO:Creating metrics dataframe
2024-11-04 11:38:33,918:INFO:Initializing Gradient Boosting Classifier
2024-11-04 11:38:33,918:INFO:Total runtime is 0.11532211303710939 minutes
2024-11-04 11:38:33,922:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:33,922:INFO:Initializing create_model()
2024-11-04 11:38:33,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:33,922:INFO:Checking exceptions
2024-11-04 11:38:33,922:INFO:Importing libraries
2024-11-04 11:38:33,922:INFO:Copying training dataset
2024-11-04 11:38:33,925:INFO:Defining folds
2024-11-04 11:38:33,925:INFO:Declaring metric variables
2024-11-04 11:38:33,928:INFO:Importing untrained model
2024-11-04 11:38:33,933:INFO:Gradient Boosting Classifier Imported successfully
2024-11-04 11:38:33,942:INFO:Starting cross validation
2024-11-04 11:38:33,945:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:34,465:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,479:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,613:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,621:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,637:INFO:Calculating mean and std
2024-11-04 11:38:34,638:INFO:Creating metrics dataframe
2024-11-04 11:38:34,640:INFO:Uploading results into container
2024-11-04 11:38:34,641:INFO:Uploading model into container now
2024-11-04 11:38:34,641:INFO:_master_model_container: 10
2024-11-04 11:38:34,641:INFO:_display_container: 2
2024-11-04 11:38:34,642:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-04 11:38:34,642:INFO:create_model() successfully completed......................................
2024-11-04 11:38:34,716:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:34,716:INFO:Creating metrics dataframe
2024-11-04 11:38:34,725:INFO:Initializing Linear Discriminant Analysis
2024-11-04 11:38:34,725:INFO:Total runtime is 0.12876780430475873 minutes
2024-11-04 11:38:34,728:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:34,729:INFO:Initializing create_model()
2024-11-04 11:38:34,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:34,729:INFO:Checking exceptions
2024-11-04 11:38:34,729:INFO:Importing libraries
2024-11-04 11:38:34,729:INFO:Copying training dataset
2024-11-04 11:38:34,733:INFO:Defining folds
2024-11-04 11:38:34,733:INFO:Declaring metric variables
2024-11-04 11:38:34,738:INFO:Importing untrained model
2024-11-04 11:38:34,741:INFO:Linear Discriminant Analysis Imported successfully
2024-11-04 11:38:34,752:INFO:Starting cross validation
2024-11-04 11:38:34,755:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:34,890:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,912:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,930:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,934:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-04 11:38:34,955:INFO:Calculating mean and std
2024-11-04 11:38:34,957:INFO:Creating metrics dataframe
2024-11-04 11:38:34,964:INFO:Uploading results into container
2024-11-04 11:38:34,965:INFO:Uploading model into container now
2024-11-04 11:38:34,966:INFO:_master_model_container: 11
2024-11-04 11:38:34,967:INFO:_display_container: 2
2024-11-04 11:38:34,967:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-04 11:38:34,968:INFO:create_model() successfully completed......................................
2024-11-04 11:38:35,084:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:35,085:INFO:Creating metrics dataframe
2024-11-04 11:38:35,096:INFO:Initializing Extra Trees Classifier
2024-11-04 11:38:35,096:INFO:Total runtime is 0.134952453772227 minutes
2024-11-04 11:38:35,099:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:35,100:INFO:Initializing create_model()
2024-11-04 11:38:35,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:35,100:INFO:Checking exceptions
2024-11-04 11:38:35,100:INFO:Importing libraries
2024-11-04 11:38:35,100:INFO:Copying training dataset
2024-11-04 11:38:35,104:INFO:Defining folds
2024-11-04 11:38:35,105:INFO:Declaring metric variables
2024-11-04 11:38:35,107:INFO:Importing untrained model
2024-11-04 11:38:35,111:INFO:Extra Trees Classifier Imported successfully
2024-11-04 11:38:35,119:INFO:Starting cross validation
2024-11-04 11:38:35,122:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:38:35,512:INFO:Calculating mean and std
2024-11-04 11:38:35,512:INFO:Creating metrics dataframe
2024-11-04 11:38:35,514:INFO:Uploading results into container
2024-11-04 11:38:35,515:INFO:Uploading model into container now
2024-11-04 11:38:35,515:INFO:_master_model_container: 12
2024-11-04 11:38:35,516:INFO:_display_container: 2
2024-11-04 11:38:35,516:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-04 11:38:35,516:INFO:create_model() successfully completed......................................
2024-11-04 11:38:35,591:INFO:SubProcess create_model() end ==================================
2024-11-04 11:38:35,591:INFO:Creating metrics dataframe
2024-11-04 11:38:35,598:INFO:Initializing Light Gradient Boosting Machine
2024-11-04 11:38:35,598:INFO:Total runtime is 0.14332181215286258 minutes
2024-11-04 11:38:35,602:INFO:SubProcess create_model() called ==================================
2024-11-04 11:38:35,602:INFO:Initializing create_model()
2024-11-04 11:38:35,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:38:35,602:INFO:Checking exceptions
2024-11-04 11:38:35,602:INFO:Importing libraries
2024-11-04 11:38:35,602:INFO:Copying training dataset
2024-11-04 11:38:35,606:INFO:Defining folds
2024-11-04 11:38:35,606:INFO:Declaring metric variables
2024-11-04 11:38:35,608:INFO:Importing untrained model
2024-11-04 11:38:35,611:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-04 11:38:35,618:INFO:Starting cross validation
2024-11-04 11:38:35,621:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:48:02,819:INFO:Calculating mean and std
2024-11-04 11:48:02,820:INFO:Creating metrics dataframe
2024-11-04 11:48:02,822:INFO:Uploading results into container
2024-11-04 11:48:02,823:INFO:Uploading model into container now
2024-11-04 11:48:02,823:INFO:_master_model_container: 13
2024-11-04 11:48:02,823:INFO:_display_container: 2
2024-11-04 11:48:02,824:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-04 11:48:02,824:INFO:create_model() successfully completed......................................
2024-11-04 11:48:02,913:INFO:SubProcess create_model() end ==================================
2024-11-04 11:48:02,913:INFO:Creating metrics dataframe
2024-11-04 11:48:02,924:INFO:Initializing Dummy Classifier
2024-11-04 11:48:02,924:INFO:Total runtime is 9.59874997138977 minutes
2024-11-04 11:48:02,926:INFO:SubProcess create_model() called ==================================
2024-11-04 11:48:02,926:INFO:Initializing create_model()
2024-11-04 11:48:02,926:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f307f774950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:48:02,926:INFO:Checking exceptions
2024-11-04 11:48:02,927:INFO:Importing libraries
2024-11-04 11:48:02,927:INFO:Copying training dataset
2024-11-04 11:48:02,931:INFO:Defining folds
2024-11-04 11:48:02,931:INFO:Declaring metric variables
2024-11-04 11:48:02,934:INFO:Importing untrained model
2024-11-04 11:48:02,939:INFO:Dummy Classifier Imported successfully
2024-11-04 11:48:02,945:INFO:Starting cross validation
2024-11-04 11:48:02,949:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-04 11:48:03,163:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:48:03,303:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:48:03,309:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:48:03,315:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-04 11:48:03,328:INFO:Calculating mean and std
2024-11-04 11:48:03,330:INFO:Creating metrics dataframe
2024-11-04 11:48:03,332:INFO:Uploading results into container
2024-11-04 11:48:03,333:INFO:Uploading model into container now
2024-11-04 11:48:03,333:INFO:_master_model_container: 14
2024-11-04 11:48:03,333:INFO:_display_container: 2
2024-11-04 11:48:03,334:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-04 11:48:03,334:INFO:create_model() successfully completed......................................
2024-11-04 11:48:03,448:INFO:SubProcess create_model() end ==================================
2024-11-04 11:48:03,448:INFO:Creating metrics dataframe
2024-11-04 11:48:03,461:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-04 11:48:03,472:INFO:Initializing create_model()
2024-11-04 11:48:03,472:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-04 11:48:03,472:INFO:Checking exceptions
2024-11-04 11:48:03,474:INFO:Importing libraries
2024-11-04 11:48:03,474:INFO:Copying training dataset
2024-11-04 11:48:03,480:INFO:Defining folds
2024-11-04 11:48:03,480:INFO:Declaring metric variables
2024-11-04 11:48:03,480:INFO:Importing untrained model
2024-11-04 11:48:03,480:INFO:Declaring custom model
2024-11-04 11:48:03,481:INFO:Ridge Classifier Imported successfully
2024-11-04 11:48:03,483:INFO:Cross validation set to False
2024-11-04 11:48:03,484:INFO:Fitting Model
2024-11-04 11:48:03,575:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-04 11:48:03,575:INFO:create_model() successfully completed......................................
2024-11-04 11:48:03,692:INFO:_master_model_container: 14
2024-11-04 11:48:03,692:INFO:_display_container: 2
2024-11-04 11:48:03,692:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-04 11:48:03,692:INFO:compare_models() successfully completed......................................
2024-11-04 11:57:54,426:INFO:Initializing save_model()
2024-11-04 11:57:54,426:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo_agotamiento, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-04 11:57:54,426:INFO:Adding model into prep_pipe
2024-11-04 11:57:54,443:INFO:modelo_agotamiento.pkl saved in current working directory
2024-11-04 11:57:54,449:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-04 11:57:54,449:INFO:save_model() successfully completed......................................
2024-11-04 11:58:26,507:INFO:Initializing save_model()
2024-11-04 11:58:26,508:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-04 11:58:26,508:INFO:Adding model into prep_pipe
2024-11-04 11:58:26,527:INFO:modelo-SA45-ECE.pkl saved in current working directory
2024-11-04 11:58:26,535:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio...
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-04 11:58:26,535:INFO:save_model() successfully completed......................................
2024-11-04 11:59:59,780:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:59:59,780:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:59:59,780:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 11:59:59,780:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:00:00,218:INFO:Initializing load_model()
2024-11-04 12:00:00,218:INFO:load_model(model_name=modelo_agotamiento, platform=None, authentication=None, verbose=True)
2024-11-04 12:00:21,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:00:21,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:00:21,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:00:21,221:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:00:21,701:INFO:Initializing load_model()
2024-11-04 12:00:21,701:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:00:21,734:INFO:Initializing predict_model()
2024-11-04 12:00:21,735:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fc65dd16e50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc697095300>)
2024-11-04 12:00:21,735:INFO:Checking exceptions
2024-11-04 12:00:21,735:INFO:Preloading libraries
2024-11-04 12:00:21,735:INFO:Set up data.
2024-11-04 12:00:21,738:INFO:Set up index.
2024-11-04 12:01:31,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:31,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:31,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:31,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:31,868:INFO:Initializing load_model()
2024-11-04 12:01:31,868:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:01:31,899:INFO:Initializing predict_model()
2024-11-04 12:01:31,899:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fb9dbb5bf90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fba14f99300>)
2024-11-04 12:01:31,899:INFO:Checking exceptions
2024-11-04 12:01:31,899:INFO:Preloading libraries
2024-11-04 12:01:31,900:INFO:Set up data.
2024-11-04 12:01:31,902:INFO:Set up index.
2024-11-04 12:01:51,049:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:51,049:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:51,049:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:51,049:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:51,522:INFO:Initializing load_model()
2024-11-04 12:01:51,522:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:01:51,552:INFO:Initializing predict_model()
2024-11-04 12:01:51,553:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f413f16e690>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f4178599300>)
2024-11-04 12:01:51,553:INFO:Checking exceptions
2024-11-04 12:01:51,553:INFO:Preloading libraries
2024-11-04 12:01:51,553:INFO:Set up data.
2024-11-04 12:01:51,556:INFO:Set up index.
2024-11-04 12:01:58,797:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:58,798:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:58,798:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:58,798:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:01:59,252:INFO:Initializing load_model()
2024-11-04 12:01:59,252:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:01:59,282:INFO:Initializing predict_model()
2024-11-04 12:01:59,282:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f9f3d977e10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9f76c95300>)
2024-11-04 12:01:59,282:INFO:Checking exceptions
2024-11-04 12:01:59,283:INFO:Preloading libraries
2024-11-04 12:01:59,283:INFO:Set up data.
2024-11-04 12:01:59,285:INFO:Set up index.
2024-11-04 12:02:44,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:02:44,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:02:44,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:02:44,885:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:02:45,323:INFO:Initializing load_model()
2024-11-04 12:02:45,323:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:02:45,352:INFO:Initializing predict_model()
2024-11-04 12:02:45,352:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f6301033e90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f6336379300>)
2024-11-04 12:02:45,352:INFO:Checking exceptions
2024-11-04 12:02:45,352:INFO:Preloading libraries
2024-11-04 12:02:45,353:INFO:Set up data.
2024-11-04 12:02:45,357:INFO:Set up index.
2024-11-04 12:04:54,564:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:04:54,564:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:04:54,564:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:04:54,564:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:04:54,991:INFO:Initializing load_model()
2024-11-04 12:04:54,991:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:04:55,018:INFO:Initializing predict_model()
2024-11-04 12:04:55,018:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7fa5ed74bf50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fa626a95300>)
2024-11-04 12:04:55,018:INFO:Checking exceptions
2024-11-04 12:04:55,018:INFO:Preloading libraries
2024-11-04 12:04:55,019:INFO:Set up data.
2024-11-04 12:04:55,021:INFO:Set up index.
2024-11-04 12:05:00,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:05:00,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:05:00,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:05:00,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-04 12:05:00,809:INFO:Initializing load_model()
2024-11-04 12:05:00,809:INFO:load_model(model_name=modelo-SA45-ECE, platform=None, authentication=None, verbose=True)
2024-11-04 12:05:00,838:INFO:Initializing predict_model()
2024-11-04 12:05:00,838:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f364cfc7e90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=[], transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Somatización', 'Obsesión/com',
                                             'Sen. Emocio.', 'Depresión',
                                             'Ansiedad', 'Hostilidad',
                                             'Ans. Fóbica', 'Ideación Paran.',
                                             'Psicoticismo'],
                                    transformer=SimpleImputer(strategy='most_freque...
                                             'Psicoticismo'],
                                    transformer=OneHotEncoder(cols=['Somatización',
                                                                    'Obsesión/com',
                                                                    'Sen. '
                                                                    'Emocio.',
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model', RidgeClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f3682295300>)
2024-11-04 12:05:00,838:INFO:Checking exceptions
2024-11-04 12:05:00,838:INFO:Preloading libraries
2024-11-04 12:05:00,839:INFO:Set up data.
2024-11-04 12:05:00,841:INFO:Set up index.
2024-11-04 12:07:21,395:INFO:Initializing plot_model()
2024-11-04 12:07:21,396:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f305faec2d0>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-04 12:07:21,397:INFO:Checking exceptions
2024-11-04 12:07:21,409:INFO:Preloading libraries
2024-11-04 12:07:21,410:INFO:Copying training dataset
2024-11-04 12:07:21,410:INFO:Plot type: confusion_matrix
2024-11-04 12:07:21,761:INFO:Fitting Model
2024-11-04 12:07:21,762:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-04 12:07:21,762:INFO:Scoring test/hold-out set
2024-11-04 12:07:22,049:INFO:Visual Rendered Successfully
2024-11-04 12:07:22,129:INFO:plot_model() successfully completed......................................
2024-11-11 09:38:51,768:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-11 09:38:51,776:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-11 09:38:51,776:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-11 09:38:51,776:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-11 09:38:59,459:INFO:PyCaret ClassificationExperiment
2024-11-11 09:38:59,459:INFO:Logging name: clf-default-name
2024-11-11 09:38:59,459:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-11 09:38:59,459:INFO:version 3.3.2
2024-11-11 09:38:59,459:INFO:Initializing setup()
2024-11-11 09:38:59,459:INFO:self.USI: 1923
2024-11-11 09:38:59,459:INFO:self._variable_keys: {'_ml_usecase', 'X', 'fold_shuffle_param', '_available_plots', 'X_train', 'y_train', 'seed', 'target_param', 'memory', 'n_jobs_param', 'USI', 'exp_id', 'exp_name_log', 'pipeline', 'gpu_param', 'data', 'logging_param', 'fix_imbalance', 'gpu_n_jobs_param', 'X_test', 'idx', 'y', 'html_param', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'y_test', 'is_multiclass'}
2024-11-11 09:38:59,459:INFO:Checking environment
2024-11-11 09:38:59,459:INFO:python_version: 3.11.2
2024-11-11 09:38:59,459:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-11 09:38:59,459:INFO:machine: x86_64
2024-11-11 09:38:59,459:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-11 09:38:59,460:INFO:Memory: svmem(total=16721203200, available=10087202816, percent=39.7, used=6145228800, free=5725507584, active=1480114176, inactive=8465965056, buffers=496222208, cached=4354244608, shared=139919360, slab=543014912)
2024-11-11 09:38:59,461:INFO:Physical Core: 4
2024-11-11 09:38:59,461:INFO:Logical Core: 8
2024-11-11 09:38:59,461:INFO:Checking libraries
2024-11-11 09:38:59,461:INFO:System:
2024-11-11 09:38:59,461:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-11 09:38:59,461:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-11 09:38:59,461:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-11 09:38:59,461:INFO:PyCaret required dependencies:
2024-11-11 09:38:59,660:INFO:                 pip: 23.0.1
2024-11-11 09:38:59,660:INFO:          setuptools: 66.1.1
2024-11-11 09:38:59,660:INFO:             pycaret: 3.3.2
2024-11-11 09:38:59,660:INFO:             IPython: 8.29.0
2024-11-11 09:38:59,660:INFO:          ipywidgets: 8.1.5
2024-11-11 09:38:59,660:INFO:                tqdm: 4.66.6
2024-11-11 09:38:59,660:INFO:               numpy: 1.26.4
2024-11-11 09:38:59,660:INFO:              pandas: 2.1.4
2024-11-11 09:38:59,660:INFO:              jinja2: 3.1.4
2024-11-11 09:38:59,660:INFO:               scipy: 1.11.4
2024-11-11 09:38:59,660:INFO:              joblib: 1.3.2
2024-11-11 09:38:59,660:INFO:             sklearn: 1.4.2
2024-11-11 09:38:59,660:INFO:                pyod: 2.0.2
2024-11-11 09:38:59,660:INFO:            imblearn: 0.12.4
2024-11-11 09:38:59,660:INFO:   category_encoders: 2.6.4
2024-11-11 09:38:59,660:INFO:            lightgbm: 4.5.0
2024-11-11 09:38:59,660:INFO:               numba: 0.60.0
2024-11-11 09:38:59,661:INFO:            requests: 2.32.3
2024-11-11 09:38:59,661:INFO:          matplotlib: 3.7.5
2024-11-11 09:38:59,661:INFO:          scikitplot: 0.3.7
2024-11-11 09:38:59,661:INFO:         yellowbrick: 1.5
2024-11-11 09:38:59,661:INFO:              plotly: 5.24.1
2024-11-11 09:38:59,661:INFO:    plotly-resampler: Not installed
2024-11-11 09:38:59,661:INFO:             kaleido: 0.2.1
2024-11-11 09:38:59,661:INFO:           schemdraw: 0.15
2024-11-11 09:38:59,661:INFO:         statsmodels: 0.14.4
2024-11-11 09:38:59,661:INFO:              sktime: 0.26.0
2024-11-11 09:38:59,661:INFO:               tbats: 1.1.3
2024-11-11 09:38:59,661:INFO:            pmdarima: 2.0.4
2024-11-11 09:38:59,661:INFO:              psutil: 6.1.0
2024-11-11 09:38:59,661:INFO:          markupsafe: 3.0.2
2024-11-11 09:38:59,661:INFO:             pickle5: Not installed
2024-11-11 09:38:59,661:INFO:         cloudpickle: 3.1.0
2024-11-11 09:38:59,661:INFO:         deprecation: 2.1.0
2024-11-11 09:38:59,661:INFO:              xxhash: 3.5.0
2024-11-11 09:38:59,661:INFO:           wurlitzer: 3.1.1
2024-11-11 09:38:59,661:INFO:PyCaret optional dependencies:
2024-11-11 09:38:59,677:INFO:                shap: Not installed
2024-11-11 09:38:59,677:INFO:           interpret: Not installed
2024-11-11 09:38:59,677:INFO:                umap: Not installed
2024-11-11 09:38:59,677:INFO:     ydata_profiling: Not installed
2024-11-11 09:38:59,677:INFO:  explainerdashboard: Not installed
2024-11-11 09:38:59,677:INFO:             autoviz: Not installed
2024-11-11 09:38:59,677:INFO:           fairlearn: Not installed
2024-11-11 09:38:59,677:INFO:          deepchecks: Not installed
2024-11-11 09:38:59,677:INFO:             xgboost: Not installed
2024-11-11 09:38:59,677:INFO:            catboost: Not installed
2024-11-11 09:38:59,677:INFO:              kmodes: Not installed
2024-11-11 09:38:59,677:INFO:             mlxtend: Not installed
2024-11-11 09:38:59,677:INFO:       statsforecast: Not installed
2024-11-11 09:38:59,677:INFO:        tune_sklearn: Not installed
2024-11-11 09:38:59,677:INFO:                 ray: Not installed
2024-11-11 09:38:59,677:INFO:            hyperopt: Not installed
2024-11-11 09:38:59,677:INFO:              optuna: Not installed
2024-11-11 09:38:59,677:INFO:               skopt: Not installed
2024-11-11 09:38:59,677:INFO:              mlflow: Not installed
2024-11-11 09:38:59,677:INFO:              gradio: Not installed
2024-11-11 09:38:59,677:INFO:             fastapi: Not installed
2024-11-11 09:38:59,677:INFO:             uvicorn: Not installed
2024-11-11 09:38:59,677:INFO:              m2cgen: Not installed
2024-11-11 09:38:59,677:INFO:           evidently: Not installed
2024-11-11 09:38:59,677:INFO:               fugue: Not installed
2024-11-11 09:38:59,677:INFO:           streamlit: Not installed
2024-11-11 09:38:59,677:INFO:             prophet: Not installed
2024-11-11 09:38:59,677:INFO:None
2024-11-11 09:38:59,677:INFO:Set up data.
2024-11-11 09:38:59,723:INFO:Set up folding strategy.
2024-11-11 09:38:59,723:INFO:Set up train/test split.
2024-11-11 09:38:59,772:INFO:Set up index.
2024-11-11 09:38:59,772:INFO:Assigning column types.
2024-11-11 09:38:59,783:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-11 09:38:59,837:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-11 09:38:59,851:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:38:59,889:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:38:59,889:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:38:59,924:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-11 09:38:59,925:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:38:59,944:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:38:59,944:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:38:59,944:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-11 09:38:59,975:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:38:59,997:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:38:59,997:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,028:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:39:00,052:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,052:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,053:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-11 09:39:00,102:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,159:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,160:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,162:INFO:Preparing preprocessing pipeline...
2024-11-11 09:39:00,163:INFO:Set up simple imputation.
2024-11-11 09:39:00,165:INFO:Set up encoding of categorical features.
2024-11-11 09:39:00,166:INFO:Set up column name cleaning.
2024-11-11 09:39:00,289:INFO:Finished creating preprocessing pipeline.
2024-11-11 09:39:00,294:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Sexo', 'Agotamiento Emo.'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somat...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-11 09:39:00,294:INFO:Creating final display dataframe.
2024-11-11 09:39:00,606:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 13)
4        Transformed data shape         (372, 33)
5   Transformed train set shape         (260, 33)
6    Transformed test set shape         (112, 33)
7              Numeric features                 2
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              1923
2024-11-11 09:39:00,679:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,680:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,736:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,736:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:00,738:INFO:setup() successfully completed in 1.28s...............
2024-11-11 09:39:15,645:INFO:PyCaret ClassificationExperiment
2024-11-11 09:39:15,645:INFO:Logging name: clf-default-name
2024-11-11 09:39:15,645:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-11 09:39:15,645:INFO:version 3.3.2
2024-11-11 09:39:15,645:INFO:Initializing setup()
2024-11-11 09:39:15,646:INFO:self.USI: c136
2024-11-11 09:39:15,646:INFO:self._variable_keys: {'_ml_usecase', 'X', 'fold_shuffle_param', '_available_plots', 'X_train', 'y_train', 'seed', 'target_param', 'memory', 'n_jobs_param', 'USI', 'exp_id', 'exp_name_log', 'pipeline', 'gpu_param', 'data', 'logging_param', 'fix_imbalance', 'gpu_n_jobs_param', 'X_test', 'idx', 'y', 'html_param', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'y_test', 'is_multiclass'}
2024-11-11 09:39:15,646:INFO:Checking environment
2024-11-11 09:39:15,646:INFO:python_version: 3.11.2
2024-11-11 09:39:15,646:INFO:python_build: ('main', 'Aug 26 2024 07:20:54')
2024-11-11 09:39:15,646:INFO:machine: x86_64
2024-11-11 09:39:15,646:INFO:platform: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-11 09:39:15,646:INFO:Memory: svmem(total=16721203200, available=10080407552, percent=39.7, used=6152003584, free=5714841600, active=1480302592, inactive=8488849408, buffers=496410624, cached=4357947392, shared=139919360, slab=543256576)
2024-11-11 09:39:15,647:INFO:Physical Core: 4
2024-11-11 09:39:15,647:INFO:Logical Core: 8
2024-11-11 09:39:15,647:INFO:Checking libraries
2024-11-11 09:39:15,647:INFO:System:
2024-11-11 09:39:15,647:INFO:    python: 3.11.2 (main, Aug 26 2024, 07:20:54) [GCC 12.2.0]
2024-11-11 09:39:15,647:INFO:executable: /home/kurt/coding/TesisPycaret/backend/.venv/bin/python
2024-11-11 09:39:15,647:INFO:   machine: Linux-6.1.0-26-amd64-x86_64-with-glibc2.36
2024-11-11 09:39:15,647:INFO:PyCaret required dependencies:
2024-11-11 09:39:15,647:INFO:                 pip: 23.0.1
2024-11-11 09:39:15,647:INFO:          setuptools: 66.1.1
2024-11-11 09:39:15,647:INFO:             pycaret: 3.3.2
2024-11-11 09:39:15,647:INFO:             IPython: 8.29.0
2024-11-11 09:39:15,647:INFO:          ipywidgets: 8.1.5
2024-11-11 09:39:15,647:INFO:                tqdm: 4.66.6
2024-11-11 09:39:15,647:INFO:               numpy: 1.26.4
2024-11-11 09:39:15,647:INFO:              pandas: 2.1.4
2024-11-11 09:39:15,647:INFO:              jinja2: 3.1.4
2024-11-11 09:39:15,647:INFO:               scipy: 1.11.4
2024-11-11 09:39:15,647:INFO:              joblib: 1.3.2
2024-11-11 09:39:15,647:INFO:             sklearn: 1.4.2
2024-11-11 09:39:15,647:INFO:                pyod: 2.0.2
2024-11-11 09:39:15,647:INFO:            imblearn: 0.12.4
2024-11-11 09:39:15,647:INFO:   category_encoders: 2.6.4
2024-11-11 09:39:15,647:INFO:            lightgbm: 4.5.0
2024-11-11 09:39:15,647:INFO:               numba: 0.60.0
2024-11-11 09:39:15,647:INFO:            requests: 2.32.3
2024-11-11 09:39:15,647:INFO:          matplotlib: 3.7.5
2024-11-11 09:39:15,647:INFO:          scikitplot: 0.3.7
2024-11-11 09:39:15,647:INFO:         yellowbrick: 1.5
2024-11-11 09:39:15,647:INFO:              plotly: 5.24.1
2024-11-11 09:39:15,647:INFO:    plotly-resampler: Not installed
2024-11-11 09:39:15,647:INFO:             kaleido: 0.2.1
2024-11-11 09:39:15,647:INFO:           schemdraw: 0.15
2024-11-11 09:39:15,647:INFO:         statsmodels: 0.14.4
2024-11-11 09:39:15,647:INFO:              sktime: 0.26.0
2024-11-11 09:39:15,648:INFO:               tbats: 1.1.3
2024-11-11 09:39:15,648:INFO:            pmdarima: 2.0.4
2024-11-11 09:39:15,648:INFO:              psutil: 6.1.0
2024-11-11 09:39:15,648:INFO:          markupsafe: 3.0.2
2024-11-11 09:39:15,648:INFO:             pickle5: Not installed
2024-11-11 09:39:15,648:INFO:         cloudpickle: 3.1.0
2024-11-11 09:39:15,648:INFO:         deprecation: 2.1.0
2024-11-11 09:39:15,648:INFO:              xxhash: 3.5.0
2024-11-11 09:39:15,648:INFO:           wurlitzer: 3.1.1
2024-11-11 09:39:15,648:INFO:PyCaret optional dependencies:
2024-11-11 09:39:15,648:INFO:                shap: Not installed
2024-11-11 09:39:15,648:INFO:           interpret: Not installed
2024-11-11 09:39:15,648:INFO:                umap: Not installed
2024-11-11 09:39:15,648:INFO:     ydata_profiling: Not installed
2024-11-11 09:39:15,648:INFO:  explainerdashboard: Not installed
2024-11-11 09:39:15,648:INFO:             autoviz: Not installed
2024-11-11 09:39:15,648:INFO:           fairlearn: Not installed
2024-11-11 09:39:15,648:INFO:          deepchecks: Not installed
2024-11-11 09:39:15,648:INFO:             xgboost: Not installed
2024-11-11 09:39:15,648:INFO:            catboost: Not installed
2024-11-11 09:39:15,648:INFO:              kmodes: Not installed
2024-11-11 09:39:15,648:INFO:             mlxtend: Not installed
2024-11-11 09:39:15,648:INFO:       statsforecast: Not installed
2024-11-11 09:39:15,648:INFO:        tune_sklearn: Not installed
2024-11-11 09:39:15,648:INFO:                 ray: Not installed
2024-11-11 09:39:15,648:INFO:            hyperopt: Not installed
2024-11-11 09:39:15,648:INFO:              optuna: Not installed
2024-11-11 09:39:15,648:INFO:               skopt: Not installed
2024-11-11 09:39:15,648:INFO:              mlflow: Not installed
2024-11-11 09:39:15,648:INFO:              gradio: Not installed
2024-11-11 09:39:15,648:INFO:             fastapi: Not installed
2024-11-11 09:39:15,648:INFO:             uvicorn: Not installed
2024-11-11 09:39:15,648:INFO:              m2cgen: Not installed
2024-11-11 09:39:15,648:INFO:           evidently: Not installed
2024-11-11 09:39:15,648:INFO:               fugue: Not installed
2024-11-11 09:39:15,649:INFO:           streamlit: Not installed
2024-11-11 09:39:15,649:INFO:             prophet: Not installed
2024-11-11 09:39:15,649:INFO:None
2024-11-11 09:39:15,649:INFO:Set up data.
2024-11-11 09:39:15,657:INFO:Set up folding strategy.
2024-11-11 09:39:15,657:INFO:Set up train/test split.
2024-11-11 09:39:15,663:INFO:Set up index.
2024-11-11 09:39:15,663:INFO:Assigning column types.
2024-11-11 09:39:15,667:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-11 09:39:15,724:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,725:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,749:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,749:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,793:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,793:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,817:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,818:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-11 09:39:15,855:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,876:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,910:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-11 09:39:15,928:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,928:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,929:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-11 09:39:15,979:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:15,979:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,038:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,039:INFO:Preparing preprocessing pipeline...
2024-11-11 09:39:16,040:INFO:Set up simple imputation.
2024-11-11 09:39:16,042:INFO:Set up encoding of categorical features.
2024-11-11 09:39:16,042:INFO:Set up column name cleaning.
2024-11-11 09:39:16,165:INFO:Finished creating preprocessing pipeline.
2024-11-11 09:39:16,171:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Sexo', 'Agotamiento Emo.'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somat...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-11 09:39:16,171:INFO:Creating final display dataframe.
2024-11-11 09:39:16,451:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 13)
4        Transformed data shape         (372, 33)
5   Transformed train set shape         (260, 33)
6    Transformed test set shape         (112, 33)
7              Numeric features                 2
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c136
2024-11-11 09:39:16,527:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,527:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,589:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,590:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-11 09:39:16,591:INFO:setup() successfully completed in 0.95s...............
2024-11-11 09:39:19,689:INFO:Initializing compare_models()
2024-11-11 09:39:19,689:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-11 09:39:19,690:INFO:Checking exceptions
2024-11-11 09:39:19,699:INFO:Preparing display monitor
2024-11-11 09:39:19,728:INFO:Initializing Logistic Regression
2024-11-11 09:39:19,728:INFO:Total runtime is 5.4478645324707035e-06 minutes
2024-11-11 09:39:19,732:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:19,733:INFO:Initializing create_model()
2024-11-11 09:39:19,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:19,733:INFO:Checking exceptions
2024-11-11 09:39:19,733:INFO:Importing libraries
2024-11-11 09:39:19,733:INFO:Copying training dataset
2024-11-11 09:39:19,740:INFO:Defining folds
2024-11-11 09:39:19,740:INFO:Declaring metric variables
2024-11-11 09:39:19,746:INFO:Importing untrained model
2024-11-11 09:39:19,751:INFO:Logistic Regression Imported successfully
2024-11-11 09:39:19,760:INFO:Starting cross validation
2024-11-11 09:39:19,763:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:21,718:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:21,718:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:21,730:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:21,739:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:21,759:INFO:Calculating mean and std
2024-11-11 09:39:21,762:INFO:Creating metrics dataframe
2024-11-11 09:39:21,768:INFO:Uploading results into container
2024-11-11 09:39:21,769:INFO:Uploading model into container now
2024-11-11 09:39:21,770:INFO:_master_model_container: 1
2024-11-11 09:39:21,770:INFO:_display_container: 2
2024-11-11 09:39:21,771:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-11 09:39:21,771:INFO:create_model() successfully completed......................................
2024-11-11 09:39:21,957:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:21,958:INFO:Creating metrics dataframe
2024-11-11 09:39:21,963:INFO:Initializing K Neighbors Classifier
2024-11-11 09:39:21,964:INFO:Total runtime is 0.037268853187561034 minutes
2024-11-11 09:39:21,967:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:21,968:INFO:Initializing create_model()
2024-11-11 09:39:21,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:21,968:INFO:Checking exceptions
2024-11-11 09:39:21,968:INFO:Importing libraries
2024-11-11 09:39:21,968:INFO:Copying training dataset
2024-11-11 09:39:21,973:INFO:Defining folds
2024-11-11 09:39:21,974:INFO:Declaring metric variables
2024-11-11 09:39:21,977:INFO:Importing untrained model
2024-11-11 09:39:21,980:INFO:K Neighbors Classifier Imported successfully
2024-11-11 09:39:21,986:INFO:Starting cross validation
2024-11-11 09:39:21,989:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:23,642:INFO:Calculating mean and std
2024-11-11 09:39:23,643:INFO:Creating metrics dataframe
2024-11-11 09:39:23,646:INFO:Uploading results into container
2024-11-11 09:39:23,647:INFO:Uploading model into container now
2024-11-11 09:39:23,647:INFO:_master_model_container: 2
2024-11-11 09:39:23,648:INFO:_display_container: 2
2024-11-11 09:39:23,648:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-11 09:39:23,648:INFO:create_model() successfully completed......................................
2024-11-11 09:39:23,773:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:23,773:INFO:Creating metrics dataframe
2024-11-11 09:39:23,779:INFO:Initializing Naive Bayes
2024-11-11 09:39:23,779:INFO:Total runtime is 0.0675286054611206 minutes
2024-11-11 09:39:23,783:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:23,783:INFO:Initializing create_model()
2024-11-11 09:39:23,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:23,783:INFO:Checking exceptions
2024-11-11 09:39:23,783:INFO:Importing libraries
2024-11-11 09:39:23,784:INFO:Copying training dataset
2024-11-11 09:39:23,788:INFO:Defining folds
2024-11-11 09:39:23,788:INFO:Declaring metric variables
2024-11-11 09:39:23,791:INFO:Importing untrained model
2024-11-11 09:39:23,794:INFO:Naive Bayes Imported successfully
2024-11-11 09:39:23,802:INFO:Starting cross validation
2024-11-11 09:39:23,806:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:24,035:INFO:Calculating mean and std
2024-11-11 09:39:24,036:INFO:Creating metrics dataframe
2024-11-11 09:39:24,038:INFO:Uploading results into container
2024-11-11 09:39:24,038:INFO:Uploading model into container now
2024-11-11 09:39:24,038:INFO:_master_model_container: 3
2024-11-11 09:39:24,038:INFO:_display_container: 2
2024-11-11 09:39:24,039:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-11 09:39:24,039:INFO:create_model() successfully completed......................................
2024-11-11 09:39:24,146:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:24,146:INFO:Creating metrics dataframe
2024-11-11 09:39:24,152:INFO:Initializing Decision Tree Classifier
2024-11-11 09:39:24,153:INFO:Total runtime is 0.07375047604242961 minutes
2024-11-11 09:39:24,155:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:24,156:INFO:Initializing create_model()
2024-11-11 09:39:24,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:24,156:INFO:Checking exceptions
2024-11-11 09:39:24,156:INFO:Importing libraries
2024-11-11 09:39:24,156:INFO:Copying training dataset
2024-11-11 09:39:24,160:INFO:Defining folds
2024-11-11 09:39:24,160:INFO:Declaring metric variables
2024-11-11 09:39:24,165:INFO:Importing untrained model
2024-11-11 09:39:24,168:INFO:Decision Tree Classifier Imported successfully
2024-11-11 09:39:24,174:INFO:Starting cross validation
2024-11-11 09:39:24,178:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:24,369:INFO:Calculating mean and std
2024-11-11 09:39:24,370:INFO:Creating metrics dataframe
2024-11-11 09:39:24,371:INFO:Uploading results into container
2024-11-11 09:39:24,372:INFO:Uploading model into container now
2024-11-11 09:39:24,372:INFO:_master_model_container: 4
2024-11-11 09:39:24,373:INFO:_display_container: 2
2024-11-11 09:39:24,373:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-11 09:39:24,373:INFO:create_model() successfully completed......................................
2024-11-11 09:39:24,466:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:24,466:INFO:Creating metrics dataframe
2024-11-11 09:39:24,472:INFO:Initializing SVM - Linear Kernel
2024-11-11 09:39:24,472:INFO:Total runtime is 0.07906928857167562 minutes
2024-11-11 09:39:24,474:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:24,474:INFO:Initializing create_model()
2024-11-11 09:39:24,475:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:24,475:INFO:Checking exceptions
2024-11-11 09:39:24,475:INFO:Importing libraries
2024-11-11 09:39:24,475:INFO:Copying training dataset
2024-11-11 09:39:24,479:INFO:Defining folds
2024-11-11 09:39:24,479:INFO:Declaring metric variables
2024-11-11 09:39:24,482:INFO:Importing untrained model
2024-11-11 09:39:24,485:INFO:SVM - Linear Kernel Imported successfully
2024-11-11 09:39:24,491:INFO:Starting cross validation
2024-11-11 09:39:24,494:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:24,676:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:24,681:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:24,690:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:24,694:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:24,713:INFO:Calculating mean and std
2024-11-11 09:39:24,714:INFO:Creating metrics dataframe
2024-11-11 09:39:24,716:INFO:Uploading results into container
2024-11-11 09:39:24,717:INFO:Uploading model into container now
2024-11-11 09:39:24,717:INFO:_master_model_container: 5
2024-11-11 09:39:24,718:INFO:_display_container: 2
2024-11-11 09:39:24,718:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-11 09:39:24,718:INFO:create_model() successfully completed......................................
2024-11-11 09:39:24,813:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:24,813:INFO:Creating metrics dataframe
2024-11-11 09:39:24,819:INFO:Initializing Ridge Classifier
2024-11-11 09:39:24,819:INFO:Total runtime is 0.08486342827479046 minutes
2024-11-11 09:39:24,822:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:24,822:INFO:Initializing create_model()
2024-11-11 09:39:24,822:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:24,822:INFO:Checking exceptions
2024-11-11 09:39:24,822:INFO:Importing libraries
2024-11-11 09:39:24,822:INFO:Copying training dataset
2024-11-11 09:39:24,826:INFO:Defining folds
2024-11-11 09:39:24,826:INFO:Declaring metric variables
2024-11-11 09:39:24,829:INFO:Importing untrained model
2024-11-11 09:39:24,834:INFO:Ridge Classifier Imported successfully
2024-11-11 09:39:24,842:INFO:Starting cross validation
2024-11-11 09:39:24,846:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:25,140:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:25,140:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:25,141:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:25,142:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:25,156:INFO:Calculating mean and std
2024-11-11 09:39:25,157:INFO:Creating metrics dataframe
2024-11-11 09:39:25,158:INFO:Uploading results into container
2024-11-11 09:39:25,159:INFO:Uploading model into container now
2024-11-11 09:39:25,159:INFO:_master_model_container: 6
2024-11-11 09:39:25,159:INFO:_display_container: 2
2024-11-11 09:39:25,159:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-11 09:39:25,159:INFO:create_model() successfully completed......................................
2024-11-11 09:39:25,256:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:25,256:INFO:Creating metrics dataframe
2024-11-11 09:39:25,263:INFO:Initializing Random Forest Classifier
2024-11-11 09:39:25,263:INFO:Total runtime is 0.09225129286448162 minutes
2024-11-11 09:39:25,265:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:25,266:INFO:Initializing create_model()
2024-11-11 09:39:25,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:25,266:INFO:Checking exceptions
2024-11-11 09:39:25,266:INFO:Importing libraries
2024-11-11 09:39:25,266:INFO:Copying training dataset
2024-11-11 09:39:25,270:INFO:Defining folds
2024-11-11 09:39:25,270:INFO:Declaring metric variables
2024-11-11 09:39:25,273:INFO:Importing untrained model
2024-11-11 09:39:25,276:INFO:Random Forest Classifier Imported successfully
2024-11-11 09:39:25,282:INFO:Starting cross validation
2024-11-11 09:39:25,285:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:25,715:INFO:Calculating mean and std
2024-11-11 09:39:25,718:INFO:Creating metrics dataframe
2024-11-11 09:39:25,724:INFO:Uploading results into container
2024-11-11 09:39:25,725:INFO:Uploading model into container now
2024-11-11 09:39:25,726:INFO:_master_model_container: 7
2024-11-11 09:39:25,726:INFO:_display_container: 2
2024-11-11 09:39:25,727:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-11 09:39:25,727:INFO:create_model() successfully completed......................................
2024-11-11 09:39:25,879:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:25,879:INFO:Creating metrics dataframe
2024-11-11 09:39:25,885:INFO:Initializing Quadratic Discriminant Analysis
2024-11-11 09:39:25,885:INFO:Total runtime is 0.10261947711308798 minutes
2024-11-11 09:39:25,887:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:25,888:INFO:Initializing create_model()
2024-11-11 09:39:25,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:25,888:INFO:Checking exceptions
2024-11-11 09:39:25,888:INFO:Importing libraries
2024-11-11 09:39:25,888:INFO:Copying training dataset
2024-11-11 09:39:25,892:INFO:Defining folds
2024-11-11 09:39:25,892:INFO:Declaring metric variables
2024-11-11 09:39:25,895:INFO:Importing untrained model
2024-11-11 09:39:25,898:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-11 09:39:25,904:INFO:Starting cross validation
2024-11-11 09:39:25,907:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:26,103:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-11 09:39:26,103:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-11 09:39:26,103:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-11 09:39:26,103:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-11 09:39:26,164:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,167:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,168:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,168:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,186:INFO:Calculating mean and std
2024-11-11 09:39:26,188:INFO:Creating metrics dataframe
2024-11-11 09:39:26,191:INFO:Uploading results into container
2024-11-11 09:39:26,191:INFO:Uploading model into container now
2024-11-11 09:39:26,192:INFO:_master_model_container: 8
2024-11-11 09:39:26,192:INFO:_display_container: 2
2024-11-11 09:39:26,192:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-11 09:39:26,192:INFO:create_model() successfully completed......................................
2024-11-11 09:39:26,296:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:26,296:INFO:Creating metrics dataframe
2024-11-11 09:39:26,303:INFO:Initializing Ada Boost Classifier
2024-11-11 09:39:26,303:INFO:Total runtime is 0.10959359010060629 minutes
2024-11-11 09:39:26,306:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:26,307:INFO:Initializing create_model()
2024-11-11 09:39:26,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:26,307:INFO:Checking exceptions
2024-11-11 09:39:26,307:INFO:Importing libraries
2024-11-11 09:39:26,307:INFO:Copying training dataset
2024-11-11 09:39:26,312:INFO:Defining folds
2024-11-11 09:39:26,312:INFO:Declaring metric variables
2024-11-11 09:39:26,316:INFO:Importing untrained model
2024-11-11 09:39:26,320:INFO:Ada Boost Classifier Imported successfully
2024-11-11 09:39:26,327:INFO:Starting cross validation
2024-11-11 09:39:26,331:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:26,460:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-11 09:39:26,460:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-11 09:39:26,473:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-11 09:39:26,475:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-11 09:39:26,562:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,566:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,592:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,602:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:26,611:INFO:Calculating mean and std
2024-11-11 09:39:26,612:INFO:Creating metrics dataframe
2024-11-11 09:39:26,614:INFO:Uploading results into container
2024-11-11 09:39:26,614:INFO:Uploading model into container now
2024-11-11 09:39:26,614:INFO:_master_model_container: 9
2024-11-11 09:39:26,614:INFO:_display_container: 2
2024-11-11 09:39:26,615:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-11 09:39:26,615:INFO:create_model() successfully completed......................................
2024-11-11 09:39:26,709:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:26,709:INFO:Creating metrics dataframe
2024-11-11 09:39:26,718:INFO:Initializing Gradient Boosting Classifier
2024-11-11 09:39:26,718:INFO:Total runtime is 0.11650394201278687 minutes
2024-11-11 09:39:26,722:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:26,722:INFO:Initializing create_model()
2024-11-11 09:39:26,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:26,723:INFO:Checking exceptions
2024-11-11 09:39:26,723:INFO:Importing libraries
2024-11-11 09:39:26,723:INFO:Copying training dataset
2024-11-11 09:39:26,727:INFO:Defining folds
2024-11-11 09:39:26,727:INFO:Declaring metric variables
2024-11-11 09:39:26,731:INFO:Importing untrained model
2024-11-11 09:39:26,735:INFO:Gradient Boosting Classifier Imported successfully
2024-11-11 09:39:26,740:INFO:Starting cross validation
2024-11-11 09:39:26,743:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:27,098:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,098:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,098:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,099:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,114:INFO:Calculating mean and std
2024-11-11 09:39:27,115:INFO:Creating metrics dataframe
2024-11-11 09:39:27,117:INFO:Uploading results into container
2024-11-11 09:39:27,117:INFO:Uploading model into container now
2024-11-11 09:39:27,118:INFO:_master_model_container: 10
2024-11-11 09:39:27,118:INFO:_display_container: 2
2024-11-11 09:39:27,118:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-11 09:39:27,118:INFO:create_model() successfully completed......................................
2024-11-11 09:39:27,212:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:27,212:INFO:Creating metrics dataframe
2024-11-11 09:39:27,220:INFO:Initializing Linear Discriminant Analysis
2024-11-11 09:39:27,220:INFO:Total runtime is 0.12488001187642415 minutes
2024-11-11 09:39:27,223:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:27,224:INFO:Initializing create_model()
2024-11-11 09:39:27,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:27,224:INFO:Checking exceptions
2024-11-11 09:39:27,224:INFO:Importing libraries
2024-11-11 09:39:27,224:INFO:Copying training dataset
2024-11-11 09:39:27,229:INFO:Defining folds
2024-11-11 09:39:27,229:INFO:Declaring metric variables
2024-11-11 09:39:27,232:INFO:Importing untrained model
2024-11-11 09:39:27,235:INFO:Linear Discriminant Analysis Imported successfully
2024-11-11 09:39:27,240:INFO:Starting cross validation
2024-11-11 09:39:27,243:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:27,414:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,415:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,415:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,415:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-11 09:39:27,432:INFO:Calculating mean and std
2024-11-11 09:39:27,433:INFO:Creating metrics dataframe
2024-11-11 09:39:27,434:INFO:Uploading results into container
2024-11-11 09:39:27,434:INFO:Uploading model into container now
2024-11-11 09:39:27,435:INFO:_master_model_container: 11
2024-11-11 09:39:27,435:INFO:_display_container: 2
2024-11-11 09:39:27,435:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-11 09:39:27,435:INFO:create_model() successfully completed......................................
2024-11-11 09:39:27,528:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:27,528:INFO:Creating metrics dataframe
2024-11-11 09:39:27,536:INFO:Initializing Extra Trees Classifier
2024-11-11 09:39:27,536:INFO:Total runtime is 0.13014344374338785 minutes
2024-11-11 09:39:27,540:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:27,540:INFO:Initializing create_model()
2024-11-11 09:39:27,540:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:27,540:INFO:Checking exceptions
2024-11-11 09:39:27,540:INFO:Importing libraries
2024-11-11 09:39:27,541:INFO:Copying training dataset
2024-11-11 09:39:27,545:INFO:Defining folds
2024-11-11 09:39:27,545:INFO:Declaring metric variables
2024-11-11 09:39:27,548:INFO:Importing untrained model
2024-11-11 09:39:27,551:INFO:Extra Trees Classifier Imported successfully
2024-11-11 09:39:27,558:INFO:Starting cross validation
2024-11-11 09:39:27,561:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:39:27,986:INFO:Calculating mean and std
2024-11-11 09:39:27,987:INFO:Creating metrics dataframe
2024-11-11 09:39:27,988:INFO:Uploading results into container
2024-11-11 09:39:27,989:INFO:Uploading model into container now
2024-11-11 09:39:27,989:INFO:_master_model_container: 12
2024-11-11 09:39:27,989:INFO:_display_container: 2
2024-11-11 09:39:27,989:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-11 09:39:27,990:INFO:create_model() successfully completed......................................
2024-11-11 09:39:28,092:INFO:SubProcess create_model() end ==================================
2024-11-11 09:39:28,092:INFO:Creating metrics dataframe
2024-11-11 09:39:28,101:INFO:Initializing Light Gradient Boosting Machine
2024-11-11 09:39:28,101:INFO:Total runtime is 0.13956421216328938 minutes
2024-11-11 09:39:28,104:INFO:SubProcess create_model() called ==================================
2024-11-11 09:39:28,104:INFO:Initializing create_model()
2024-11-11 09:39:28,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:39:28,105:INFO:Checking exceptions
2024-11-11 09:39:28,105:INFO:Importing libraries
2024-11-11 09:39:28,105:INFO:Copying training dataset
2024-11-11 09:39:28,109:INFO:Defining folds
2024-11-11 09:39:28,109:INFO:Declaring metric variables
2024-11-11 09:39:28,112:INFO:Importing untrained model
2024-11-11 09:39:28,117:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-11 09:39:28,123:INFO:Starting cross validation
2024-11-11 09:39:28,126:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:42:57,070:INFO:Calculating mean and std
2024-11-11 09:42:57,071:INFO:Creating metrics dataframe
2024-11-11 09:42:57,073:INFO:Uploading results into container
2024-11-11 09:42:57,073:INFO:Uploading model into container now
2024-11-11 09:42:57,073:INFO:_master_model_container: 13
2024-11-11 09:42:57,073:INFO:_display_container: 2
2024-11-11 09:42:57,074:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-11 09:42:57,074:INFO:create_model() successfully completed......................................
2024-11-11 09:42:57,177:INFO:SubProcess create_model() end ==================================
2024-11-11 09:42:57,177:INFO:Creating metrics dataframe
2024-11-11 09:42:57,186:INFO:Initializing Dummy Classifier
2024-11-11 09:42:57,186:INFO:Total runtime is 3.624306511878967 minutes
2024-11-11 09:42:57,188:INFO:SubProcess create_model() called ==================================
2024-11-11 09:42:57,189:INFO:Initializing create_model()
2024-11-11 09:42:57,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f89f5da48d0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:42:57,189:INFO:Checking exceptions
2024-11-11 09:42:57,189:INFO:Importing libraries
2024-11-11 09:42:57,189:INFO:Copying training dataset
2024-11-11 09:42:57,194:INFO:Defining folds
2024-11-11 09:42:57,194:INFO:Declaring metric variables
2024-11-11 09:42:57,196:INFO:Importing untrained model
2024-11-11 09:42:57,199:INFO:Dummy Classifier Imported successfully
2024-11-11 09:42:57,204:INFO:Starting cross validation
2024-11-11 09:42:57,207:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-11 09:42:57,401:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-11 09:42:57,401:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-11 09:42:57,421:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-11 09:42:57,424:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-11 09:42:57,434:INFO:Calculating mean and std
2024-11-11 09:42:57,435:INFO:Creating metrics dataframe
2024-11-11 09:42:57,437:INFO:Uploading results into container
2024-11-11 09:42:57,437:INFO:Uploading model into container now
2024-11-11 09:42:57,438:INFO:_master_model_container: 14
2024-11-11 09:42:57,438:INFO:_display_container: 2
2024-11-11 09:42:57,438:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-11 09:42:57,438:INFO:create_model() successfully completed......................................
2024-11-11 09:42:57,550:INFO:SubProcess create_model() end ==================================
2024-11-11 09:42:57,550:INFO:Creating metrics dataframe
2024-11-11 09:42:57,566:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-11 09:42:57,574:INFO:Initializing create_model()
2024-11-11 09:42:57,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best'), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-11 09:42:57,574:INFO:Checking exceptions
2024-11-11 09:42:57,577:INFO:Importing libraries
2024-11-11 09:42:57,577:INFO:Copying training dataset
2024-11-11 09:42:57,582:INFO:Defining folds
2024-11-11 09:42:57,582:INFO:Declaring metric variables
2024-11-11 09:42:57,583:INFO:Importing untrained model
2024-11-11 09:42:57,583:INFO:Declaring custom model
2024-11-11 09:42:57,583:INFO:Decision Tree Classifier Imported successfully
2024-11-11 09:42:57,586:INFO:Cross validation set to False
2024-11-11 09:42:57,586:INFO:Fitting Model
2024-11-11 09:42:57,699:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-11 09:42:57,700:INFO:create_model() successfully completed......................................
2024-11-11 09:42:57,829:INFO:_master_model_container: 14
2024-11-11 09:42:57,830:INFO:_display_container: 2
2024-11-11 09:42:57,830:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-11 09:42:57,830:INFO:compare_models() successfully completed......................................
2024-11-11 09:43:48,529:INFO:Initializing plot_model()
2024-11-11 09:43:48,529:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f89f68d2350>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best'), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-11 09:43:48,529:INFO:Checking exceptions
2024-11-11 09:43:48,542:INFO:Preloading libraries
2024-11-11 09:43:48,542:INFO:Copying training dataset
2024-11-11 09:43:48,543:INFO:Plot type: confusion_matrix
2024-11-11 09:43:48,922:INFO:Fitting Model
2024-11-11 09:43:48,927:WARNING:/home/kurt/coding/TesisPycaret/backend/.venv/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
  warnings.warn(

2024-11-11 09:43:48,928:INFO:Scoring test/hold-out set
2024-11-11 09:43:49,218:INFO:Visual Rendered Successfully
2024-11-11 09:43:49,316:INFO:plot_model() successfully completed......................................
2024-11-11 10:25:24,964:INFO:Initializing save_model()
2024-11-11 10:25:24,964:INFO:save_model(model=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best'), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Sexo', 'Agotamiento Emo.'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somat...
                                                                    'Depresión',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fóbica',
                                                                    'Ideación '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-11 10:25:24,964:INFO:Adding model into prep_pipe
2024-11-11 10:25:24,985:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-11 10:25:24,990:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Sexo', 'Agotamiento Emo.'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatización', 'O...
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                        criterion='gini', max_depth=None,
                                        max_features=None, max_leaf_nodes=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, random_state=123,
                                        splitter='best'))],
         verbose=False)
2024-11-11 10:25:24,991:INFO:save_model() successfully completed......................................
2024-11-25 11:55:44,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 11:55:44,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 11:55:44,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 11:55:44,019:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 11:55:46,369:INFO:PyCaret ClassificationExperiment
2024-11-25 11:55:46,369:INFO:Logging name: clf-default-name
2024-11-25 11:55:46,369:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 11:55:46,369:INFO:version 3.3.2
2024-11-25 11:55:46,369:INFO:Initializing setup()
2024-11-25 11:55:46,369:INFO:self.USI: f447
2024-11-25 11:55:46,369:INFO:self._variable_keys: {'html_param', 'idx', 'fix_imbalance', 'gpu_n_jobs_param', 'pipeline', 'data', 'X', 'seed', 'X_test', 'logging_param', 'X_train', 'y', 'fold_generator', 'is_multiclass', 'gpu_param', 'y_test', 'n_jobs_param', '_ml_usecase', 'target_param', 'fold_shuffle_param', 'fold_groups_param', 'USI', 'log_plots_param', 'y_train', 'exp_id', '_available_plots', 'exp_name_log', 'memory'}
2024-11-25 11:55:46,369:INFO:Checking environment
2024-11-25 11:55:46,369:INFO:python_version: 3.11.9
2024-11-25 11:55:46,369:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 11:55:46,369:INFO:machine: AMD64
2024-11-25 11:55:46,369:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 11:55:46,385:INFO:Memory: svmem(total=25525833728, available=14662430720, percent=42.6, used=10863403008, free=14662430720)
2024-11-25 11:55:46,385:INFO:Physical Core: 4
2024-11-25 11:55:46,385:INFO:Logical Core: 8
2024-11-25 11:55:46,385:INFO:Checking libraries
2024-11-25 11:55:46,385:INFO:System:
2024-11-25 11:55:46,385:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 11:55:46,385:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 11:55:46,385:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 11:55:46,385:INFO:PyCaret required dependencies:
2024-11-25 11:55:46,407:INFO:                 pip: 24.0
2024-11-25 11:55:46,407:INFO:          setuptools: 65.5.0
2024-11-25 11:55:46,407:INFO:             pycaret: 3.3.2
2024-11-25 11:55:46,407:INFO:             IPython: 8.29.0
2024-11-25 11:55:46,407:INFO:          ipywidgets: 8.1.5
2024-11-25 11:55:46,407:INFO:                tqdm: 4.66.6
2024-11-25 11:55:46,407:INFO:               numpy: 1.26.4
2024-11-25 11:55:46,407:INFO:              pandas: 2.1.4
2024-11-25 11:55:46,407:INFO:              jinja2: 3.1.4
2024-11-25 11:55:46,407:INFO:               scipy: 1.11.4
2024-11-25 11:55:46,407:INFO:              joblib: 1.3.2
2024-11-25 11:55:46,407:INFO:             sklearn: 1.4.2
2024-11-25 11:55:46,407:INFO:                pyod: 2.0.2
2024-11-25 11:55:46,407:INFO:            imblearn: 0.12.4
2024-11-25 11:55:46,407:INFO:   category_encoders: 2.6.4
2024-11-25 11:55:46,407:INFO:            lightgbm: 4.5.0
2024-11-25 11:55:46,407:INFO:               numba: 0.60.0
2024-11-25 11:55:46,407:INFO:            requests: 2.32.3
2024-11-25 11:55:46,407:INFO:          matplotlib: 3.7.5
2024-11-25 11:55:46,407:INFO:          scikitplot: 0.3.7
2024-11-25 11:55:46,407:INFO:         yellowbrick: 1.5
2024-11-25 11:55:46,407:INFO:              plotly: 5.24.1
2024-11-25 11:55:46,407:INFO:    plotly-resampler: Not installed
2024-11-25 11:55:46,407:INFO:             kaleido: 0.2.1
2024-11-25 11:55:46,407:INFO:           schemdraw: 0.15
2024-11-25 11:55:46,407:INFO:         statsmodels: 0.14.4
2024-11-25 11:55:46,407:INFO:              sktime: 0.26.0
2024-11-25 11:55:46,407:INFO:               tbats: 1.1.3
2024-11-25 11:55:46,407:INFO:            pmdarima: 2.0.4
2024-11-25 11:55:46,407:INFO:              psutil: 6.1.0
2024-11-25 11:55:46,407:INFO:          markupsafe: 3.0.2
2024-11-25 11:55:46,407:INFO:             pickle5: Not installed
2024-11-25 11:55:46,407:INFO:         cloudpickle: 3.1.0
2024-11-25 11:55:46,407:INFO:         deprecation: 2.1.0
2024-11-25 11:55:46,407:INFO:              xxhash: 3.5.0
2024-11-25 11:55:46,407:INFO:           wurlitzer: 3.1.1
2024-11-25 11:55:46,407:INFO:PyCaret optional dependencies:
2024-11-25 11:55:46,423:INFO:                shap: Not installed
2024-11-25 11:55:46,423:INFO:           interpret: Not installed
2024-11-25 11:55:46,423:INFO:                umap: Not installed
2024-11-25 11:55:46,423:INFO:     ydata_profiling: Not installed
2024-11-25 11:55:46,423:INFO:  explainerdashboard: Not installed
2024-11-25 11:55:46,423:INFO:             autoviz: Not installed
2024-11-25 11:55:46,423:INFO:           fairlearn: Not installed
2024-11-25 11:55:46,423:INFO:          deepchecks: Not installed
2024-11-25 11:55:46,423:INFO:             xgboost: Not installed
2024-11-25 11:55:46,423:INFO:            catboost: Not installed
2024-11-25 11:55:46,423:INFO:              kmodes: Not installed
2024-11-25 11:55:46,423:INFO:             mlxtend: Not installed
2024-11-25 11:55:46,423:INFO:       statsforecast: Not installed
2024-11-25 11:55:46,423:INFO:        tune_sklearn: Not installed
2024-11-25 11:55:46,423:INFO:                 ray: Not installed
2024-11-25 11:55:46,423:INFO:            hyperopt: Not installed
2024-11-25 11:55:46,423:INFO:              optuna: Not installed
2024-11-25 11:55:46,423:INFO:               skopt: Not installed
2024-11-25 11:55:46,423:INFO:              mlflow: Not installed
2024-11-25 11:55:46,423:INFO:              gradio: Not installed
2024-11-25 11:55:46,423:INFO:             fastapi: Not installed
2024-11-25 11:55:46,423:INFO:             uvicorn: Not installed
2024-11-25 11:55:46,423:INFO:              m2cgen: Not installed
2024-11-25 11:55:46,423:INFO:           evidently: Not installed
2024-11-25 11:55:46,423:INFO:               fugue: Not installed
2024-11-25 11:55:46,423:INFO:           streamlit: Not installed
2024-11-25 11:55:46,423:INFO:             prophet: Not installed
2024-11-25 11:55:46,423:INFO:None
2024-11-25 11:55:46,423:INFO:Set up data.
2024-11-25 11:55:46,438:INFO:Set up folding strategy.
2024-11-25 11:55:46,438:INFO:Set up train/test split.
2024-11-25 11:55:46,454:INFO:Set up index.
2024-11-25 11:55:46,454:INFO:Assigning column types.
2024-11-25 11:55:46,454:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 11:55:46,501:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,501:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,570:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,570:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,623:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,655:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,655:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,655:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 11:55:46,702:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,786:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:46,808:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,808:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,808:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 11:55:46,887:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,887:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,971:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:46,971:INFO:Preparing preprocessing pipeline...
2024-11-25 11:55:46,971:INFO:Set up simple imputation.
2024-11-25 11:55:46,971:INFO:Set up encoding of categorical features.
2024-11-25 11:55:46,971:INFO:Set up column name cleaning.
2024-11-25 11:55:47,243:INFO:Finished creating preprocessing pipeline.
2024-11-25 11:55:47,250:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 11:55:47,250:INFO:Creating final display dataframe.
2024-11-25 11:55:47,548:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              f447
2024-11-25 11:55:47,617:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:47,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:47,696:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:47,696:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:47,696:INFO:setup() successfully completed in 1.33s...............
2024-11-25 11:55:52,334:INFO:PyCaret ClassificationExperiment
2024-11-25 11:55:52,334:INFO:Logging name: clf-default-name
2024-11-25 11:55:52,334:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 11:55:52,334:INFO:version 3.3.2
2024-11-25 11:55:52,334:INFO:Initializing setup()
2024-11-25 11:55:52,334:INFO:self.USI: 62f0
2024-11-25 11:55:52,334:INFO:self._variable_keys: {'html_param', 'idx', 'fix_imbalance', 'gpu_n_jobs_param', 'pipeline', 'data', 'X', 'seed', 'X_test', 'logging_param', 'X_train', 'y', 'fold_generator', 'is_multiclass', 'gpu_param', 'y_test', 'n_jobs_param', '_ml_usecase', 'target_param', 'fold_shuffle_param', 'fold_groups_param', 'USI', 'log_plots_param', 'y_train', 'exp_id', '_available_plots', 'exp_name_log', 'memory'}
2024-11-25 11:55:52,334:INFO:Checking environment
2024-11-25 11:55:52,334:INFO:python_version: 3.11.9
2024-11-25 11:55:52,334:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 11:55:52,334:INFO:machine: AMD64
2024-11-25 11:55:52,334:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 11:55:52,334:INFO:Memory: svmem(total=25525833728, available=14655074304, percent=42.6, used=10870759424, free=14655074304)
2024-11-25 11:55:52,334:INFO:Physical Core: 4
2024-11-25 11:55:52,334:INFO:Logical Core: 8
2024-11-25 11:55:52,334:INFO:Checking libraries
2024-11-25 11:55:52,334:INFO:System:
2024-11-25 11:55:52,334:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 11:55:52,334:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 11:55:52,334:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 11:55:52,334:INFO:PyCaret required dependencies:
2024-11-25 11:55:52,334:INFO:                 pip: 24.0
2024-11-25 11:55:52,334:INFO:          setuptools: 65.5.0
2024-11-25 11:55:52,334:INFO:             pycaret: 3.3.2
2024-11-25 11:55:52,334:INFO:             IPython: 8.29.0
2024-11-25 11:55:52,334:INFO:          ipywidgets: 8.1.5
2024-11-25 11:55:52,334:INFO:                tqdm: 4.66.6
2024-11-25 11:55:52,334:INFO:               numpy: 1.26.4
2024-11-25 11:55:52,334:INFO:              pandas: 2.1.4
2024-11-25 11:55:52,334:INFO:              jinja2: 3.1.4
2024-11-25 11:55:52,334:INFO:               scipy: 1.11.4
2024-11-25 11:55:52,334:INFO:              joblib: 1.3.2
2024-11-25 11:55:52,334:INFO:             sklearn: 1.4.2
2024-11-25 11:55:52,334:INFO:                pyod: 2.0.2
2024-11-25 11:55:52,334:INFO:            imblearn: 0.12.4
2024-11-25 11:55:52,334:INFO:   category_encoders: 2.6.4
2024-11-25 11:55:52,334:INFO:            lightgbm: 4.5.0
2024-11-25 11:55:52,334:INFO:               numba: 0.60.0
2024-11-25 11:55:52,334:INFO:            requests: 2.32.3
2024-11-25 11:55:52,334:INFO:          matplotlib: 3.7.5
2024-11-25 11:55:52,334:INFO:          scikitplot: 0.3.7
2024-11-25 11:55:52,334:INFO:         yellowbrick: 1.5
2024-11-25 11:55:52,334:INFO:              plotly: 5.24.1
2024-11-25 11:55:52,334:INFO:    plotly-resampler: Not installed
2024-11-25 11:55:52,334:INFO:             kaleido: 0.2.1
2024-11-25 11:55:52,334:INFO:           schemdraw: 0.15
2024-11-25 11:55:52,334:INFO:         statsmodels: 0.14.4
2024-11-25 11:55:52,334:INFO:              sktime: 0.26.0
2024-11-25 11:55:52,334:INFO:               tbats: 1.1.3
2024-11-25 11:55:52,334:INFO:            pmdarima: 2.0.4
2024-11-25 11:55:52,334:INFO:              psutil: 6.1.0
2024-11-25 11:55:52,334:INFO:          markupsafe: 3.0.2
2024-11-25 11:55:52,334:INFO:             pickle5: Not installed
2024-11-25 11:55:52,334:INFO:         cloudpickle: 3.1.0
2024-11-25 11:55:52,334:INFO:         deprecation: 2.1.0
2024-11-25 11:55:52,334:INFO:              xxhash: 3.5.0
2024-11-25 11:55:52,334:INFO:           wurlitzer: 3.1.1
2024-11-25 11:55:52,334:INFO:PyCaret optional dependencies:
2024-11-25 11:55:52,334:INFO:                shap: Not installed
2024-11-25 11:55:52,334:INFO:           interpret: Not installed
2024-11-25 11:55:52,334:INFO:                umap: Not installed
2024-11-25 11:55:52,334:INFO:     ydata_profiling: Not installed
2024-11-25 11:55:52,334:INFO:  explainerdashboard: Not installed
2024-11-25 11:55:52,334:INFO:             autoviz: Not installed
2024-11-25 11:55:52,334:INFO:           fairlearn: Not installed
2024-11-25 11:55:52,334:INFO:          deepchecks: Not installed
2024-11-25 11:55:52,334:INFO:             xgboost: Not installed
2024-11-25 11:55:52,334:INFO:            catboost: Not installed
2024-11-25 11:55:52,334:INFO:              kmodes: Not installed
2024-11-25 11:55:52,334:INFO:             mlxtend: Not installed
2024-11-25 11:55:52,334:INFO:       statsforecast: Not installed
2024-11-25 11:55:52,334:INFO:        tune_sklearn: Not installed
2024-11-25 11:55:52,334:INFO:                 ray: Not installed
2024-11-25 11:55:52,334:INFO:            hyperopt: Not installed
2024-11-25 11:55:52,334:INFO:              optuna: Not installed
2024-11-25 11:55:52,334:INFO:               skopt: Not installed
2024-11-25 11:55:52,334:INFO:              mlflow: Not installed
2024-11-25 11:55:52,334:INFO:              gradio: Not installed
2024-11-25 11:55:52,334:INFO:             fastapi: Not installed
2024-11-25 11:55:52,334:INFO:             uvicorn: Not installed
2024-11-25 11:55:52,334:INFO:              m2cgen: Not installed
2024-11-25 11:55:52,334:INFO:           evidently: Not installed
2024-11-25 11:55:52,334:INFO:               fugue: Not installed
2024-11-25 11:55:52,334:INFO:           streamlit: Not installed
2024-11-25 11:55:52,334:INFO:             prophet: Not installed
2024-11-25 11:55:52,334:INFO:None
2024-11-25 11:55:52,334:INFO:Set up data.
2024-11-25 11:55:52,350:INFO:Set up folding strategy.
2024-11-25 11:55:52,350:INFO:Set up train/test split.
2024-11-25 11:55:52,350:INFO:Set up index.
2024-11-25 11:55:52,350:INFO:Assigning column types.
2024-11-25 11:55:52,350:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 11:55:52,419:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,419:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,450:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,450:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,488:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,488:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,519:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,519:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,519:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 11:55:52,566:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,596:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,645:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 11:55:52,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,668:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 11:55:52,749:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,749:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,818:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:52,818:INFO:Preparing preprocessing pipeline...
2024-11-25 11:55:52,833:INFO:Set up simple imputation.
2024-11-25 11:55:52,833:INFO:Set up encoding of categorical features.
2024-11-25 11:55:52,833:INFO:Set up column name cleaning.
2024-11-25 11:55:52,981:INFO:Finished creating preprocessing pipeline.
2024-11-25 11:55:52,987:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 11:55:52,987:INFO:Creating final display dataframe.
2024-11-25 11:55:53,271:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              62f0
2024-11-25 11:55:53,338:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:53,338:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:53,407:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:53,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 11:55:53,407:INFO:setup() successfully completed in 1.07s...............
2024-11-25 11:55:58,099:INFO:Initializing compare_models()
2024-11-25 11:55:58,099:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 11:55:58,099:INFO:Checking exceptions
2024-11-25 11:55:58,105:INFO:Preparing display monitor
2024-11-25 11:55:58,121:INFO:Initializing Logistic Regression
2024-11-25 11:55:58,121:INFO:Total runtime is 0.0 minutes
2024-11-25 11:55:58,121:INFO:SubProcess create_model() called ==================================
2024-11-25 11:55:58,121:INFO:Initializing create_model()
2024-11-25 11:55:58,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:55:58,121:INFO:Checking exceptions
2024-11-25 11:55:58,121:INFO:Importing libraries
2024-11-25 11:55:58,121:INFO:Copying training dataset
2024-11-25 11:55:58,136:INFO:Defining folds
2024-11-25 11:55:58,136:INFO:Declaring metric variables
2024-11-25 11:55:58,136:INFO:Importing untrained model
2024-11-25 11:55:58,136:INFO:Logistic Regression Imported successfully
2024-11-25 11:55:58,168:INFO:Starting cross validation
2024-11-25 11:55:58,168:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:01,286:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:01,297:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:01,307:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:01,383:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:01,393:INFO:Calculating mean and std
2024-11-25 11:56:01,393:INFO:Creating metrics dataframe
2024-11-25 11:56:01,396:INFO:Uploading results into container
2024-11-25 11:56:01,396:INFO:Uploading model into container now
2024-11-25 11:56:01,396:INFO:_master_model_container: 1
2024-11-25 11:56:01,396:INFO:_display_container: 2
2024-11-25 11:56:01,396:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 11:56:01,396:INFO:create_model() successfully completed......................................
2024-11-25 11:56:01,478:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:01,478:INFO:Creating metrics dataframe
2024-11-25 11:56:01,488:INFO:Initializing K Neighbors Classifier
2024-11-25 11:56:01,488:INFO:Total runtime is 0.05612099568049113 minutes
2024-11-25 11:56:01,488:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:01,488:INFO:Initializing create_model()
2024-11-25 11:56:01,488:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:01,488:INFO:Checking exceptions
2024-11-25 11:56:01,488:INFO:Importing libraries
2024-11-25 11:56:01,488:INFO:Copying training dataset
2024-11-25 11:56:01,488:INFO:Defining folds
2024-11-25 11:56:01,488:INFO:Declaring metric variables
2024-11-25 11:56:01,498:INFO:Importing untrained model
2024-11-25 11:56:01,498:INFO:K Neighbors Classifier Imported successfully
2024-11-25 11:56:01,498:INFO:Starting cross validation
2024-11-25 11:56:01,514:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:04,195:INFO:Calculating mean and std
2024-11-25 11:56:04,195:INFO:Creating metrics dataframe
2024-11-25 11:56:04,195:INFO:Uploading results into container
2024-11-25 11:56:04,195:INFO:Uploading model into container now
2024-11-25 11:56:04,195:INFO:_master_model_container: 2
2024-11-25 11:56:04,195:INFO:_display_container: 2
2024-11-25 11:56:04,195:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 11:56:04,195:INFO:create_model() successfully completed......................................
2024-11-25 11:56:04,306:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:04,306:INFO:Creating metrics dataframe
2024-11-25 11:56:04,306:INFO:Initializing Naive Bayes
2024-11-25 11:56:04,306:INFO:Total runtime is 0.10309410095214844 minutes
2024-11-25 11:56:04,322:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:04,322:INFO:Initializing create_model()
2024-11-25 11:56:04,322:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:04,322:INFO:Checking exceptions
2024-11-25 11:56:04,322:INFO:Importing libraries
2024-11-25 11:56:04,322:INFO:Copying training dataset
2024-11-25 11:56:04,322:INFO:Defining folds
2024-11-25 11:56:04,322:INFO:Declaring metric variables
2024-11-25 11:56:04,322:INFO:Importing untrained model
2024-11-25 11:56:04,338:INFO:Naive Bayes Imported successfully
2024-11-25 11:56:04,338:INFO:Starting cross validation
2024-11-25 11:56:04,338:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:04,569:INFO:Calculating mean and std
2024-11-25 11:56:04,569:INFO:Creating metrics dataframe
2024-11-25 11:56:04,569:INFO:Uploading results into container
2024-11-25 11:56:04,569:INFO:Uploading model into container now
2024-11-25 11:56:04,569:INFO:_master_model_container: 3
2024-11-25 11:56:04,569:INFO:_display_container: 2
2024-11-25 11:56:04,569:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 11:56:04,569:INFO:create_model() successfully completed......................................
2024-11-25 11:56:04,638:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:04,638:INFO:Creating metrics dataframe
2024-11-25 11:56:04,638:INFO:Initializing Decision Tree Classifier
2024-11-25 11:56:04,638:INFO:Total runtime is 0.10862478812535604 minutes
2024-11-25 11:56:04,654:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:04,654:INFO:Initializing create_model()
2024-11-25 11:56:04,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:04,654:INFO:Checking exceptions
2024-11-25 11:56:04,654:INFO:Importing libraries
2024-11-25 11:56:04,654:INFO:Copying training dataset
2024-11-25 11:56:04,654:INFO:Defining folds
2024-11-25 11:56:04,654:INFO:Declaring metric variables
2024-11-25 11:56:04,654:INFO:Importing untrained model
2024-11-25 11:56:04,670:INFO:Decision Tree Classifier Imported successfully
2024-11-25 11:56:04,676:INFO:Starting cross validation
2024-11-25 11:56:04,676:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:04,906:INFO:Calculating mean and std
2024-11-25 11:56:04,906:INFO:Creating metrics dataframe
2024-11-25 11:56:04,907:INFO:Uploading results into container
2024-11-25 11:56:04,907:INFO:Uploading model into container now
2024-11-25 11:56:04,907:INFO:_master_model_container: 4
2024-11-25 11:56:04,907:INFO:_display_container: 2
2024-11-25 11:56:04,907:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 11:56:04,907:INFO:create_model() successfully completed......................................
2024-11-25 11:56:04,983:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:04,983:INFO:Creating metrics dataframe
2024-11-25 11:56:04,991:INFO:Initializing SVM - Linear Kernel
2024-11-25 11:56:04,991:INFO:Total runtime is 0.11449888547261557 minutes
2024-11-25 11:56:04,991:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:04,991:INFO:Initializing create_model()
2024-11-25 11:56:04,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:04,991:INFO:Checking exceptions
2024-11-25 11:56:04,991:INFO:Importing libraries
2024-11-25 11:56:04,991:INFO:Copying training dataset
2024-11-25 11:56:04,991:INFO:Defining folds
2024-11-25 11:56:04,991:INFO:Declaring metric variables
2024-11-25 11:56:05,001:INFO:Importing untrained model
2024-11-25 11:56:05,001:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 11:56:05,001:INFO:Starting cross validation
2024-11-25 11:56:05,016:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:05,215:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,215:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,256:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,276:INFO:Calculating mean and std
2024-11-25 11:56:05,276:INFO:Creating metrics dataframe
2024-11-25 11:56:05,276:INFO:Uploading results into container
2024-11-25 11:56:05,276:INFO:Uploading model into container now
2024-11-25 11:56:05,276:INFO:_master_model_container: 5
2024-11-25 11:56:05,276:INFO:_display_container: 2
2024-11-25 11:56:05,276:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 11:56:05,276:INFO:create_model() successfully completed......................................
2024-11-25 11:56:05,348:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:05,348:INFO:Creating metrics dataframe
2024-11-25 11:56:05,348:INFO:Initializing Ridge Classifier
2024-11-25 11:56:05,348:INFO:Total runtime is 0.12045150995254517 minutes
2024-11-25 11:56:05,348:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:05,348:INFO:Initializing create_model()
2024-11-25 11:56:05,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:05,348:INFO:Checking exceptions
2024-11-25 11:56:05,348:INFO:Importing libraries
2024-11-25 11:56:05,348:INFO:Copying training dataset
2024-11-25 11:56:05,363:INFO:Defining folds
2024-11-25 11:56:05,363:INFO:Declaring metric variables
2024-11-25 11:56:05,363:INFO:Importing untrained model
2024-11-25 11:56:05,363:INFO:Ridge Classifier Imported successfully
2024-11-25 11:56:05,379:INFO:Starting cross validation
2024-11-25 11:56:05,379:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:05,608:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,618:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,618:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,618:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:05,649:INFO:Calculating mean and std
2024-11-25 11:56:05,649:INFO:Creating metrics dataframe
2024-11-25 11:56:05,649:INFO:Uploading results into container
2024-11-25 11:56:05,649:INFO:Uploading model into container now
2024-11-25 11:56:05,649:INFO:_master_model_container: 6
2024-11-25 11:56:05,649:INFO:_display_container: 2
2024-11-25 11:56:05,649:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 11:56:05,649:INFO:create_model() successfully completed......................................
2024-11-25 11:56:05,717:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:05,717:INFO:Creating metrics dataframe
2024-11-25 11:56:05,732:INFO:Initializing Random Forest Classifier
2024-11-25 11:56:05,732:INFO:Total runtime is 0.12686197757720946 minutes
2024-11-25 11:56:05,732:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:05,732:INFO:Initializing create_model()
2024-11-25 11:56:05,732:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:05,732:INFO:Checking exceptions
2024-11-25 11:56:05,732:INFO:Importing libraries
2024-11-25 11:56:05,732:INFO:Copying training dataset
2024-11-25 11:56:05,732:INFO:Defining folds
2024-11-25 11:56:05,732:INFO:Declaring metric variables
2024-11-25 11:56:05,732:INFO:Importing untrained model
2024-11-25 11:56:05,748:INFO:Random Forest Classifier Imported successfully
2024-11-25 11:56:05,748:INFO:Starting cross validation
2024-11-25 11:56:05,748:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:06,225:INFO:Calculating mean and std
2024-11-25 11:56:06,225:INFO:Creating metrics dataframe
2024-11-25 11:56:06,225:INFO:Uploading results into container
2024-11-25 11:56:06,225:INFO:Uploading model into container now
2024-11-25 11:56:06,225:INFO:_master_model_container: 7
2024-11-25 11:56:06,225:INFO:_display_container: 2
2024-11-25 11:56:06,225:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 11:56:06,225:INFO:create_model() successfully completed......................................
2024-11-25 11:56:06,299:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:06,299:INFO:Creating metrics dataframe
2024-11-25 11:56:06,299:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 11:56:06,299:INFO:Total runtime is 0.13630960782368975 minutes
2024-11-25 11:56:06,299:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:06,299:INFO:Initializing create_model()
2024-11-25 11:56:06,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:06,299:INFO:Checking exceptions
2024-11-25 11:56:06,299:INFO:Importing libraries
2024-11-25 11:56:06,299:INFO:Copying training dataset
2024-11-25 11:56:06,315:INFO:Defining folds
2024-11-25 11:56:06,315:INFO:Declaring metric variables
2024-11-25 11:56:06,315:INFO:Importing untrained model
2024-11-25 11:56:06,315:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 11:56:06,331:INFO:Starting cross validation
2024-11-25 11:56:06,331:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:06,486:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 11:56:06,486:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 11:56:06,506:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 11:56:06,516:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 11:56:06,536:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,536:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,556:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,557:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,577:INFO:Calculating mean and std
2024-11-25 11:56:06,577:INFO:Creating metrics dataframe
2024-11-25 11:56:06,577:INFO:Uploading results into container
2024-11-25 11:56:06,577:INFO:Uploading model into container now
2024-11-25 11:56:06,577:INFO:_master_model_container: 8
2024-11-25 11:56:06,577:INFO:_display_container: 2
2024-11-25 11:56:06,577:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 11:56:06,577:INFO:create_model() successfully completed......................................
2024-11-25 11:56:06,647:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:06,647:INFO:Creating metrics dataframe
2024-11-25 11:56:06,647:INFO:Initializing Ada Boost Classifier
2024-11-25 11:56:06,662:INFO:Total runtime is 0.14235535065333046 minutes
2024-11-25 11:56:06,662:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:06,662:INFO:Initializing create_model()
2024-11-25 11:56:06,662:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:06,662:INFO:Checking exceptions
2024-11-25 11:56:06,662:INFO:Importing libraries
2024-11-25 11:56:06,662:INFO:Copying training dataset
2024-11-25 11:56:06,662:INFO:Defining folds
2024-11-25 11:56:06,662:INFO:Declaring metric variables
2024-11-25 11:56:06,662:INFO:Importing untrained model
2024-11-25 11:56:06,678:INFO:Ada Boost Classifier Imported successfully
2024-11-25 11:56:06,684:INFO:Starting cross validation
2024-11-25 11:56:06,684:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:06,831:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 11:56:06,841:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 11:56:06,841:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 11:56:06,841:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 11:56:06,994:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,996:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:06,998:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,006:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,025:INFO:Calculating mean and std
2024-11-25 11:56:07,025:INFO:Creating metrics dataframe
2024-11-25 11:56:07,027:INFO:Uploading results into container
2024-11-25 11:56:07,027:INFO:Uploading model into container now
2024-11-25 11:56:07,027:INFO:_master_model_container: 9
2024-11-25 11:56:07,027:INFO:_display_container: 2
2024-11-25 11:56:07,027:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 11:56:07,027:INFO:create_model() successfully completed......................................
2024-11-25 11:56:07,100:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:07,100:INFO:Creating metrics dataframe
2024-11-25 11:56:07,100:INFO:Initializing Gradient Boosting Classifier
2024-11-25 11:56:07,100:INFO:Total runtime is 0.14964988629023232 minutes
2024-11-25 11:56:07,110:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:07,110:INFO:Initializing create_model()
2024-11-25 11:56:07,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:07,110:INFO:Checking exceptions
2024-11-25 11:56:07,110:INFO:Importing libraries
2024-11-25 11:56:07,110:INFO:Copying training dataset
2024-11-25 11:56:07,110:INFO:Defining folds
2024-11-25 11:56:07,110:INFO:Declaring metric variables
2024-11-25 11:56:07,110:INFO:Importing untrained model
2024-11-25 11:56:07,125:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 11:56:07,125:INFO:Starting cross validation
2024-11-25 11:56:07,125:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:07,647:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,691:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,720:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,740:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:07,750:INFO:Calculating mean and std
2024-11-25 11:56:07,750:INFO:Creating metrics dataframe
2024-11-25 11:56:07,750:INFO:Uploading results into container
2024-11-25 11:56:07,750:INFO:Uploading model into container now
2024-11-25 11:56:07,750:INFO:_master_model_container: 10
2024-11-25 11:56:07,750:INFO:_display_container: 2
2024-11-25 11:56:07,750:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 11:56:07,750:INFO:create_model() successfully completed......................................
2024-11-25 11:56:07,820:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:07,820:INFO:Creating metrics dataframe
2024-11-25 11:56:07,830:INFO:Initializing Linear Discriminant Analysis
2024-11-25 11:56:07,830:INFO:Total runtime is 0.1618269642194112 minutes
2024-11-25 11:56:07,830:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:07,830:INFO:Initializing create_model()
2024-11-25 11:56:07,830:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:07,830:INFO:Checking exceptions
2024-11-25 11:56:07,830:INFO:Importing libraries
2024-11-25 11:56:07,830:INFO:Copying training dataset
2024-11-25 11:56:07,830:INFO:Defining folds
2024-11-25 11:56:07,830:INFO:Declaring metric variables
2024-11-25 11:56:07,846:INFO:Importing untrained model
2024-11-25 11:56:07,846:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 11:56:07,846:INFO:Starting cross validation
2024-11-25 11:56:07,846:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:08,041:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:08,043:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:08,071:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:08,073:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 11:56:08,090:INFO:Calculating mean and std
2024-11-25 11:56:08,090:INFO:Creating metrics dataframe
2024-11-25 11:56:08,092:INFO:Uploading results into container
2024-11-25 11:56:08,092:INFO:Uploading model into container now
2024-11-25 11:56:08,092:INFO:_master_model_container: 11
2024-11-25 11:56:08,092:INFO:_display_container: 2
2024-11-25 11:56:08,092:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 11:56:08,092:INFO:create_model() successfully completed......................................
2024-11-25 11:56:08,166:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:08,166:INFO:Creating metrics dataframe
2024-11-25 11:56:08,174:INFO:Initializing Extra Trees Classifier
2024-11-25 11:56:08,174:INFO:Total runtime is 0.16755551099777222 minutes
2024-11-25 11:56:08,174:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:08,174:INFO:Initializing create_model()
2024-11-25 11:56:08,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:08,174:INFO:Checking exceptions
2024-11-25 11:56:08,174:INFO:Importing libraries
2024-11-25 11:56:08,174:INFO:Copying training dataset
2024-11-25 11:56:08,184:INFO:Defining folds
2024-11-25 11:56:08,184:INFO:Declaring metric variables
2024-11-25 11:56:08,184:INFO:Importing untrained model
2024-11-25 11:56:08,184:INFO:Extra Trees Classifier Imported successfully
2024-11-25 11:56:08,184:INFO:Starting cross validation
2024-11-25 11:56:08,200:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:08,650:INFO:Calculating mean and std
2024-11-25 11:56:08,650:INFO:Creating metrics dataframe
2024-11-25 11:56:08,650:INFO:Uploading results into container
2024-11-25 11:56:08,650:INFO:Uploading model into container now
2024-11-25 11:56:08,650:INFO:_master_model_container: 12
2024-11-25 11:56:08,650:INFO:_display_container: 2
2024-11-25 11:56:08,650:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 11:56:08,650:INFO:create_model() successfully completed......................................
2024-11-25 11:56:08,716:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:08,716:INFO:Creating metrics dataframe
2024-11-25 11:56:08,731:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 11:56:08,731:INFO:Total runtime is 0.17684184710184733 minutes
2024-11-25 11:56:08,731:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:08,731:INFO:Initializing create_model()
2024-11-25 11:56:08,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:08,731:INFO:Checking exceptions
2024-11-25 11:56:08,731:INFO:Importing libraries
2024-11-25 11:56:08,731:INFO:Copying training dataset
2024-11-25 11:56:08,731:INFO:Defining folds
2024-11-25 11:56:08,731:INFO:Declaring metric variables
2024-11-25 11:56:08,731:INFO:Importing untrained model
2024-11-25 11:56:08,747:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 11:56:08,747:INFO:Starting cross validation
2024-11-25 11:56:08,763:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:09,467:INFO:Calculating mean and std
2024-11-25 11:56:09,468:INFO:Creating metrics dataframe
2024-11-25 11:56:09,468:INFO:Uploading results into container
2024-11-25 11:56:09,468:INFO:Uploading model into container now
2024-11-25 11:56:09,468:INFO:_master_model_container: 13
2024-11-25 11:56:09,468:INFO:_display_container: 2
2024-11-25 11:56:09,468:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 11:56:09,468:INFO:create_model() successfully completed......................................
2024-11-25 11:56:09,569:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:09,569:INFO:Creating metrics dataframe
2024-11-25 11:56:09,580:INFO:Initializing Dummy Classifier
2024-11-25 11:56:09,580:INFO:Total runtime is 0.19098103046417236 minutes
2024-11-25 11:56:09,580:INFO:SubProcess create_model() called ==================================
2024-11-25 11:56:09,588:INFO:Initializing create_model()
2024-11-25 11:56:09,588:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A0B78FF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:09,588:INFO:Checking exceptions
2024-11-25 11:56:09,588:INFO:Importing libraries
2024-11-25 11:56:09,588:INFO:Copying training dataset
2024-11-25 11:56:09,588:INFO:Defining folds
2024-11-25 11:56:09,588:INFO:Declaring metric variables
2024-11-25 11:56:09,588:INFO:Importing untrained model
2024-11-25 11:56:09,598:INFO:Dummy Classifier Imported successfully
2024-11-25 11:56:09,608:INFO:Starting cross validation
2024-11-25 11:56:09,608:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 11:56:09,854:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 11:56:09,865:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 11:56:09,878:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 11:56:09,881:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 11:56:09,895:INFO:Calculating mean and std
2024-11-25 11:56:09,896:INFO:Creating metrics dataframe
2024-11-25 11:56:09,896:INFO:Uploading results into container
2024-11-25 11:56:09,899:INFO:Uploading model into container now
2024-11-25 11:56:09,899:INFO:_master_model_container: 14
2024-11-25 11:56:09,899:INFO:_display_container: 2
2024-11-25 11:56:09,899:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 11:56:09,900:INFO:create_model() successfully completed......................................
2024-11-25 11:56:09,964:INFO:SubProcess create_model() end ==================================
2024-11-25 11:56:09,964:INFO:Creating metrics dataframe
2024-11-25 11:56:09,980:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 11:56:09,996:INFO:Initializing create_model()
2024-11-25 11:56:09,996:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 11:56:09,996:INFO:Checking exceptions
2024-11-25 11:56:09,996:INFO:Importing libraries
2024-11-25 11:56:09,996:INFO:Copying training dataset
2024-11-25 11:56:09,999:INFO:Defining folds
2024-11-25 11:56:09,999:INFO:Declaring metric variables
2024-11-25 11:56:09,999:INFO:Importing untrained model
2024-11-25 11:56:09,999:INFO:Declaring custom model
2024-11-25 11:56:09,999:INFO:Ridge Classifier Imported successfully
2024-11-25 11:56:09,999:INFO:Cross validation set to False
2024-11-25 11:56:09,999:INFO:Fitting Model
2024-11-25 11:56:10,099:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 11:56:10,099:INFO:create_model() successfully completed......................................
2024-11-25 11:56:10,199:INFO:_master_model_container: 14
2024-11-25 11:56:10,199:INFO:_display_container: 2
2024-11-25 11:56:10,199:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 11:56:10,199:INFO:compare_models() successfully completed......................................
2024-11-25 12:01:02,373:INFO:Initializing plot_model()
2024-11-25 12:01:02,373:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:01:02,373:INFO:Checking exceptions
2024-11-25 12:01:02,380:INFO:Preloading libraries
2024-11-25 12:01:02,381:INFO:Copying training dataset
2024-11-25 12:01:02,381:INFO:Plot type: confusion_matrix
2024-11-25 12:01:02,802:INFO:Fitting Model
2024-11-25 12:01:02,803:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:01:02,803:INFO:Scoring test/hold-out set
2024-11-25 12:01:02,956:INFO:Visual Rendered Successfully
2024-11-25 12:01:03,044:INFO:plot_model() successfully completed......................................
2024-11-25 12:04:13,152:INFO:Initializing finalize_model()
2024-11-25 12:04:13,152:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:04:13,153:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:04:13,157:INFO:Initializing create_model()
2024-11-25 12:04:13,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A0B77A8A50>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:04:13,158:INFO:Checking exceptions
2024-11-25 12:04:13,160:INFO:Importing libraries
2024-11-25 12:04:13,160:INFO:Copying training dataset
2024-11-25 12:04:13,161:INFO:Defining folds
2024-11-25 12:04:13,161:INFO:Declaring metric variables
2024-11-25 12:04:13,162:INFO:Importing untrained model
2024-11-25 12:04:13,162:INFO:Declaring custom model
2024-11-25 12:04:13,162:INFO:Ridge Classifier Imported successfully
2024-11-25 12:04:13,165:INFO:Cross validation set to False
2024-11-25 12:04:13,165:INFO:Fitting Model
2024-11-25 12:04:13,296:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:04:13,296:INFO:create_model() successfully completed......................................
2024-11-25 12:04:13,383:INFO:_master_model_container: 14
2024-11-25 12:04:13,383:INFO:_display_container: 2
2024-11-25 12:04:13,391:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:04:13,391:INFO:finalize_model() successfully completed......................................
2024-11-25 12:12:50,342:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_4d6a0a736b4d4a8e9ab10cada6ff52f3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,342:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_9195987f300441b996e8c6994be1caf4_440f0bcf4da14f4fa740a7982516f46c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,342:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_17d71d93d04c4fd28a60844c2368e48e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_8b1e09e6db794368bf310d4b3dbd85af_0d61ec5254044b69948967bbb895d1a0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_863e84aca5514ac79c19d53d063f6416
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_f76883bef8ee4bf0a595c07b6f57a3cc_767b0487aef7476597bb06ef1ac5b459
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_e83c56c72b504aee8e8b2beb15ac380a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7b76ab6343d74a0b8f93e1001feba986_b4135b2e19904b5bb43715e51dd23936
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_3b2afabff08e430f8173222230274601
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_9139f6d61e094709bfcc248a0d48e29d_3ef34d68a0e24497a2aa47299a1f43a5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_6d8e6b1f44d1479fb27c8874b54b9172
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_44ed6f90845f4323ba3b9f91f8d66fba_48c226bd061c48c193538c66b9a17220
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_6698360ef28b4528bb02c286d3735506
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_56381ac40db54981a861878a17a1edd2_e5d0e06c311c476fbdff6ad16f843993
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_0aaa3bf62c34485f95fab8d58aae7a5f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_23140f849d144924a94e3654d9717106_23665799cc094622aef9676f9b211c28
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_976e40c1e8c3442588af78a45efdaf85
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_8471bbf3a5c04aa89531e5e8f4536c25_d7e74044dbc44231bd749ac8c96a103c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_989806d6908241e1bc67bf09f3b46835
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_e5e1cd981e67453faa6b93816eff721c_b5c9f48f1e514204a7ce215697a9243c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_9eefa9838fc14adbb3d6553421fe7092
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_b36da475c85749f282aa32a26cb46e25_0487531e572f43099d801c9175e1e6ee
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_7bccbd7bc4544c0a9e6d7a37e629ea45
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_fe55c4b2861f4269969433e9c5f11e29_9f86cbb8dc7949e0af6d4d2438d5d480
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_36fffc3613794521a1c7314c311de93d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_8de47e3b33a1435e8eeb7d639ee9d621_9d0364b6ef564912a649b1ea5e5a2c00
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_5ce688aea2804b4abb870996b882d804
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:12:50,346:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_1112_7ce0af98baa04d718aa30750cab3907c_981f1163b6f74717b8b8225382850557
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:13:00,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:13:00,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:13:00,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:13:00,047:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:13:00,498:INFO:PyCaret ClassificationExperiment
2024-11-25 12:13:00,502:INFO:Logging name: clf-default-name
2024-11-25 12:13:00,502:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:13:00,502:INFO:version 3.3.2
2024-11-25 12:13:00,502:INFO:Initializing setup()
2024-11-25 12:13:00,502:INFO:self.USI: 1961
2024-11-25 12:13:00,502:INFO:self._variable_keys: {'idx', 'n_jobs_param', 'X_train', 'data', 'seed', 'memory', 'fold_groups_param', 'fold_generator', 'X', '_available_plots', 'is_multiclass', '_ml_usecase', 'gpu_param', 'html_param', 'y', 'fold_shuffle_param', 'X_test', 'target_param', 'y_train', 'pipeline', 'gpu_n_jobs_param', 'log_plots_param', 'exp_name_log', 'exp_id', 'USI', 'y_test', 'logging_param', 'fix_imbalance'}
2024-11-25 12:13:00,502:INFO:Checking environment
2024-11-25 12:13:00,502:INFO:python_version: 3.11.9
2024-11-25 12:13:00,502:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:13:00,502:INFO:machine: AMD64
2024-11-25 12:13:00,502:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:13:00,506:INFO:Memory: svmem(total=25525833728, available=12991107072, percent=49.1, used=12534726656, free=12991107072)
2024-11-25 12:13:00,506:INFO:Physical Core: 4
2024-11-25 12:13:00,506:INFO:Logical Core: 8
2024-11-25 12:13:00,506:INFO:Checking libraries
2024-11-25 12:13:00,506:INFO:System:
2024-11-25 12:13:00,506:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:13:00,506:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:13:00,506:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:13:00,506:INFO:PyCaret required dependencies:
2024-11-25 12:13:00,531:INFO:                 pip: 24.0
2024-11-25 12:13:00,531:INFO:          setuptools: 65.5.0
2024-11-25 12:13:00,531:INFO:             pycaret: 3.3.2
2024-11-25 12:13:00,531:INFO:             IPython: 8.29.0
2024-11-25 12:13:00,531:INFO:          ipywidgets: 8.1.5
2024-11-25 12:13:00,531:INFO:                tqdm: 4.66.6
2024-11-25 12:13:00,531:INFO:               numpy: 1.26.4
2024-11-25 12:13:00,531:INFO:              pandas: 2.1.4
2024-11-25 12:13:00,531:INFO:              jinja2: 3.1.4
2024-11-25 12:13:00,531:INFO:               scipy: 1.11.4
2024-11-25 12:13:00,531:INFO:              joblib: 1.3.2
2024-11-25 12:13:00,531:INFO:             sklearn: 1.4.2
2024-11-25 12:13:00,531:INFO:                pyod: 2.0.2
2024-11-25 12:13:00,531:INFO:            imblearn: 0.12.4
2024-11-25 12:13:00,531:INFO:   category_encoders: 2.6.4
2024-11-25 12:13:00,531:INFO:            lightgbm: 4.5.0
2024-11-25 12:13:00,531:INFO:               numba: 0.60.0
2024-11-25 12:13:00,531:INFO:            requests: 2.32.3
2024-11-25 12:13:00,531:INFO:          matplotlib: 3.7.5
2024-11-25 12:13:00,531:INFO:          scikitplot: 0.3.7
2024-11-25 12:13:00,531:INFO:         yellowbrick: 1.5
2024-11-25 12:13:00,531:INFO:              plotly: 5.24.1
2024-11-25 12:13:00,531:INFO:    plotly-resampler: Not installed
2024-11-25 12:13:00,531:INFO:             kaleido: 0.2.1
2024-11-25 12:13:00,531:INFO:           schemdraw: 0.15
2024-11-25 12:13:00,531:INFO:         statsmodels: 0.14.4
2024-11-25 12:13:00,531:INFO:              sktime: 0.26.0
2024-11-25 12:13:00,531:INFO:               tbats: 1.1.3
2024-11-25 12:13:00,531:INFO:            pmdarima: 2.0.4
2024-11-25 12:13:00,531:INFO:              psutil: 6.1.0
2024-11-25 12:13:00,531:INFO:          markupsafe: 3.0.2
2024-11-25 12:13:00,531:INFO:             pickle5: Not installed
2024-11-25 12:13:00,531:INFO:         cloudpickle: 3.1.0
2024-11-25 12:13:00,531:INFO:         deprecation: 2.1.0
2024-11-25 12:13:00,531:INFO:              xxhash: 3.5.0
2024-11-25 12:13:00,531:INFO:           wurlitzer: 3.1.1
2024-11-25 12:13:00,531:INFO:PyCaret optional dependencies:
2024-11-25 12:13:00,543:INFO:                shap: Not installed
2024-11-25 12:13:00,543:INFO:           interpret: Not installed
2024-11-25 12:13:00,543:INFO:                umap: Not installed
2024-11-25 12:13:00,543:INFO:     ydata_profiling: Not installed
2024-11-25 12:13:00,543:INFO:  explainerdashboard: Not installed
2024-11-25 12:13:00,543:INFO:             autoviz: Not installed
2024-11-25 12:13:00,543:INFO:           fairlearn: Not installed
2024-11-25 12:13:00,543:INFO:          deepchecks: Not installed
2024-11-25 12:13:00,543:INFO:             xgboost: Not installed
2024-11-25 12:13:00,543:INFO:            catboost: Not installed
2024-11-25 12:13:00,543:INFO:              kmodes: Not installed
2024-11-25 12:13:00,543:INFO:             mlxtend: Not installed
2024-11-25 12:13:00,543:INFO:       statsforecast: Not installed
2024-11-25 12:13:00,543:INFO:        tune_sklearn: Not installed
2024-11-25 12:13:00,543:INFO:                 ray: Not installed
2024-11-25 12:13:00,543:INFO:            hyperopt: Not installed
2024-11-25 12:13:00,543:INFO:              optuna: Not installed
2024-11-25 12:13:00,543:INFO:               skopt: Not installed
2024-11-25 12:13:00,543:INFO:              mlflow: Not installed
2024-11-25 12:13:00,543:INFO:              gradio: Not installed
2024-11-25 12:13:00,543:INFO:             fastapi: Not installed
2024-11-25 12:13:00,543:INFO:             uvicorn: Not installed
2024-11-25 12:13:00,543:INFO:              m2cgen: Not installed
2024-11-25 12:13:00,543:INFO:           evidently: Not installed
2024-11-25 12:13:00,543:INFO:               fugue: Not installed
2024-11-25 12:13:00,543:INFO:           streamlit: Not installed
2024-11-25 12:13:00,543:INFO:             prophet: Not installed
2024-11-25 12:13:00,543:INFO:None
2024-11-25 12:13:00,543:INFO:Set up data.
2024-11-25 12:13:00,555:INFO:Set up folding strategy.
2024-11-25 12:13:00,555:INFO:Set up train/test split.
2024-11-25 12:13:00,559:INFO:Set up index.
2024-11-25 12:13:00,559:INFO:Assigning column types.
2024-11-25 12:13:00,563:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:13:00,602:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,607:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,638:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,638:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,694:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,727:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,727:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,727:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:13:00,774:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,806:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,806:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,851:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:00,883:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,883:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,883:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:13:00,955:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:00,955:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,046:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,048:INFO:Preparing preprocessing pipeline...
2024-11-25 12:13:01,048:INFO:Set up simple imputation.
2024-11-25 12:13:01,056:INFO:Set up encoding of categorical features.
2024-11-25 12:13:01,056:INFO:Set up column name cleaning.
2024-11-25 12:13:01,215:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:13:01,215:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:13:01,215:INFO:Creating final display dataframe.
2024-11-25 12:13:01,402:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              1961
2024-11-25 12:13:01,483:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,483:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,552:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,552:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,552:INFO:setup() successfully completed in 1.06s...............
2024-11-25 12:13:01,566:INFO:PyCaret ClassificationExperiment
2024-11-25 12:13:01,566:INFO:Logging name: clf-default-name
2024-11-25 12:13:01,566:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:13:01,566:INFO:version 3.3.2
2024-11-25 12:13:01,566:INFO:Initializing setup()
2024-11-25 12:13:01,566:INFO:self.USI: 200b
2024-11-25 12:13:01,566:INFO:self._variable_keys: {'idx', 'n_jobs_param', 'X_train', 'data', 'seed', 'memory', 'fold_groups_param', 'fold_generator', 'X', '_available_plots', 'is_multiclass', '_ml_usecase', 'gpu_param', 'html_param', 'y', 'fold_shuffle_param', 'X_test', 'target_param', 'y_train', 'pipeline', 'gpu_n_jobs_param', 'log_plots_param', 'exp_name_log', 'exp_id', 'USI', 'y_test', 'logging_param', 'fix_imbalance'}
2024-11-25 12:13:01,566:INFO:Checking environment
2024-11-25 12:13:01,566:INFO:python_version: 3.11.9
2024-11-25 12:13:01,566:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:13:01,566:INFO:machine: AMD64
2024-11-25 12:13:01,566:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:13:01,566:INFO:Memory: svmem(total=25525833728, available=12996096000, percent=49.1, used=12529737728, free=12996096000)
2024-11-25 12:13:01,566:INFO:Physical Core: 4
2024-11-25 12:13:01,566:INFO:Logical Core: 8
2024-11-25 12:13:01,566:INFO:Checking libraries
2024-11-25 12:13:01,566:INFO:System:
2024-11-25 12:13:01,566:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:13:01,566:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:13:01,566:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:13:01,566:INFO:PyCaret required dependencies:
2024-11-25 12:13:01,566:INFO:                 pip: 24.0
2024-11-25 12:13:01,566:INFO:          setuptools: 65.5.0
2024-11-25 12:13:01,566:INFO:             pycaret: 3.3.2
2024-11-25 12:13:01,566:INFO:             IPython: 8.29.0
2024-11-25 12:13:01,566:INFO:          ipywidgets: 8.1.5
2024-11-25 12:13:01,566:INFO:                tqdm: 4.66.6
2024-11-25 12:13:01,566:INFO:               numpy: 1.26.4
2024-11-25 12:13:01,566:INFO:              pandas: 2.1.4
2024-11-25 12:13:01,566:INFO:              jinja2: 3.1.4
2024-11-25 12:13:01,566:INFO:               scipy: 1.11.4
2024-11-25 12:13:01,566:INFO:              joblib: 1.3.2
2024-11-25 12:13:01,566:INFO:             sklearn: 1.4.2
2024-11-25 12:13:01,566:INFO:                pyod: 2.0.2
2024-11-25 12:13:01,566:INFO:            imblearn: 0.12.4
2024-11-25 12:13:01,566:INFO:   category_encoders: 2.6.4
2024-11-25 12:13:01,566:INFO:            lightgbm: 4.5.0
2024-11-25 12:13:01,566:INFO:               numba: 0.60.0
2024-11-25 12:13:01,566:INFO:            requests: 2.32.3
2024-11-25 12:13:01,566:INFO:          matplotlib: 3.7.5
2024-11-25 12:13:01,566:INFO:          scikitplot: 0.3.7
2024-11-25 12:13:01,566:INFO:         yellowbrick: 1.5
2024-11-25 12:13:01,566:INFO:              plotly: 5.24.1
2024-11-25 12:13:01,566:INFO:    plotly-resampler: Not installed
2024-11-25 12:13:01,566:INFO:             kaleido: 0.2.1
2024-11-25 12:13:01,566:INFO:           schemdraw: 0.15
2024-11-25 12:13:01,566:INFO:         statsmodels: 0.14.4
2024-11-25 12:13:01,566:INFO:              sktime: 0.26.0
2024-11-25 12:13:01,566:INFO:               tbats: 1.1.3
2024-11-25 12:13:01,566:INFO:            pmdarima: 2.0.4
2024-11-25 12:13:01,566:INFO:              psutil: 6.1.0
2024-11-25 12:13:01,566:INFO:          markupsafe: 3.0.2
2024-11-25 12:13:01,566:INFO:             pickle5: Not installed
2024-11-25 12:13:01,566:INFO:         cloudpickle: 3.1.0
2024-11-25 12:13:01,566:INFO:         deprecation: 2.1.0
2024-11-25 12:13:01,566:INFO:              xxhash: 3.5.0
2024-11-25 12:13:01,566:INFO:           wurlitzer: 3.1.1
2024-11-25 12:13:01,566:INFO:PyCaret optional dependencies:
2024-11-25 12:13:01,566:INFO:                shap: Not installed
2024-11-25 12:13:01,566:INFO:           interpret: Not installed
2024-11-25 12:13:01,566:INFO:                umap: Not installed
2024-11-25 12:13:01,566:INFO:     ydata_profiling: Not installed
2024-11-25 12:13:01,566:INFO:  explainerdashboard: Not installed
2024-11-25 12:13:01,566:INFO:             autoviz: Not installed
2024-11-25 12:13:01,566:INFO:           fairlearn: Not installed
2024-11-25 12:13:01,566:INFO:          deepchecks: Not installed
2024-11-25 12:13:01,566:INFO:             xgboost: Not installed
2024-11-25 12:13:01,566:INFO:            catboost: Not installed
2024-11-25 12:13:01,566:INFO:              kmodes: Not installed
2024-11-25 12:13:01,566:INFO:             mlxtend: Not installed
2024-11-25 12:13:01,566:INFO:       statsforecast: Not installed
2024-11-25 12:13:01,566:INFO:        tune_sklearn: Not installed
2024-11-25 12:13:01,566:INFO:                 ray: Not installed
2024-11-25 12:13:01,566:INFO:            hyperopt: Not installed
2024-11-25 12:13:01,566:INFO:              optuna: Not installed
2024-11-25 12:13:01,566:INFO:               skopt: Not installed
2024-11-25 12:13:01,566:INFO:              mlflow: Not installed
2024-11-25 12:13:01,566:INFO:              gradio: Not installed
2024-11-25 12:13:01,566:INFO:             fastapi: Not installed
2024-11-25 12:13:01,566:INFO:             uvicorn: Not installed
2024-11-25 12:13:01,566:INFO:              m2cgen: Not installed
2024-11-25 12:13:01,566:INFO:           evidently: Not installed
2024-11-25 12:13:01,566:INFO:               fugue: Not installed
2024-11-25 12:13:01,566:INFO:           streamlit: Not installed
2024-11-25 12:13:01,566:INFO:             prophet: Not installed
2024-11-25 12:13:01,566:INFO:None
2024-11-25 12:13:01,566:INFO:Set up data.
2024-11-25 12:13:01,582:INFO:Set up folding strategy.
2024-11-25 12:13:01,582:INFO:Set up train/test split.
2024-11-25 12:13:01,582:INFO:Set up index.
2024-11-25 12:13:01,582:INFO:Assigning column types.
2024-11-25 12:13:01,582:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:13:01,621:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,621:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,652:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,699:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,699:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,724:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,724:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,724:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:13:01,766:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,797:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,838:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:13:01,859:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,859:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:13:01,932:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:01,932:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,004:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,004:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,006:INFO:Preparing preprocessing pipeline...
2024-11-25 12:13:02,006:INFO:Set up simple imputation.
2024-11-25 12:13:02,006:INFO:Set up encoding of categorical features.
2024-11-25 12:13:02,006:INFO:Set up column name cleaning.
2024-11-25 12:13:02,164:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:13:02,164:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:13:02,164:INFO:Creating final display dataframe.
2024-11-25 12:13:02,392:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              200b
2024-11-25 12:13:02,489:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,489:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:13:02,573:INFO:setup() successfully completed in 1.01s...............
2024-11-25 12:13:02,585:INFO:Initializing compare_models()
2024-11-25 12:13:02,585:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:13:02,585:INFO:Checking exceptions
2024-11-25 12:13:02,585:INFO:Preparing display monitor
2024-11-25 12:13:02,626:INFO:Initializing Logistic Regression
2024-11-25 12:13:02,626:INFO:Total runtime is 0.0 minutes
2024-11-25 12:13:02,626:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:02,626:INFO:Initializing create_model()
2024-11-25 12:13:02,626:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:02,626:INFO:Checking exceptions
2024-11-25 12:13:02,626:INFO:Importing libraries
2024-11-25 12:13:02,626:INFO:Copying training dataset
2024-11-25 12:13:02,626:INFO:Defining folds
2024-11-25 12:13:02,626:INFO:Declaring metric variables
2024-11-25 12:13:02,641:INFO:Importing untrained model
2024-11-25 12:13:02,641:INFO:Logistic Regression Imported successfully
2024-11-25 12:13:02,657:INFO:Starting cross validation
2024-11-25 12:13:02,657:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:06,072:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:06,072:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:06,113:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:06,117:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:06,134:INFO:Calculating mean and std
2024-11-25 12:13:06,134:INFO:Creating metrics dataframe
2024-11-25 12:13:06,138:INFO:Uploading results into container
2024-11-25 12:13:06,140:INFO:Uploading model into container now
2024-11-25 12:13:06,140:INFO:_master_model_container: 1
2024-11-25 12:13:06,140:INFO:_display_container: 2
2024-11-25 12:13:06,140:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:13:06,140:INFO:create_model() successfully completed......................................
2024-11-25 12:13:06,232:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:06,232:INFO:Creating metrics dataframe
2024-11-25 12:13:06,242:INFO:Initializing K Neighbors Classifier
2024-11-25 12:13:06,242:INFO:Total runtime is 0.06027439037958781 minutes
2024-11-25 12:13:06,242:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:06,242:INFO:Initializing create_model()
2024-11-25 12:13:06,242:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:06,242:INFO:Checking exceptions
2024-11-25 12:13:06,242:INFO:Importing libraries
2024-11-25 12:13:06,242:INFO:Copying training dataset
2024-11-25 12:13:06,250:INFO:Defining folds
2024-11-25 12:13:06,250:INFO:Declaring metric variables
2024-11-25 12:13:06,250:INFO:Importing untrained model
2024-11-25 12:13:06,260:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:13:06,260:INFO:Starting cross validation
2024-11-25 12:13:06,260:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:09,343:INFO:Calculating mean and std
2024-11-25 12:13:09,343:INFO:Creating metrics dataframe
2024-11-25 12:13:09,343:INFO:Uploading results into container
2024-11-25 12:13:09,343:INFO:Uploading model into container now
2024-11-25 12:13:09,343:INFO:_master_model_container: 2
2024-11-25 12:13:09,343:INFO:_display_container: 2
2024-11-25 12:13:09,343:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:13:09,343:INFO:create_model() successfully completed......................................
2024-11-25 12:13:09,445:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:09,445:INFO:Creating metrics dataframe
2024-11-25 12:13:09,460:INFO:Initializing Naive Bayes
2024-11-25 12:13:09,460:INFO:Total runtime is 0.11391024589538574 minutes
2024-11-25 12:13:09,460:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:09,460:INFO:Initializing create_model()
2024-11-25 12:13:09,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:09,460:INFO:Checking exceptions
2024-11-25 12:13:09,460:INFO:Importing libraries
2024-11-25 12:13:09,460:INFO:Copying training dataset
2024-11-25 12:13:09,460:INFO:Defining folds
2024-11-25 12:13:09,460:INFO:Declaring metric variables
2024-11-25 12:13:09,476:INFO:Importing untrained model
2024-11-25 12:13:09,476:INFO:Naive Bayes Imported successfully
2024-11-25 12:13:09,476:INFO:Starting cross validation
2024-11-25 12:13:09,491:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:09,778:INFO:Calculating mean and std
2024-11-25 12:13:09,778:INFO:Creating metrics dataframe
2024-11-25 12:13:09,778:INFO:Uploading results into container
2024-11-25 12:13:09,778:INFO:Uploading model into container now
2024-11-25 12:13:09,778:INFO:_master_model_container: 3
2024-11-25 12:13:09,778:INFO:_display_container: 2
2024-11-25 12:13:09,778:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:13:09,778:INFO:create_model() successfully completed......................................
2024-11-25 12:13:09,840:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:09,840:INFO:Creating metrics dataframe
2024-11-25 12:13:09,848:INFO:Initializing Decision Tree Classifier
2024-11-25 12:13:09,848:INFO:Total runtime is 0.1203682820002238 minutes
2024-11-25 12:13:09,858:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:09,858:INFO:Initializing create_model()
2024-11-25 12:13:09,858:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:09,858:INFO:Checking exceptions
2024-11-25 12:13:09,858:INFO:Importing libraries
2024-11-25 12:13:09,858:INFO:Copying training dataset
2024-11-25 12:13:09,858:INFO:Defining folds
2024-11-25 12:13:09,858:INFO:Declaring metric variables
2024-11-25 12:13:09,858:INFO:Importing untrained model
2024-11-25 12:13:09,858:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:13:09,873:INFO:Starting cross validation
2024-11-25 12:13:09,873:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:10,126:INFO:Calculating mean and std
2024-11-25 12:13:10,126:INFO:Creating metrics dataframe
2024-11-25 12:13:10,126:INFO:Uploading results into container
2024-11-25 12:13:10,126:INFO:Uploading model into container now
2024-11-25 12:13:10,130:INFO:_master_model_container: 4
2024-11-25 12:13:10,130:INFO:_display_container: 2
2024-11-25 12:13:10,130:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:13:10,130:INFO:create_model() successfully completed......................................
2024-11-25 12:13:10,192:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:10,192:INFO:Creating metrics dataframe
2024-11-25 12:13:10,192:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:13:10,192:INFO:Total runtime is 0.12611180543899536 minutes
2024-11-25 12:13:10,192:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:10,192:INFO:Initializing create_model()
2024-11-25 12:13:10,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:10,192:INFO:Checking exceptions
2024-11-25 12:13:10,192:INFO:Importing libraries
2024-11-25 12:13:10,192:INFO:Copying training dataset
2024-11-25 12:13:10,208:INFO:Defining folds
2024-11-25 12:13:10,208:INFO:Declaring metric variables
2024-11-25 12:13:10,208:INFO:Importing untrained model
2024-11-25 12:13:10,208:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:13:10,224:INFO:Starting cross validation
2024-11-25 12:13:10,230:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:10,456:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,456:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,498:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,508:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,528:INFO:Calculating mean and std
2024-11-25 12:13:10,528:INFO:Creating metrics dataframe
2024-11-25 12:13:10,530:INFO:Uploading results into container
2024-11-25 12:13:10,530:INFO:Uploading model into container now
2024-11-25 12:13:10,530:INFO:_master_model_container: 5
2024-11-25 12:13:10,530:INFO:_display_container: 2
2024-11-25 12:13:10,530:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:13:10,530:INFO:create_model() successfully completed......................................
2024-11-25 12:13:10,593:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:10,593:INFO:Creating metrics dataframe
2024-11-25 12:13:10,593:INFO:Initializing Ridge Classifier
2024-11-25 12:13:10,593:INFO:Total runtime is 0.1327887495358785 minutes
2024-11-25 12:13:10,609:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:10,609:INFO:Initializing create_model()
2024-11-25 12:13:10,609:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:10,609:INFO:Checking exceptions
2024-11-25 12:13:10,609:INFO:Importing libraries
2024-11-25 12:13:10,609:INFO:Copying training dataset
2024-11-25 12:13:10,609:INFO:Defining folds
2024-11-25 12:13:10,609:INFO:Declaring metric variables
2024-11-25 12:13:10,609:INFO:Importing untrained model
2024-11-25 12:13:10,609:INFO:Ridge Classifier Imported successfully
2024-11-25 12:13:10,624:INFO:Starting cross validation
2024-11-25 12:13:10,631:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:10,863:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,863:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,873:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,883:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:10,904:INFO:Calculating mean and std
2024-11-25 12:13:10,904:INFO:Creating metrics dataframe
2024-11-25 12:13:10,904:INFO:Uploading results into container
2024-11-25 12:13:10,904:INFO:Uploading model into container now
2024-11-25 12:13:10,904:INFO:_master_model_container: 6
2024-11-25 12:13:10,904:INFO:_display_container: 2
2024-11-25 12:13:10,904:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:13:10,904:INFO:create_model() successfully completed......................................
2024-11-25 12:13:10,963:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:10,963:INFO:Creating metrics dataframe
2024-11-25 12:13:10,963:INFO:Initializing Random Forest Classifier
2024-11-25 12:13:10,963:INFO:Total runtime is 0.13894944588343303 minutes
2024-11-25 12:13:10,978:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:10,978:INFO:Initializing create_model()
2024-11-25 12:13:10,978:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:10,978:INFO:Checking exceptions
2024-11-25 12:13:10,978:INFO:Importing libraries
2024-11-25 12:13:10,978:INFO:Copying training dataset
2024-11-25 12:13:10,978:INFO:Defining folds
2024-11-25 12:13:10,978:INFO:Declaring metric variables
2024-11-25 12:13:10,978:INFO:Importing untrained model
2024-11-25 12:13:10,978:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:13:10,994:INFO:Starting cross validation
2024-11-25 12:13:10,994:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:11,598:INFO:Calculating mean and std
2024-11-25 12:13:11,598:INFO:Creating metrics dataframe
2024-11-25 12:13:11,598:INFO:Uploading results into container
2024-11-25 12:13:11,598:INFO:Uploading model into container now
2024-11-25 12:13:11,598:INFO:_master_model_container: 7
2024-11-25 12:13:11,598:INFO:_display_container: 2
2024-11-25 12:13:11,598:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:13:11,598:INFO:create_model() successfully completed......................................
2024-11-25 12:13:11,663:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:11,663:INFO:Creating metrics dataframe
2024-11-25 12:13:11,663:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:13:11,663:INFO:Total runtime is 0.1506279945373535 minutes
2024-11-25 12:13:11,663:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:11,663:INFO:Initializing create_model()
2024-11-25 12:13:11,663:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:11,663:INFO:Checking exceptions
2024-11-25 12:13:11,663:INFO:Importing libraries
2024-11-25 12:13:11,663:INFO:Copying training dataset
2024-11-25 12:13:11,679:INFO:Defining folds
2024-11-25 12:13:11,679:INFO:Declaring metric variables
2024-11-25 12:13:11,679:INFO:Importing untrained model
2024-11-25 12:13:11,679:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:13:11,695:INFO:Starting cross validation
2024-11-25 12:13:11,695:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:11,849:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:11,855:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:11,880:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:11,890:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:11,904:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:11,906:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:11,931:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:11,937:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:11,952:INFO:Calculating mean and std
2024-11-25 12:13:11,952:INFO:Creating metrics dataframe
2024-11-25 12:13:11,952:INFO:Uploading results into container
2024-11-25 12:13:11,952:INFO:Uploading model into container now
2024-11-25 12:13:11,956:INFO:_master_model_container: 8
2024-11-25 12:13:11,956:INFO:_display_container: 2
2024-11-25 12:13:11,956:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:13:11,956:INFO:create_model() successfully completed......................................
2024-11-25 12:13:12,013:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:12,013:INFO:Creating metrics dataframe
2024-11-25 12:13:12,013:INFO:Initializing Ada Boost Classifier
2024-11-25 12:13:12,013:INFO:Total runtime is 0.15645208756128948 minutes
2024-11-25 12:13:12,028:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:12,028:INFO:Initializing create_model()
2024-11-25 12:13:12,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:12,028:INFO:Checking exceptions
2024-11-25 12:13:12,028:INFO:Importing libraries
2024-11-25 12:13:12,028:INFO:Copying training dataset
2024-11-25 12:13:12,028:INFO:Defining folds
2024-11-25 12:13:12,028:INFO:Declaring metric variables
2024-11-25 12:13:12,028:INFO:Importing untrained model
2024-11-25 12:13:12,028:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:13:12,044:INFO:Starting cross validation
2024-11-25 12:13:12,044:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:12,217:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:12,217:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:12,217:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:12,227:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:12,371:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:12,381:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:12,401:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:12,401:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:12,421:INFO:Calculating mean and std
2024-11-25 12:13:12,421:INFO:Creating metrics dataframe
2024-11-25 12:13:12,421:INFO:Uploading results into container
2024-11-25 12:13:12,421:INFO:Uploading model into container now
2024-11-25 12:13:12,421:INFO:_master_model_container: 9
2024-11-25 12:13:12,421:INFO:_display_container: 2
2024-11-25 12:13:12,421:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:13:12,421:INFO:create_model() successfully completed......................................
2024-11-25 12:13:12,477:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:12,477:INFO:Creating metrics dataframe
2024-11-25 12:13:12,492:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:13:12,492:INFO:Total runtime is 0.1644403656323751 minutes
2024-11-25 12:13:12,492:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:12,492:INFO:Initializing create_model()
2024-11-25 12:13:12,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:12,492:INFO:Checking exceptions
2024-11-25 12:13:12,492:INFO:Importing libraries
2024-11-25 12:13:12,492:INFO:Copying training dataset
2024-11-25 12:13:12,492:INFO:Defining folds
2024-11-25 12:13:12,492:INFO:Declaring metric variables
2024-11-25 12:13:12,508:INFO:Importing untrained model
2024-11-25 12:13:12,508:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:13:12,508:INFO:Starting cross validation
2024-11-25 12:13:12,523:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:13,159:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,159:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,169:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,216:INFO:Calculating mean and std
2024-11-25 12:13:13,216:INFO:Creating metrics dataframe
2024-11-25 12:13:13,219:INFO:Uploading results into container
2024-11-25 12:13:13,219:INFO:Uploading model into container now
2024-11-25 12:13:13,219:INFO:_master_model_container: 10
2024-11-25 12:13:13,219:INFO:_display_container: 2
2024-11-25 12:13:13,219:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:13:13,219:INFO:create_model() successfully completed......................................
2024-11-25 12:13:13,283:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:13,283:INFO:Creating metrics dataframe
2024-11-25 12:13:13,292:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:13:13,292:INFO:Total runtime is 0.17776517470677697 minutes
2024-11-25 12:13:13,292:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:13,292:INFO:Initializing create_model()
2024-11-25 12:13:13,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:13,292:INFO:Checking exceptions
2024-11-25 12:13:13,292:INFO:Importing libraries
2024-11-25 12:13:13,292:INFO:Copying training dataset
2024-11-25 12:13:13,302:INFO:Defining folds
2024-11-25 12:13:13,302:INFO:Declaring metric variables
2024-11-25 12:13:13,302:INFO:Importing untrained model
2024-11-25 12:13:13,302:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:13:13,317:INFO:Starting cross validation
2024-11-25 12:13:13,317:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:13,604:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,604:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,614:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,624:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:13,644:INFO:Calculating mean and std
2024-11-25 12:13:13,645:INFO:Creating metrics dataframe
2024-11-25 12:13:13,645:INFO:Uploading results into container
2024-11-25 12:13:13,645:INFO:Uploading model into container now
2024-11-25 12:13:13,645:INFO:_master_model_container: 11
2024-11-25 12:13:13,645:INFO:_display_container: 2
2024-11-25 12:13:13,645:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:13:13,645:INFO:create_model() successfully completed......................................
2024-11-25 12:13:13,702:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:13,702:INFO:Creating metrics dataframe
2024-11-25 12:13:13,718:INFO:Initializing Extra Trees Classifier
2024-11-25 12:13:13,718:INFO:Total runtime is 0.18486542304356895 minutes
2024-11-25 12:13:13,718:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:13,718:INFO:Initializing create_model()
2024-11-25 12:13:13,718:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:13,718:INFO:Checking exceptions
2024-11-25 12:13:13,718:INFO:Importing libraries
2024-11-25 12:13:13,718:INFO:Copying training dataset
2024-11-25 12:13:13,718:INFO:Defining folds
2024-11-25 12:13:13,718:INFO:Declaring metric variables
2024-11-25 12:13:13,733:INFO:Importing untrained model
2024-11-25 12:13:13,733:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:13:13,733:INFO:Starting cross validation
2024-11-25 12:13:13,749:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:14,230:INFO:Calculating mean and std
2024-11-25 12:13:14,230:INFO:Creating metrics dataframe
2024-11-25 12:13:14,230:INFO:Uploading results into container
2024-11-25 12:13:14,230:INFO:Uploading model into container now
2024-11-25 12:13:14,230:INFO:_master_model_container: 12
2024-11-25 12:13:14,230:INFO:_display_container: 2
2024-11-25 12:13:14,230:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:13:14,230:INFO:create_model() successfully completed......................................
2024-11-25 12:13:14,295:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:14,295:INFO:Creating metrics dataframe
2024-11-25 12:13:14,295:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:13:14,295:INFO:Total runtime is 0.194488537311554 minutes
2024-11-25 12:13:14,311:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:14,311:INFO:Initializing create_model()
2024-11-25 12:13:14,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:14,311:INFO:Checking exceptions
2024-11-25 12:13:14,311:INFO:Importing libraries
2024-11-25 12:13:14,311:INFO:Copying training dataset
2024-11-25 12:13:14,311:INFO:Defining folds
2024-11-25 12:13:14,311:INFO:Declaring metric variables
2024-11-25 12:13:14,311:INFO:Importing untrained model
2024-11-25 12:13:14,311:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:13:14,326:INFO:Starting cross validation
2024-11-25 12:13:14,326:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:15,189:INFO:Calculating mean and std
2024-11-25 12:13:15,189:INFO:Creating metrics dataframe
2024-11-25 12:13:15,193:INFO:Uploading results into container
2024-11-25 12:13:15,195:INFO:Uploading model into container now
2024-11-25 12:13:15,195:INFO:_master_model_container: 13
2024-11-25 12:13:15,195:INFO:_display_container: 2
2024-11-25 12:13:15,195:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:13:15,195:INFO:create_model() successfully completed......................................
2024-11-25 12:13:15,297:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:15,297:INFO:Creating metrics dataframe
2024-11-25 12:13:15,307:INFO:Initializing Dummy Classifier
2024-11-25 12:13:15,307:INFO:Total runtime is 0.21136059761047368 minutes
2024-11-25 12:13:15,307:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:15,307:INFO:Initializing create_model()
2024-11-25 12:13:15,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6B9F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:15,307:INFO:Checking exceptions
2024-11-25 12:13:15,307:INFO:Importing libraries
2024-11-25 12:13:15,307:INFO:Copying training dataset
2024-11-25 12:13:15,315:INFO:Defining folds
2024-11-25 12:13:15,315:INFO:Declaring metric variables
2024-11-25 12:13:15,315:INFO:Importing untrained model
2024-11-25 12:13:15,328:INFO:Dummy Classifier Imported successfully
2024-11-25 12:13:15,336:INFO:Starting cross validation
2024-11-25 12:13:15,336:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:15,577:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:15,579:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:15,597:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:15,597:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:15,613:INFO:Calculating mean and std
2024-11-25 12:13:15,613:INFO:Creating metrics dataframe
2024-11-25 12:13:15,613:INFO:Uploading results into container
2024-11-25 12:13:15,613:INFO:Uploading model into container now
2024-11-25 12:13:15,613:INFO:_master_model_container: 14
2024-11-25 12:13:15,613:INFO:_display_container: 2
2024-11-25 12:13:15,613:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:13:15,613:INFO:create_model() successfully completed......................................
2024-11-25 12:13:15,680:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:15,680:INFO:Creating metrics dataframe
2024-11-25 12:13:15,707:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:13:15,719:INFO:Initializing create_model()
2024-11-25 12:13:15,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:15,720:INFO:Checking exceptions
2024-11-25 12:13:15,722:INFO:Importing libraries
2024-11-25 12:13:15,722:INFO:Copying training dataset
2024-11-25 12:13:15,726:INFO:Defining folds
2024-11-25 12:13:15,726:INFO:Declaring metric variables
2024-11-25 12:13:15,729:INFO:Importing untrained model
2024-11-25 12:13:15,729:INFO:Declaring custom model
2024-11-25 12:13:15,729:INFO:Ridge Classifier Imported successfully
2024-11-25 12:13:15,729:INFO:Cross validation set to False
2024-11-25 12:13:15,729:INFO:Fitting Model
2024-11-25 12:13:15,834:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:13:15,834:INFO:create_model() successfully completed......................................
2024-11-25 12:13:15,918:INFO:_master_model_container: 14
2024-11-25 12:13:15,918:INFO:_display_container: 2
2024-11-25 12:13:15,918:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:13:15,918:INFO:compare_models() successfully completed......................................
2024-11-25 12:13:15,929:INFO:Initializing compare_models()
2024-11-25 12:13:15,929:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:13:15,929:INFO:Checking exceptions
2024-11-25 12:13:15,929:INFO:Preparing display monitor
2024-11-25 12:13:15,960:INFO:Initializing Logistic Regression
2024-11-25 12:13:15,960:INFO:Total runtime is 0.0 minutes
2024-11-25 12:13:15,976:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:15,976:INFO:Initializing create_model()
2024-11-25 12:13:15,976:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:15,976:INFO:Checking exceptions
2024-11-25 12:13:15,976:INFO:Importing libraries
2024-11-25 12:13:15,976:INFO:Copying training dataset
2024-11-25 12:13:15,976:INFO:Defining folds
2024-11-25 12:13:15,976:INFO:Declaring metric variables
2024-11-25 12:13:15,976:INFO:Importing untrained model
2024-11-25 12:13:15,992:INFO:Logistic Regression Imported successfully
2024-11-25 12:13:15,992:INFO:Starting cross validation
2024-11-25 12:13:16,007:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:16,414:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,416:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,424:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,434:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,436:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,447:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,447:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,467:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,632:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,642:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:16,663:INFO:Calculating mean and std
2024-11-25 12:13:16,663:INFO:Creating metrics dataframe
2024-11-25 12:13:16,663:INFO:Uploading results into container
2024-11-25 12:13:16,663:INFO:Uploading model into container now
2024-11-25 12:13:16,663:INFO:_master_model_container: 1
2024-11-25 12:13:16,663:INFO:_display_container: 2
2024-11-25 12:13:16,663:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:13:16,663:INFO:create_model() successfully completed......................................
2024-11-25 12:13:16,718:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:16,718:INFO:Creating metrics dataframe
2024-11-25 12:13:16,733:INFO:Initializing K Neighbors Classifier
2024-11-25 12:13:16,733:INFO:Total runtime is 0.012883575757344563 minutes
2024-11-25 12:13:16,733:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:16,733:INFO:Initializing create_model()
2024-11-25 12:13:16,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:16,733:INFO:Checking exceptions
2024-11-25 12:13:16,733:INFO:Importing libraries
2024-11-25 12:13:16,733:INFO:Copying training dataset
2024-11-25 12:13:16,733:INFO:Defining folds
2024-11-25 12:13:16,733:INFO:Declaring metric variables
2024-11-25 12:13:16,733:INFO:Importing untrained model
2024-11-25 12:13:16,749:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:13:16,749:INFO:Starting cross validation
2024-11-25 12:13:16,765:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:17,415:INFO:Calculating mean and std
2024-11-25 12:13:17,415:INFO:Creating metrics dataframe
2024-11-25 12:13:17,415:INFO:Uploading results into container
2024-11-25 12:13:17,415:INFO:Uploading model into container now
2024-11-25 12:13:17,415:INFO:_master_model_container: 2
2024-11-25 12:13:17,415:INFO:_display_container: 2
2024-11-25 12:13:17,415:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:13:17,415:INFO:create_model() successfully completed......................................
2024-11-25 12:13:17,472:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:17,472:INFO:Creating metrics dataframe
2024-11-25 12:13:17,487:INFO:Initializing Naive Bayes
2024-11-25 12:13:17,487:INFO:Total runtime is 0.025446037451426186 minutes
2024-11-25 12:13:17,487:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:17,487:INFO:Initializing create_model()
2024-11-25 12:13:17,487:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:17,487:INFO:Checking exceptions
2024-11-25 12:13:17,487:INFO:Importing libraries
2024-11-25 12:13:17,487:INFO:Copying training dataset
2024-11-25 12:13:17,503:INFO:Defining folds
2024-11-25 12:13:17,503:INFO:Declaring metric variables
2024-11-25 12:13:17,503:INFO:Importing untrained model
2024-11-25 12:13:17,503:INFO:Naive Bayes Imported successfully
2024-11-25 12:13:17,518:INFO:Starting cross validation
2024-11-25 12:13:17,518:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:18,126:INFO:Calculating mean and std
2024-11-25 12:13:18,126:INFO:Creating metrics dataframe
2024-11-25 12:13:18,126:INFO:Uploading results into container
2024-11-25 12:13:18,126:INFO:Uploading model into container now
2024-11-25 12:13:18,126:INFO:_master_model_container: 3
2024-11-25 12:13:18,126:INFO:_display_container: 2
2024-11-25 12:13:18,126:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:13:18,126:INFO:create_model() successfully completed......................................
2024-11-25 12:13:18,189:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:18,189:INFO:Creating metrics dataframe
2024-11-25 12:13:18,197:INFO:Initializing Decision Tree Classifier
2024-11-25 12:13:18,197:INFO:Total runtime is 0.03728237946828206 minutes
2024-11-25 12:13:18,197:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:18,207:INFO:Initializing create_model()
2024-11-25 12:13:18,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:18,207:INFO:Checking exceptions
2024-11-25 12:13:18,207:INFO:Importing libraries
2024-11-25 12:13:18,207:INFO:Copying training dataset
2024-11-25 12:13:18,207:INFO:Defining folds
2024-11-25 12:13:18,207:INFO:Declaring metric variables
2024-11-25 12:13:18,207:INFO:Importing untrained model
2024-11-25 12:13:18,207:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:13:18,223:INFO:Starting cross validation
2024-11-25 12:13:18,223:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:18,818:INFO:Calculating mean and std
2024-11-25 12:13:18,818:INFO:Creating metrics dataframe
2024-11-25 12:13:18,818:INFO:Uploading results into container
2024-11-25 12:13:18,818:INFO:Uploading model into container now
2024-11-25 12:13:18,818:INFO:_master_model_container: 4
2024-11-25 12:13:18,818:INFO:_display_container: 2
2024-11-25 12:13:18,818:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:13:18,818:INFO:create_model() successfully completed......................................
2024-11-25 12:13:18,875:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:18,875:INFO:Creating metrics dataframe
2024-11-25 12:13:18,890:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:13:18,890:INFO:Total runtime is 0.04882925351460775 minutes
2024-11-25 12:13:18,890:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:18,890:INFO:Initializing create_model()
2024-11-25 12:13:18,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:18,890:INFO:Checking exceptions
2024-11-25 12:13:18,890:INFO:Importing libraries
2024-11-25 12:13:18,890:INFO:Copying training dataset
2024-11-25 12:13:18,906:INFO:Defining folds
2024-11-25 12:13:18,906:INFO:Declaring metric variables
2024-11-25 12:13:18,906:INFO:Importing untrained model
2024-11-25 12:13:18,906:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:13:18,921:INFO:Starting cross validation
2024-11-25 12:13:18,928:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:19,337:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,345:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,345:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,347:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,357:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,363:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,375:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,385:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,551:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,561:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:19,582:INFO:Calculating mean and std
2024-11-25 12:13:19,582:INFO:Creating metrics dataframe
2024-11-25 12:13:19,582:INFO:Uploading results into container
2024-11-25 12:13:19,582:INFO:Uploading model into container now
2024-11-25 12:13:19,582:INFO:_master_model_container: 5
2024-11-25 12:13:19,582:INFO:_display_container: 2
2024-11-25 12:13:19,582:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:13:19,582:INFO:create_model() successfully completed......................................
2024-11-25 12:13:19,642:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:19,642:INFO:Creating metrics dataframe
2024-11-25 12:13:19,658:INFO:Initializing Ridge Classifier
2024-11-25 12:13:19,658:INFO:Total runtime is 0.06162165006001791 minutes
2024-11-25 12:13:19,658:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:19,658:INFO:Initializing create_model()
2024-11-25 12:13:19,658:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:19,658:INFO:Checking exceptions
2024-11-25 12:13:19,658:INFO:Importing libraries
2024-11-25 12:13:19,658:INFO:Copying training dataset
2024-11-25 12:13:19,673:INFO:Defining folds
2024-11-25 12:13:19,673:INFO:Declaring metric variables
2024-11-25 12:13:19,673:INFO:Importing untrained model
2024-11-25 12:13:19,673:INFO:Ridge Classifier Imported successfully
2024-11-25 12:13:19,673:INFO:Starting cross validation
2024-11-25 12:13:19,689:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:20,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,072:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,074:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,074:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,084:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,084:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,094:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,108:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,268:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:20,294:INFO:Calculating mean and std
2024-11-25 12:13:20,294:INFO:Creating metrics dataframe
2024-11-25 12:13:20,294:INFO:Uploading results into container
2024-11-25 12:13:20,297:INFO:Uploading model into container now
2024-11-25 12:13:20,297:INFO:_master_model_container: 6
2024-11-25 12:13:20,297:INFO:_display_container: 2
2024-11-25 12:13:20,297:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:13:20,297:INFO:create_model() successfully completed......................................
2024-11-25 12:13:20,362:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:20,362:INFO:Creating metrics dataframe
2024-11-25 12:13:20,370:INFO:Initializing Random Forest Classifier
2024-11-25 12:13:20,370:INFO:Total runtime is 0.07349101702372234 minutes
2024-11-25 12:13:20,370:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:20,380:INFO:Initializing create_model()
2024-11-25 12:13:20,380:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:20,380:INFO:Checking exceptions
2024-11-25 12:13:20,380:INFO:Importing libraries
2024-11-25 12:13:20,380:INFO:Copying training dataset
2024-11-25 12:13:20,380:INFO:Defining folds
2024-11-25 12:13:20,380:INFO:Declaring metric variables
2024-11-25 12:13:20,380:INFO:Importing untrained model
2024-11-25 12:13:20,380:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:13:20,396:INFO:Starting cross validation
2024-11-25 12:13:20,396:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:21,661:INFO:Calculating mean and std
2024-11-25 12:13:21,661:INFO:Creating metrics dataframe
2024-11-25 12:13:21,661:INFO:Uploading results into container
2024-11-25 12:13:21,661:INFO:Uploading model into container now
2024-11-25 12:13:21,661:INFO:_master_model_container: 7
2024-11-25 12:13:21,661:INFO:_display_container: 2
2024-11-25 12:13:21,661:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:13:21,661:INFO:create_model() successfully completed......................................
2024-11-25 12:13:21,731:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:21,731:INFO:Creating metrics dataframe
2024-11-25 12:13:21,731:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:13:21,731:INFO:Total runtime is 0.09616852601369222 minutes
2024-11-25 12:13:21,731:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:21,731:INFO:Initializing create_model()
2024-11-25 12:13:21,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:21,731:INFO:Checking exceptions
2024-11-25 12:13:21,731:INFO:Importing libraries
2024-11-25 12:13:21,731:INFO:Copying training dataset
2024-11-25 12:13:21,746:INFO:Defining folds
2024-11-25 12:13:21,746:INFO:Declaring metric variables
2024-11-25 12:13:21,746:INFO:Importing untrained model
2024-11-25 12:13:21,746:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:13:21,762:INFO:Starting cross validation
2024-11-25 12:13:21,768:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:22,050:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,052:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,052:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,060:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,083:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,093:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,152:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,154:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,154:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,165:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,165:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,183:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,195:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,209:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,332:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,332:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:13:22,379:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,379:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:22,394:INFO:Calculating mean and std
2024-11-25 12:13:22,396:INFO:Creating metrics dataframe
2024-11-25 12:13:22,400:INFO:Uploading results into container
2024-11-25 12:13:22,401:INFO:Uploading model into container now
2024-11-25 12:13:22,402:INFO:_master_model_container: 8
2024-11-25 12:13:22,402:INFO:_display_container: 2
2024-11-25 12:13:22,402:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:13:22,402:INFO:create_model() successfully completed......................................
2024-11-25 12:13:22,466:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:22,466:INFO:Creating metrics dataframe
2024-11-25 12:13:22,482:INFO:Initializing Ada Boost Classifier
2024-11-25 12:13:22,482:INFO:Total runtime is 0.10869757334391275 minutes
2024-11-25 12:13:22,482:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:22,482:INFO:Initializing create_model()
2024-11-25 12:13:22,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:22,482:INFO:Checking exceptions
2024-11-25 12:13:22,482:INFO:Importing libraries
2024-11-25 12:13:22,482:INFO:Copying training dataset
2024-11-25 12:13:22,482:INFO:Defining folds
2024-11-25 12:13:22,482:INFO:Declaring metric variables
2024-11-25 12:13:22,482:INFO:Importing untrained model
2024-11-25 12:13:22,498:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:13:22,508:INFO:Starting cross validation
2024-11-25 12:13:22,508:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:22,783:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,793:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,793:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,803:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,810:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,810:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,813:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:22,821:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:23,087:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,089:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,099:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,109:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,109:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,119:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,119:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,129:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,262:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:23,268:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:13:23,399:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,400:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:23,415:INFO:Calculating mean and std
2024-11-25 12:13:23,415:INFO:Creating metrics dataframe
2024-11-25 12:13:23,418:INFO:Uploading results into container
2024-11-25 12:13:23,418:INFO:Uploading model into container now
2024-11-25 12:13:23,418:INFO:_master_model_container: 9
2024-11-25 12:13:23,418:INFO:_display_container: 2
2024-11-25 12:13:23,419:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:13:23,419:INFO:create_model() successfully completed......................................
2024-11-25 12:13:23,493:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:23,493:INFO:Creating metrics dataframe
2024-11-25 12:13:23,503:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:13:23,504:INFO:Total runtime is 0.12571078538894653 minutes
2024-11-25 12:13:23,508:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:23,508:INFO:Initializing create_model()
2024-11-25 12:13:23,508:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:23,508:INFO:Checking exceptions
2024-11-25 12:13:23,508:INFO:Importing libraries
2024-11-25 12:13:23,508:INFO:Copying training dataset
2024-11-25 12:13:23,512:INFO:Defining folds
2024-11-25 12:13:23,512:INFO:Declaring metric variables
2024-11-25 12:13:23,518:INFO:Importing untrained model
2024-11-25 12:13:23,522:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:13:23,533:INFO:Starting cross validation
2024-11-25 12:13:23,539:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:24,693:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,694:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,695:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,697:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,737:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,765:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,804:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:24,809:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,255:INFO:Calculating mean and std
2024-11-25 12:13:25,255:INFO:Creating metrics dataframe
2024-11-25 12:13:25,258:INFO:Uploading results into container
2024-11-25 12:13:25,258:INFO:Uploading model into container now
2024-11-25 12:13:25,258:INFO:_master_model_container: 10
2024-11-25 12:13:25,259:INFO:_display_container: 2
2024-11-25 12:13:25,259:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:13:25,259:INFO:create_model() successfully completed......................................
2024-11-25 12:13:25,332:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:25,332:INFO:Creating metrics dataframe
2024-11-25 12:13:25,341:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:13:25,341:INFO:Total runtime is 0.15633604923884073 minutes
2024-11-25 12:13:25,345:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:25,345:INFO:Initializing create_model()
2024-11-25 12:13:25,345:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:25,345:INFO:Checking exceptions
2024-11-25 12:13:25,345:INFO:Importing libraries
2024-11-25 12:13:25,346:INFO:Copying training dataset
2024-11-25 12:13:25,351:INFO:Defining folds
2024-11-25 12:13:25,351:INFO:Declaring metric variables
2024-11-25 12:13:25,355:INFO:Importing untrained model
2024-11-25 12:13:25,358:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:13:25,369:INFO:Starting cross validation
2024-11-25 12:13:25,372:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:25,765:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,766:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,782:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,784:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,796:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,799:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,824:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,967:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:13:25,981:INFO:Calculating mean and std
2024-11-25 12:13:25,981:INFO:Creating metrics dataframe
2024-11-25 12:13:25,984:INFO:Uploading results into container
2024-11-25 12:13:25,984:INFO:Uploading model into container now
2024-11-25 12:13:25,984:INFO:_master_model_container: 11
2024-11-25 12:13:25,984:INFO:_display_container: 2
2024-11-25 12:13:25,985:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:13:25,985:INFO:create_model() successfully completed......................................
2024-11-25 12:13:26,052:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:26,053:INFO:Creating metrics dataframe
2024-11-25 12:13:26,065:INFO:Initializing Extra Trees Classifier
2024-11-25 12:13:26,066:INFO:Total runtime is 0.16842569907506305 minutes
2024-11-25 12:13:26,069:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:26,070:INFO:Initializing create_model()
2024-11-25 12:13:26,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:26,070:INFO:Checking exceptions
2024-11-25 12:13:26,070:INFO:Importing libraries
2024-11-25 12:13:26,070:INFO:Copying training dataset
2024-11-25 12:13:26,075:INFO:Defining folds
2024-11-25 12:13:26,075:INFO:Declaring metric variables
2024-11-25 12:13:26,078:INFO:Importing untrained model
2024-11-25 12:13:26,083:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:13:26,090:INFO:Starting cross validation
2024-11-25 12:13:26,093:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:27,269:INFO:Calculating mean and std
2024-11-25 12:13:27,270:INFO:Creating metrics dataframe
2024-11-25 12:13:27,272:INFO:Uploading results into container
2024-11-25 12:13:27,273:INFO:Uploading model into container now
2024-11-25 12:13:27,273:INFO:_master_model_container: 12
2024-11-25 12:13:27,273:INFO:_display_container: 2
2024-11-25 12:13:27,274:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:13:27,274:INFO:create_model() successfully completed......................................
2024-11-25 12:13:27,348:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:27,348:INFO:Creating metrics dataframe
2024-11-25 12:13:27,356:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:13:27,356:INFO:Total runtime is 0.18992928663889566 minutes
2024-11-25 12:13:27,359:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:27,359:INFO:Initializing create_model()
2024-11-25 12:13:27,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:27,360:INFO:Checking exceptions
2024-11-25 12:13:27,360:INFO:Importing libraries
2024-11-25 12:13:27,360:INFO:Copying training dataset
2024-11-25 12:13:27,364:INFO:Defining folds
2024-11-25 12:13:27,364:INFO:Declaring metric variables
2024-11-25 12:13:27,367:INFO:Importing untrained model
2024-11-25 12:13:27,371:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:13:27,379:INFO:Starting cross validation
2024-11-25 12:13:27,382:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:29,557:INFO:Calculating mean and std
2024-11-25 12:13:29,558:INFO:Creating metrics dataframe
2024-11-25 12:13:29,561:INFO:Uploading results into container
2024-11-25 12:13:29,562:INFO:Uploading model into container now
2024-11-25 12:13:29,563:INFO:_master_model_container: 13
2024-11-25 12:13:29,563:INFO:_display_container: 2
2024-11-25 12:13:29,564:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:13:29,564:INFO:create_model() successfully completed......................................
2024-11-25 12:13:29,661:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:29,661:INFO:Creating metrics dataframe
2024-11-25 12:13:29,681:INFO:Initializing Dummy Classifier
2024-11-25 12:13:29,681:INFO:Total runtime is 0.2286695122718811 minutes
2024-11-25 12:13:29,688:INFO:SubProcess create_model() called ==================================
2024-11-25 12:13:29,689:INFO:Initializing create_model()
2024-11-25 12:13:29,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F0E6E446D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:29,689:INFO:Checking exceptions
2024-11-25 12:13:29,690:INFO:Importing libraries
2024-11-25 12:13:29,690:INFO:Copying training dataset
2024-11-25 12:13:29,699:INFO:Defining folds
2024-11-25 12:13:29,699:INFO:Declaring metric variables
2024-11-25 12:13:29,703:INFO:Importing untrained model
2024-11-25 12:13:29,708:INFO:Dummy Classifier Imported successfully
2024-11-25 12:13:29,721:INFO:Starting cross validation
2024-11-25 12:13:29,724:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:13:30,147:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,152:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,158:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,164:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,171:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,173:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,185:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,345:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,347:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:13:30,359:INFO:Calculating mean and std
2024-11-25 12:13:30,359:INFO:Creating metrics dataframe
2024-11-25 12:13:30,362:INFO:Uploading results into container
2024-11-25 12:13:30,362:INFO:Uploading model into container now
2024-11-25 12:13:30,363:INFO:_master_model_container: 14
2024-11-25 12:13:30,363:INFO:_display_container: 2
2024-11-25 12:13:30,363:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:13:30,364:INFO:create_model() successfully completed......................................
2024-11-25 12:13:30,433:INFO:SubProcess create_model() end ==================================
2024-11-25 12:13:30,433:INFO:Creating metrics dataframe
2024-11-25 12:13:30,441:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:13:30,451:INFO:Initializing create_model()
2024-11-25 12:13:30,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6C36010>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:30,451:INFO:Checking exceptions
2024-11-25 12:13:30,453:INFO:Importing libraries
2024-11-25 12:13:30,453:INFO:Copying training dataset
2024-11-25 12:13:30,458:INFO:Defining folds
2024-11-25 12:13:30,459:INFO:Declaring metric variables
2024-11-25 12:13:30,459:INFO:Importing untrained model
2024-11-25 12:13:30,459:INFO:Declaring custom model
2024-11-25 12:13:30,460:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:13:30,462:INFO:Cross validation set to False
2024-11-25 12:13:30,462:INFO:Fitting Model
2024-11-25 12:13:30,584:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:13:30,585:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.
2024-11-25 12:13:30,585:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-25 12:13:30,585:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:13:30,585:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:13:30,586:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:13:30,586:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:13:30,586:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:13:30,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:13:30,684:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:13:30,684:INFO:create_model() successfully completed......................................
2024-11-25 12:13:30,823:INFO:_master_model_container: 14
2024-11-25 12:13:30,823:INFO:_display_container: 2
2024-11-25 12:13:30,824:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:13:30,824:INFO:compare_models() successfully completed......................................
2024-11-25 12:13:30,829:INFO:Initializing plot_model()
2024-11-25 12:13:30,830:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:13:30,830:INFO:Checking exceptions
2024-11-25 12:13:30,837:INFO:Preloading libraries
2024-11-25 12:13:30,837:INFO:Copying training dataset
2024-11-25 12:13:30,837:INFO:Plot type: confusion_matrix
2024-11-25 12:13:31,079:INFO:Fitting Model
2024-11-25 12:13:31,080:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:13:31,080:INFO:Scoring test/hold-out set
2024-11-25 12:13:31,222:INFO:Visual Rendered Successfully
2024-11-25 12:13:31,289:INFO:plot_model() successfully completed......................................
2024-11-25 12:13:31,295:INFO:Initializing finalize_model()
2024-11-25 12:13:31,295:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:13:31,295:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:13:31,299:INFO:Initializing create_model()
2024-11-25 12:13:31,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:13:31,299:INFO:Checking exceptions
2024-11-25 12:13:31,301:INFO:Importing libraries
2024-11-25 12:13:31,301:INFO:Copying training dataset
2024-11-25 12:13:31,302:INFO:Defining folds
2024-11-25 12:13:31,302:INFO:Declaring metric variables
2024-11-25 12:13:31,302:INFO:Importing untrained model
2024-11-25 12:13:31,302:INFO:Declaring custom model
2024-11-25 12:13:31,303:INFO:Ridge Classifier Imported successfully
2024-11-25 12:13:31,307:INFO:Cross validation set to False
2024-11-25 12:13:31,307:INFO:Fitting Model
2024-11-25 12:13:31,459:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:13:31,459:INFO:create_model() successfully completed......................................
2024-11-25 12:13:31,514:INFO:_master_model_container: 14
2024-11-25 12:13:31,514:INFO:_display_container: 2
2024-11-25 12:13:31,531:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:13:31,531:INFO:finalize_model() successfully completed......................................
2024-11-25 12:13:31,610:INFO:Initializing save_model()
2024-11-25 12:13:31,610:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-25 12:13:31,610:INFO:Adding model into prep_pipe
2024-11-25 12:13:31,632:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-25 12:13:31,647:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:13:31,647:INFO:save_model() successfully completed......................................
2024-11-25 12:13:57,646:INFO:Initializing evaluate_model()
2024-11-25 12:13:57,646:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:13:57,660:INFO:Initializing plot_model()
2024-11-25 12:13:57,661:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:13:57,661:INFO:Checking exceptions
2024-11-25 12:13:57,664:INFO:Preloading libraries
2024-11-25 12:13:57,665:INFO:Copying training dataset
2024-11-25 12:13:57,665:INFO:Plot type: pipeline
2024-11-25 12:13:57,820:INFO:Visual Rendered Successfully
2024-11-25 12:13:57,901:INFO:plot_model() successfully completed......................................
2024-11-25 12:14:01,420:INFO:Initializing plot_model()
2024-11-25 12:14:01,420:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:14:01,420:INFO:Checking exceptions
2024-11-25 12:14:01,423:INFO:Preloading libraries
2024-11-25 12:14:01,423:INFO:Copying training dataset
2024-11-25 12:14:01,423:INFO:Plot type: parameter
2024-11-25 12:14:01,426:INFO:Visual Rendered Successfully
2024-11-25 12:14:01,498:INFO:plot_model() successfully completed......................................
2024-11-25 12:14:04,076:INFO:Initializing plot_model()
2024-11-25 12:14:04,077:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:14:04,077:INFO:Checking exceptions
2024-11-25 12:14:04,078:INFO:Preloading libraries
2024-11-25 12:14:04,078:INFO:Copying training dataset
2024-11-25 12:14:04,078:INFO:Plot type: pr
2024-11-25 12:14:04,291:INFO:Fitting Model
2024-11-25 12:14:04,301:INFO:Scoring test/hold-out set
2024-11-25 12:14:04,476:INFO:Visual Rendered Successfully
2024-11-25 12:14:04,564:INFO:plot_model() successfully completed......................................
2024-11-25 12:14:11,532:INFO:Initializing plot_model()
2024-11-25 12:14:11,532:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:14:11,532:INFO:Checking exceptions
2024-11-25 12:14:11,535:INFO:Preloading libraries
2024-11-25 12:14:11,535:INFO:Copying training dataset
2024-11-25 12:14:11,535:INFO:Plot type: confusion_matrix
2024-11-25 12:14:11,734:INFO:Fitting Model
2024-11-25 12:14:11,734:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:14:11,735:INFO:Scoring test/hold-out set
2024-11-25 12:14:11,851:INFO:Visual Rendered Successfully
2024-11-25 12:14:11,928:INFO:plot_model() successfully completed......................................
2024-11-25 12:14:21,636:INFO:Initializing plot_model()
2024-11-25 12:14:21,636:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F0E6787A10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:14:21,636:INFO:Checking exceptions
2024-11-25 12:14:21,636:INFO:Preloading libraries
2024-11-25 12:14:21,651:INFO:Copying training dataset
2024-11-25 12:14:21,651:INFO:Plot type: feature_all
2024-11-25 12:14:22,005:INFO:Visual Rendered Successfully
2024-11-25 12:14:22,093:INFO:plot_model() successfully completed......................................
2024-11-25 12:19:00,768:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_a2ac2a87dfee4b38851b044bc6aa79c8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_9076bf241e28460e83925e886e640b19_0114f18ad17a47029125d76335b6a4c9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_3176f36ad74b4f6faa321ef660310182
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_8909dc48fd324da7b79ea71fb9f3cd23_40287f7c9d4c42769d4d7d14a0418f75
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_b7f9e94989194348a2509e040409e6f8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_9efe00d4606e47babd91a45c72289137_840f2f4750f84a8ba0b253aec37ef4ff
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_91714329c0fd4c608ee5bfdebdae4997
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_8cfc462f2b3f45ac9beeb06e2187348e_2f86bbc3d638439fa45a9674bf9f8b55
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_25fc813c9c9e41ba82f2319fcfab7ba0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_1ad1929a289244b485fd120dade57a42_13c2612e970e4c14ba8333620ee945be
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_9a9dcb94384540cc906e1edc48150b42
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_bde1c4f2442441ff82e6b73185c62b0c_8466d19f51b64d3b90f4d45f3f7edf82
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_345ae50a15cb4d51b9221ed1ed1a1779
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,770:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_1d9c8d6525c14c1f9b812b935536a633_00d64f204d8241a7a830e6ec7dc407d5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_58ec7bb5c8d546e3a9a0c294b3d27f6a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_8e0ecd6f988343898fbdd56611af5a51_b65e5296c8c34cf8a7ceec6d8b30066e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_09713e5e6e3a4fe7899760af7d6585a8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_1491f9023b3b422ca77bd9861fb597e0_546522ea441241729e33cea0a33024c8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_b1deea3617be4dfe8bfa0175a62767a0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,771:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_81b3977a057947ef978726d3bc36a524_f0c3dc05b8c6444095ca2675e3b775aa
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_742580ad3a3246adb61d271bbdc3b596
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_c3b74b1d63cd4989b6a4df06962125eb_db347cf66cd94f618e458c3d4fae54fb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_4181a01800b14e4ab77356b4f9e7db84
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_b524c634cac341119b0bb863c19a7712_49dc7b56aa914284b7fd3971213f7e33
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_498df4283e0d40d399c361a0ed6867ae
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_e7a7f0019a264b80854272d132210573_5dba5f65603a4209a362798bbf04a05a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_1c655c3940fb42058446ba0a442d6f16
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_3b56f686b0c242738031873f619625c9_77d22605aa834971a7b6b9fffd4fddf0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_5eaf7ad4524e4010813a0a6a2321f529
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_5ccb348bf94c4bcdb041ad20c44ccef8_27dad7d06e5049e7811e05c4979e3ae6
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_382217134c53465bbb3b5f3ed78ed307
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,773:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_2a878ab7cd244941985901616bb7ca8a_c402c25bbc3a4cb9a38ca4b247a2fe14
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_9e09330ea89d4895b324b120c9b561b1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_f97f675330a1413b97687ab94fda566a_95f581fc53584b228aab4b21d0f703bd
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_4a1b11d22557495e95aefdcaf4690565
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_e6a3b6bb6b624f81bca28e3bbc225cbe_dc02b63906894feba07bbeac8b93f15f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_cb44249590cf43e1a852dd4e33ca0af1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_1a2b03b1b59b4794aacc1c9c774c636e_847c0eb759954d1aad25152675dfc27c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_135a76bbc6df4a2788dc9cb15d074735
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,774:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_7303b3c8b54d472eb6cc4c53a70d8139_fbd9b3cbdc0b4b52b4fdd7856d73ce21
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_131cea3779f947caa5770eb736a482fa
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_60695a5c662946c497140b730a357f93_02830fe3fead446f928af20136d64c56
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_939fd6278e3c4803b83000ea9b930e37
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_bec3ab376c5b43ae84d59488bd6b3bfa_182f2c9aa07347caa5ddd74a659cfca9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_3d8eeede472e4089b22797c3b205fb49
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_ad9201331e3c4d83bcc2bad33034a8ce_85cb10d470234d959e42dd27f700ad26
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_b19d3828b2144a33bcd9629a58152b3d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_a7b93dd5382c45e7841bf6087096b526_c637a0e689ac49468cbc010abdf62538
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_1f6428b131b84ae8bc409c8cf7a93a5d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,775:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_2eaf3b0d0f9d425abdb0d46fbaaccd89_7e2a93dccefe4f938a0067ca957027d3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_ea8ccf77a1304bfe8cf29301fa5a77fc
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_6c17f00dfda244e988521a6fbc0bbc29_ef021a0e5d0741e5a85d0b9de7cdfdc0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_408a075eefd348b09777141a23b83dad
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_1c1d13d28a6049f992da408c4a7e7aca_61ac3fb30cac4b3f8f31cb2df4da373a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_56a95e1ce8e947338981deb7ccccf334
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:00,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_14180_952e7da119ed4480b050a67e9016182a_59cd3c7c5c174f92a8825bd4ad031c26
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:19:10,346:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:19:10,346:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:19:10,346:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:19:10,346:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:19:10,764:INFO:PyCaret ClassificationExperiment
2024-11-25 12:19:10,764:INFO:Logging name: clf-default-name
2024-11-25 12:19:10,764:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:19:10,764:INFO:version 3.3.2
2024-11-25 12:19:10,764:INFO:Initializing setup()
2024-11-25 12:19:10,764:INFO:self.USI: 9b9f
2024-11-25 12:19:10,764:INFO:self._variable_keys: {'fold_groups_param', 'n_jobs_param', 'USI', 'idx', 'memory', 'exp_name_log', 'html_param', '_ml_usecase', 'logging_param', '_available_plots', 'pipeline', 'log_plots_param', 'fold_generator', 'X_train', 'y', 'fix_imbalance', 'y_train', 'X_test', 'target_param', 'seed', 'y_test', 'gpu_n_jobs_param', 'is_multiclass', 'exp_id', 'gpu_param', 'X', 'fold_shuffle_param', 'data'}
2024-11-25 12:19:10,764:INFO:Checking environment
2024-11-25 12:19:10,764:INFO:python_version: 3.11.9
2024-11-25 12:19:10,764:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:19:10,764:INFO:machine: AMD64
2024-11-25 12:19:10,764:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:19:10,769:INFO:Memory: svmem(total=25525833728, available=14694821888, percent=42.4, used=10831011840, free=14694821888)
2024-11-25 12:19:10,769:INFO:Physical Core: 4
2024-11-25 12:19:10,769:INFO:Logical Core: 8
2024-11-25 12:19:10,769:INFO:Checking libraries
2024-11-25 12:19:10,769:INFO:System:
2024-11-25 12:19:10,769:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:19:10,770:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:19:10,770:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:19:10,770:INFO:PyCaret required dependencies:
2024-11-25 12:19:10,789:INFO:                 pip: 24.0
2024-11-25 12:19:10,789:INFO:          setuptools: 65.5.0
2024-11-25 12:19:10,789:INFO:             pycaret: 3.3.2
2024-11-25 12:19:10,789:INFO:             IPython: 8.29.0
2024-11-25 12:19:10,789:INFO:          ipywidgets: 8.1.5
2024-11-25 12:19:10,789:INFO:                tqdm: 4.66.6
2024-11-25 12:19:10,789:INFO:               numpy: 1.26.4
2024-11-25 12:19:10,789:INFO:              pandas: 2.1.4
2024-11-25 12:19:10,789:INFO:              jinja2: 3.1.4
2024-11-25 12:19:10,789:INFO:               scipy: 1.11.4
2024-11-25 12:19:10,789:INFO:              joblib: 1.3.2
2024-11-25 12:19:10,789:INFO:             sklearn: 1.4.2
2024-11-25 12:19:10,789:INFO:                pyod: 2.0.2
2024-11-25 12:19:10,789:INFO:            imblearn: 0.12.4
2024-11-25 12:19:10,789:INFO:   category_encoders: 2.6.4
2024-11-25 12:19:10,789:INFO:            lightgbm: 4.5.0
2024-11-25 12:19:10,789:INFO:               numba: 0.60.0
2024-11-25 12:19:10,790:INFO:            requests: 2.32.3
2024-11-25 12:19:10,790:INFO:          matplotlib: 3.7.5
2024-11-25 12:19:10,790:INFO:          scikitplot: 0.3.7
2024-11-25 12:19:10,790:INFO:         yellowbrick: 1.5
2024-11-25 12:19:10,790:INFO:              plotly: 5.24.1
2024-11-25 12:19:10,790:INFO:    plotly-resampler: Not installed
2024-11-25 12:19:10,790:INFO:             kaleido: 0.2.1
2024-11-25 12:19:10,790:INFO:           schemdraw: 0.15
2024-11-25 12:19:10,790:INFO:         statsmodels: 0.14.4
2024-11-25 12:19:10,790:INFO:              sktime: 0.26.0
2024-11-25 12:19:10,790:INFO:               tbats: 1.1.3
2024-11-25 12:19:10,790:INFO:            pmdarima: 2.0.4
2024-11-25 12:19:10,790:INFO:              psutil: 6.1.0
2024-11-25 12:19:10,790:INFO:          markupsafe: 3.0.2
2024-11-25 12:19:10,790:INFO:             pickle5: Not installed
2024-11-25 12:19:10,790:INFO:         cloudpickle: 3.1.0
2024-11-25 12:19:10,790:INFO:         deprecation: 2.1.0
2024-11-25 12:19:10,790:INFO:              xxhash: 3.5.0
2024-11-25 12:19:10,790:INFO:           wurlitzer: 3.1.1
2024-11-25 12:19:10,790:INFO:PyCaret optional dependencies:
2024-11-25 12:19:10,801:INFO:                shap: Not installed
2024-11-25 12:19:10,801:INFO:           interpret: Not installed
2024-11-25 12:19:10,802:INFO:                umap: Not installed
2024-11-25 12:19:10,802:INFO:     ydata_profiling: Not installed
2024-11-25 12:19:10,802:INFO:  explainerdashboard: Not installed
2024-11-25 12:19:10,802:INFO:             autoviz: Not installed
2024-11-25 12:19:10,802:INFO:           fairlearn: Not installed
2024-11-25 12:19:10,802:INFO:          deepchecks: Not installed
2024-11-25 12:19:10,802:INFO:             xgboost: Not installed
2024-11-25 12:19:10,802:INFO:            catboost: Not installed
2024-11-25 12:19:10,802:INFO:              kmodes: Not installed
2024-11-25 12:19:10,802:INFO:             mlxtend: Not installed
2024-11-25 12:19:10,802:INFO:       statsforecast: Not installed
2024-11-25 12:19:10,802:INFO:        tune_sklearn: Not installed
2024-11-25 12:19:10,802:INFO:                 ray: Not installed
2024-11-25 12:19:10,802:INFO:            hyperopt: Not installed
2024-11-25 12:19:10,802:INFO:              optuna: Not installed
2024-11-25 12:19:10,802:INFO:               skopt: Not installed
2024-11-25 12:19:10,802:INFO:              mlflow: Not installed
2024-11-25 12:19:10,802:INFO:              gradio: Not installed
2024-11-25 12:19:10,802:INFO:             fastapi: Not installed
2024-11-25 12:19:10,802:INFO:             uvicorn: Not installed
2024-11-25 12:19:10,802:INFO:              m2cgen: Not installed
2024-11-25 12:19:10,802:INFO:           evidently: Not installed
2024-11-25 12:19:10,802:INFO:               fugue: Not installed
2024-11-25 12:19:10,802:INFO:           streamlit: Not installed
2024-11-25 12:19:10,802:INFO:             prophet: Not installed
2024-11-25 12:19:10,802:INFO:None
2024-11-25 12:19:10,802:INFO:Set up data.
2024-11-25 12:19:10,808:INFO:Set up folding strategy.
2024-11-25 12:19:10,808:INFO:Set up train/test split.
2024-11-25 12:19:10,813:INFO:Set up index.
2024-11-25 12:19:10,813:INFO:Assigning column types.
2024-11-25 12:19:10,817:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:19:10,858:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:19:10,861:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:10,894:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:10,894:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:10,936:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:19:10,936:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:10,962:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:10,962:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:10,963:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:19:11,004:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,031:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,032:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,076:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,102:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,103:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:19:11,169:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,241:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,241:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,243:INFO:Preparing preprocessing pipeline...
2024-11-25 12:19:11,243:INFO:Set up simple imputation.
2024-11-25 12:19:11,246:INFO:Set up encoding of categorical features.
2024-11-25 12:19:11,247:INFO:Set up column name cleaning.
2024-11-25 12:19:11,376:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:19:11,382:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:19:11,382:INFO:Creating final display dataframe.
2024-11-25 12:19:11,574:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              9b9f
2024-11-25 12:19:11,664:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,736:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,736:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,737:INFO:setup() successfully completed in 0.98s...............
2024-11-25 12:19:11,753:INFO:PyCaret ClassificationExperiment
2024-11-25 12:19:11,753:INFO:Logging name: clf-default-name
2024-11-25 12:19:11,753:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:19:11,753:INFO:version 3.3.2
2024-11-25 12:19:11,753:INFO:Initializing setup()
2024-11-25 12:19:11,753:INFO:self.USI: c225
2024-11-25 12:19:11,753:INFO:self._variable_keys: {'fold_groups_param', 'n_jobs_param', 'USI', 'idx', 'memory', 'exp_name_log', 'html_param', '_ml_usecase', 'logging_param', '_available_plots', 'pipeline', 'log_plots_param', 'fold_generator', 'X_train', 'y', 'fix_imbalance', 'y_train', 'X_test', 'target_param', 'seed', 'y_test', 'gpu_n_jobs_param', 'is_multiclass', 'exp_id', 'gpu_param', 'X', 'fold_shuffle_param', 'data'}
2024-11-25 12:19:11,753:INFO:Checking environment
2024-11-25 12:19:11,754:INFO:python_version: 3.11.9
2024-11-25 12:19:11,754:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:19:11,754:INFO:machine: AMD64
2024-11-25 12:19:11,754:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:19:11,757:INFO:Memory: svmem(total=25525833728, available=14713221120, percent=42.4, used=10812612608, free=14713221120)
2024-11-25 12:19:11,757:INFO:Physical Core: 4
2024-11-25 12:19:11,758:INFO:Logical Core: 8
2024-11-25 12:19:11,758:INFO:Checking libraries
2024-11-25 12:19:11,758:INFO:System:
2024-11-25 12:19:11,758:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:19:11,758:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:19:11,758:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:19:11,758:INFO:PyCaret required dependencies:
2024-11-25 12:19:11,758:INFO:                 pip: 24.0
2024-11-25 12:19:11,758:INFO:          setuptools: 65.5.0
2024-11-25 12:19:11,758:INFO:             pycaret: 3.3.2
2024-11-25 12:19:11,758:INFO:             IPython: 8.29.0
2024-11-25 12:19:11,758:INFO:          ipywidgets: 8.1.5
2024-11-25 12:19:11,758:INFO:                tqdm: 4.66.6
2024-11-25 12:19:11,758:INFO:               numpy: 1.26.4
2024-11-25 12:19:11,758:INFO:              pandas: 2.1.4
2024-11-25 12:19:11,758:INFO:              jinja2: 3.1.4
2024-11-25 12:19:11,758:INFO:               scipy: 1.11.4
2024-11-25 12:19:11,758:INFO:              joblib: 1.3.2
2024-11-25 12:19:11,758:INFO:             sklearn: 1.4.2
2024-11-25 12:19:11,758:INFO:                pyod: 2.0.2
2024-11-25 12:19:11,758:INFO:            imblearn: 0.12.4
2024-11-25 12:19:11,758:INFO:   category_encoders: 2.6.4
2024-11-25 12:19:11,758:INFO:            lightgbm: 4.5.0
2024-11-25 12:19:11,758:INFO:               numba: 0.60.0
2024-11-25 12:19:11,758:INFO:            requests: 2.32.3
2024-11-25 12:19:11,758:INFO:          matplotlib: 3.7.5
2024-11-25 12:19:11,758:INFO:          scikitplot: 0.3.7
2024-11-25 12:19:11,759:INFO:         yellowbrick: 1.5
2024-11-25 12:19:11,759:INFO:              plotly: 5.24.1
2024-11-25 12:19:11,759:INFO:    plotly-resampler: Not installed
2024-11-25 12:19:11,759:INFO:             kaleido: 0.2.1
2024-11-25 12:19:11,759:INFO:           schemdraw: 0.15
2024-11-25 12:19:11,759:INFO:         statsmodels: 0.14.4
2024-11-25 12:19:11,759:INFO:              sktime: 0.26.0
2024-11-25 12:19:11,759:INFO:               tbats: 1.1.3
2024-11-25 12:19:11,759:INFO:            pmdarima: 2.0.4
2024-11-25 12:19:11,759:INFO:              psutil: 6.1.0
2024-11-25 12:19:11,759:INFO:          markupsafe: 3.0.2
2024-11-25 12:19:11,759:INFO:             pickle5: Not installed
2024-11-25 12:19:11,759:INFO:         cloudpickle: 3.1.0
2024-11-25 12:19:11,759:INFO:         deprecation: 2.1.0
2024-11-25 12:19:11,759:INFO:              xxhash: 3.5.0
2024-11-25 12:19:11,759:INFO:           wurlitzer: 3.1.1
2024-11-25 12:19:11,759:INFO:PyCaret optional dependencies:
2024-11-25 12:19:11,759:INFO:                shap: Not installed
2024-11-25 12:19:11,759:INFO:           interpret: Not installed
2024-11-25 12:19:11,759:INFO:                umap: Not installed
2024-11-25 12:19:11,759:INFO:     ydata_profiling: Not installed
2024-11-25 12:19:11,759:INFO:  explainerdashboard: Not installed
2024-11-25 12:19:11,759:INFO:             autoviz: Not installed
2024-11-25 12:19:11,759:INFO:           fairlearn: Not installed
2024-11-25 12:19:11,759:INFO:          deepchecks: Not installed
2024-11-25 12:19:11,759:INFO:             xgboost: Not installed
2024-11-25 12:19:11,759:INFO:            catboost: Not installed
2024-11-25 12:19:11,759:INFO:              kmodes: Not installed
2024-11-25 12:19:11,759:INFO:             mlxtend: Not installed
2024-11-25 12:19:11,759:INFO:       statsforecast: Not installed
2024-11-25 12:19:11,759:INFO:        tune_sklearn: Not installed
2024-11-25 12:19:11,760:INFO:                 ray: Not installed
2024-11-25 12:19:11,760:INFO:            hyperopt: Not installed
2024-11-25 12:19:11,760:INFO:              optuna: Not installed
2024-11-25 12:19:11,760:INFO:               skopt: Not installed
2024-11-25 12:19:11,760:INFO:              mlflow: Not installed
2024-11-25 12:19:11,760:INFO:              gradio: Not installed
2024-11-25 12:19:11,760:INFO:             fastapi: Not installed
2024-11-25 12:19:11,760:INFO:             uvicorn: Not installed
2024-11-25 12:19:11,760:INFO:              m2cgen: Not installed
2024-11-25 12:19:11,760:INFO:           evidently: Not installed
2024-11-25 12:19:11,760:INFO:               fugue: Not installed
2024-11-25 12:19:11,760:INFO:           streamlit: Not installed
2024-11-25 12:19:11,760:INFO:             prophet: Not installed
2024-11-25 12:19:11,760:INFO:None
2024-11-25 12:19:11,760:INFO:Set up data.
2024-11-25 12:19:11,765:INFO:Set up folding strategy.
2024-11-25 12:19:11,765:INFO:Set up train/test split.
2024-11-25 12:19:11,770:INFO:Set up index.
2024-11-25 12:19:11,771:INFO:Assigning column types.
2024-11-25 12:19:11,774:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:19:11,814:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,815:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,841:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,883:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,884:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,909:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,909:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,910:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:19:11,951:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:11,976:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:11,977:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,019:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:19:12,044:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,044:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,044:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:19:12,116:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,117:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,194:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,194:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,195:INFO:Preparing preprocessing pipeline...
2024-11-25 12:19:12,196:INFO:Set up simple imputation.
2024-11-25 12:19:12,198:INFO:Set up encoding of categorical features.
2024-11-25 12:19:12,199:INFO:Set up column name cleaning.
2024-11-25 12:19:12,328:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:19:12,333:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:19:12,333:INFO:Creating final display dataframe.
2024-11-25 12:19:12,559:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c225
2024-11-25 12:19:12,644:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,717:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,717:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:19:12,718:INFO:setup() successfully completed in 0.97s...............
2024-11-25 12:19:12,724:INFO:Initializing compare_models()
2024-11-25 12:19:12,724:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:19:12,724:INFO:Checking exceptions
2024-11-25 12:19:12,729:INFO:Preparing display monitor
2024-11-25 12:19:12,754:INFO:Initializing Logistic Regression
2024-11-25 12:19:12,754:INFO:Total runtime is 0.0 minutes
2024-11-25 12:19:12,760:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:12,761:INFO:Initializing create_model()
2024-11-25 12:19:12,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:12,761:INFO:Checking exceptions
2024-11-25 12:19:12,762:INFO:Importing libraries
2024-11-25 12:19:12,762:INFO:Copying training dataset
2024-11-25 12:19:12,766:INFO:Defining folds
2024-11-25 12:19:12,766:INFO:Declaring metric variables
2024-11-25 12:19:12,772:INFO:Importing untrained model
2024-11-25 12:19:12,777:INFO:Logistic Regression Imported successfully
2024-11-25 12:19:12,786:INFO:Starting cross validation
2024-11-25 12:19:12,790:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:16,122:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:16,157:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:16,177:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:16,187:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:16,200:INFO:Calculating mean and std
2024-11-25 12:19:16,203:INFO:Creating metrics dataframe
2024-11-25 12:19:16,206:INFO:Uploading results into container
2024-11-25 12:19:16,207:INFO:Uploading model into container now
2024-11-25 12:19:16,208:INFO:_master_model_container: 1
2024-11-25 12:19:16,209:INFO:_display_container: 2
2024-11-25 12:19:16,209:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:19:16,209:INFO:create_model() successfully completed......................................
2024-11-25 12:19:16,307:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:16,307:INFO:Creating metrics dataframe
2024-11-25 12:19:16,312:INFO:Initializing K Neighbors Classifier
2024-11-25 12:19:16,312:INFO:Total runtime is 0.05930240154266357 minutes
2024-11-25 12:19:16,315:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:16,315:INFO:Initializing create_model()
2024-11-25 12:19:16,315:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:16,316:INFO:Checking exceptions
2024-11-25 12:19:16,316:INFO:Importing libraries
2024-11-25 12:19:16,316:INFO:Copying training dataset
2024-11-25 12:19:16,321:INFO:Defining folds
2024-11-25 12:19:16,321:INFO:Declaring metric variables
2024-11-25 12:19:16,325:INFO:Importing untrained model
2024-11-25 12:19:16,328:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:19:16,337:INFO:Starting cross validation
2024-11-25 12:19:16,341:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:19,076:INFO:Calculating mean and std
2024-11-25 12:19:19,076:INFO:Creating metrics dataframe
2024-11-25 12:19:19,080:INFO:Uploading results into container
2024-11-25 12:19:19,080:INFO:Uploading model into container now
2024-11-25 12:19:19,080:INFO:_master_model_container: 2
2024-11-25 12:19:19,080:INFO:_display_container: 2
2024-11-25 12:19:19,080:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:19:19,080:INFO:create_model() successfully completed......................................
2024-11-25 12:19:19,174:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:19,174:INFO:Creating metrics dataframe
2024-11-25 12:19:19,180:INFO:Initializing Naive Bayes
2024-11-25 12:19:19,180:INFO:Total runtime is 0.107094140847524 minutes
2024-11-25 12:19:19,180:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:19,180:INFO:Initializing create_model()
2024-11-25 12:19:19,180:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:19,180:INFO:Checking exceptions
2024-11-25 12:19:19,180:INFO:Importing libraries
2024-11-25 12:19:19,180:INFO:Copying training dataset
2024-11-25 12:19:19,180:INFO:Defining folds
2024-11-25 12:19:19,180:INFO:Declaring metric variables
2024-11-25 12:19:19,180:INFO:Importing untrained model
2024-11-25 12:19:19,196:INFO:Naive Bayes Imported successfully
2024-11-25 12:19:19,196:INFO:Starting cross validation
2024-11-25 12:19:19,196:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:19,470:INFO:Calculating mean and std
2024-11-25 12:19:19,470:INFO:Creating metrics dataframe
2024-11-25 12:19:19,470:INFO:Uploading results into container
2024-11-25 12:19:19,470:INFO:Uploading model into container now
2024-11-25 12:19:19,470:INFO:_master_model_container: 3
2024-11-25 12:19:19,470:INFO:_display_container: 2
2024-11-25 12:19:19,470:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:19:19,470:INFO:create_model() successfully completed......................................
2024-11-25 12:19:19,527:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:19,527:INFO:Creating metrics dataframe
2024-11-25 12:19:19,542:INFO:Initializing Decision Tree Classifier
2024-11-25 12:19:19,542:INFO:Total runtime is 0.11313522259394328 minutes
2024-11-25 12:19:19,542:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:19,542:INFO:Initializing create_model()
2024-11-25 12:19:19,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:19,542:INFO:Checking exceptions
2024-11-25 12:19:19,542:INFO:Importing libraries
2024-11-25 12:19:19,542:INFO:Copying training dataset
2024-11-25 12:19:19,558:INFO:Defining folds
2024-11-25 12:19:19,558:INFO:Declaring metric variables
2024-11-25 12:19:19,558:INFO:Importing untrained model
2024-11-25 12:19:19,558:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:19:19,558:INFO:Starting cross validation
2024-11-25 12:19:19,574:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:19,818:INFO:Calculating mean and std
2024-11-25 12:19:19,818:INFO:Creating metrics dataframe
2024-11-25 12:19:19,818:INFO:Uploading results into container
2024-11-25 12:19:19,818:INFO:Uploading model into container now
2024-11-25 12:19:19,818:INFO:_master_model_container: 4
2024-11-25 12:19:19,818:INFO:_display_container: 2
2024-11-25 12:19:19,818:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:19:19,818:INFO:create_model() successfully completed......................................
2024-11-25 12:19:19,881:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:19,881:INFO:Creating metrics dataframe
2024-11-25 12:19:19,881:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:19:19,881:INFO:Total runtime is 0.11877205371856689 minutes
2024-11-25 12:19:19,881:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:19,896:INFO:Initializing create_model()
2024-11-25 12:19:19,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:19,896:INFO:Checking exceptions
2024-11-25 12:19:19,896:INFO:Importing libraries
2024-11-25 12:19:19,896:INFO:Copying training dataset
2024-11-25 12:19:19,896:INFO:Defining folds
2024-11-25 12:19:19,896:INFO:Declaring metric variables
2024-11-25 12:19:19,896:INFO:Importing untrained model
2024-11-25 12:19:19,896:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:19:19,912:INFO:Starting cross validation
2024-11-25 12:19:19,912:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:20,136:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,146:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,155:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,179:INFO:Calculating mean and std
2024-11-25 12:19:20,179:INFO:Creating metrics dataframe
2024-11-25 12:19:20,181:INFO:Uploading results into container
2024-11-25 12:19:20,181:INFO:Uploading model into container now
2024-11-25 12:19:20,181:INFO:_master_model_container: 5
2024-11-25 12:19:20,181:INFO:_display_container: 2
2024-11-25 12:19:20,181:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:19:20,181:INFO:create_model() successfully completed......................................
2024-11-25 12:19:20,243:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:20,243:INFO:Creating metrics dataframe
2024-11-25 12:19:20,251:INFO:Initializing Ridge Classifier
2024-11-25 12:19:20,251:INFO:Total runtime is 0.12495200236638386 minutes
2024-11-25 12:19:20,255:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:20,255:INFO:Initializing create_model()
2024-11-25 12:19:20,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:20,255:INFO:Checking exceptions
2024-11-25 12:19:20,255:INFO:Importing libraries
2024-11-25 12:19:20,255:INFO:Copying training dataset
2024-11-25 12:19:20,259:INFO:Defining folds
2024-11-25 12:19:20,259:INFO:Declaring metric variables
2024-11-25 12:19:20,263:INFO:Importing untrained model
2024-11-25 12:19:20,271:INFO:Ridge Classifier Imported successfully
2024-11-25 12:19:20,279:INFO:Starting cross validation
2024-11-25 12:19:20,283:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:20,483:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,487:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,511:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,519:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:20,531:INFO:Calculating mean and std
2024-11-25 12:19:20,531:INFO:Creating metrics dataframe
2024-11-25 12:19:20,531:INFO:Uploading results into container
2024-11-25 12:19:20,531:INFO:Uploading model into container now
2024-11-25 12:19:20,535:INFO:_master_model_container: 6
2024-11-25 12:19:20,535:INFO:_display_container: 2
2024-11-25 12:19:20,535:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:19:20,535:INFO:create_model() successfully completed......................................
2024-11-25 12:19:20,599:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:20,599:INFO:Creating metrics dataframe
2024-11-25 12:19:20,607:INFO:Initializing Random Forest Classifier
2024-11-25 12:19:20,607:INFO:Total runtime is 0.130885378519694 minutes
2024-11-25 12:19:20,611:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:20,611:INFO:Initializing create_model()
2024-11-25 12:19:20,611:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:20,611:INFO:Checking exceptions
2024-11-25 12:19:20,611:INFO:Importing libraries
2024-11-25 12:19:20,611:INFO:Copying training dataset
2024-11-25 12:19:20,615:INFO:Defining folds
2024-11-25 12:19:20,615:INFO:Declaring metric variables
2024-11-25 12:19:20,619:INFO:Importing untrained model
2024-11-25 12:19:20,623:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:19:20,631:INFO:Starting cross validation
2024-11-25 12:19:20,631:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:21,128:INFO:Calculating mean and std
2024-11-25 12:19:21,128:INFO:Creating metrics dataframe
2024-11-25 12:19:21,128:INFO:Uploading results into container
2024-11-25 12:19:21,128:INFO:Uploading model into container now
2024-11-25 12:19:21,128:INFO:_master_model_container: 7
2024-11-25 12:19:21,128:INFO:_display_container: 2
2024-11-25 12:19:21,132:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:19:21,132:INFO:create_model() successfully completed......................................
2024-11-25 12:19:21,198:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:21,198:INFO:Creating metrics dataframe
2024-11-25 12:19:21,202:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:19:21,202:INFO:Total runtime is 0.14080082178115844 minutes
2024-11-25 12:19:21,206:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:21,206:INFO:Initializing create_model()
2024-11-25 12:19:21,206:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:21,206:INFO:Checking exceptions
2024-11-25 12:19:21,206:INFO:Importing libraries
2024-11-25 12:19:21,206:INFO:Copying training dataset
2024-11-25 12:19:21,210:INFO:Defining folds
2024-11-25 12:19:21,210:INFO:Declaring metric variables
2024-11-25 12:19:21,214:INFO:Importing untrained model
2024-11-25 12:19:21,218:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:19:21,226:INFO:Starting cross validation
2024-11-25 12:19:21,230:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:21,378:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:21,382:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:21,386:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:21,394:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:21,426:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,426:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,442:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,446:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,465:INFO:Calculating mean and std
2024-11-25 12:19:21,465:INFO:Creating metrics dataframe
2024-11-25 12:19:21,468:INFO:Uploading results into container
2024-11-25 12:19:21,468:INFO:Uploading model into container now
2024-11-25 12:19:21,468:INFO:_master_model_container: 8
2024-11-25 12:19:21,468:INFO:_display_container: 2
2024-11-25 12:19:21,468:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:19:21,468:INFO:create_model() successfully completed......................................
2024-11-25 12:19:21,544:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:21,544:INFO:Creating metrics dataframe
2024-11-25 12:19:21,552:INFO:Initializing Ada Boost Classifier
2024-11-25 12:19:21,552:INFO:Total runtime is 0.1466349403063456 minutes
2024-11-25 12:19:21,556:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:21,556:INFO:Initializing create_model()
2024-11-25 12:19:21,556:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:21,556:INFO:Checking exceptions
2024-11-25 12:19:21,556:INFO:Importing libraries
2024-11-25 12:19:21,556:INFO:Copying training dataset
2024-11-25 12:19:21,560:INFO:Defining folds
2024-11-25 12:19:21,560:INFO:Declaring metric variables
2024-11-25 12:19:21,564:INFO:Importing untrained model
2024-11-25 12:19:21,564:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:19:21,576:INFO:Starting cross validation
2024-11-25 12:19:21,576:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:21,724:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:21,740:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:21,744:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:21,748:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:21,879:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,891:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,898:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,906:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:21,924:INFO:Calculating mean and std
2024-11-25 12:19:21,924:INFO:Creating metrics dataframe
2024-11-25 12:19:21,924:INFO:Uploading results into container
2024-11-25 12:19:21,928:INFO:Uploading model into container now
2024-11-25 12:19:21,928:INFO:_master_model_container: 9
2024-11-25 12:19:21,928:INFO:_display_container: 2
2024-11-25 12:19:21,928:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:19:21,928:INFO:create_model() successfully completed......................................
2024-11-25 12:19:21,992:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:21,992:INFO:Creating metrics dataframe
2024-11-25 12:19:22,000:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:19:22,000:INFO:Total runtime is 0.15409647623697914 minutes
2024-11-25 12:19:22,004:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:22,004:INFO:Initializing create_model()
2024-11-25 12:19:22,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:22,004:INFO:Checking exceptions
2024-11-25 12:19:22,004:INFO:Importing libraries
2024-11-25 12:19:22,004:INFO:Copying training dataset
2024-11-25 12:19:22,008:INFO:Defining folds
2024-11-25 12:19:22,008:INFO:Declaring metric variables
2024-11-25 12:19:22,012:INFO:Importing untrained model
2024-11-25 12:19:22,016:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:19:22,024:INFO:Starting cross validation
2024-11-25 12:19:22,028:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:22,577:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,585:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,648:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,662:INFO:Calculating mean and std
2024-11-25 12:19:22,662:INFO:Creating metrics dataframe
2024-11-25 12:19:22,662:INFO:Uploading results into container
2024-11-25 12:19:22,666:INFO:Uploading model into container now
2024-11-25 12:19:22,666:INFO:_master_model_container: 10
2024-11-25 12:19:22,666:INFO:_display_container: 2
2024-11-25 12:19:22,666:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:19:22,666:INFO:create_model() successfully completed......................................
2024-11-25 12:19:22,730:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:22,730:INFO:Creating metrics dataframe
2024-11-25 12:19:22,738:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:19:22,738:INFO:Total runtime is 0.1663988669713338 minutes
2024-11-25 12:19:22,742:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:22,742:INFO:Initializing create_model()
2024-11-25 12:19:22,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:22,742:INFO:Checking exceptions
2024-11-25 12:19:22,742:INFO:Importing libraries
2024-11-25 12:19:22,742:INFO:Copying training dataset
2024-11-25 12:19:22,746:INFO:Defining folds
2024-11-25 12:19:22,746:INFO:Declaring metric variables
2024-11-25 12:19:22,750:INFO:Importing untrained model
2024-11-25 12:19:22,754:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:19:22,766:INFO:Starting cross validation
2024-11-25 12:19:22,766:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:22,958:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,966:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,970:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,970:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:22,989:INFO:Calculating mean and std
2024-11-25 12:19:22,989:INFO:Creating metrics dataframe
2024-11-25 12:19:22,989:INFO:Uploading results into container
2024-11-25 12:19:22,989:INFO:Uploading model into container now
2024-11-25 12:19:22,989:INFO:_master_model_container: 11
2024-11-25 12:19:22,989:INFO:_display_container: 2
2024-11-25 12:19:22,989:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:19:22,989:INFO:create_model() successfully completed......................................
2024-11-25 12:19:23,055:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:23,055:INFO:Creating metrics dataframe
2024-11-25 12:19:23,063:INFO:Initializing Extra Trees Classifier
2024-11-25 12:19:23,063:INFO:Total runtime is 0.17181088924407956 minutes
2024-11-25 12:19:23,067:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:23,067:INFO:Initializing create_model()
2024-11-25 12:19:23,067:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:23,067:INFO:Checking exceptions
2024-11-25 12:19:23,067:INFO:Importing libraries
2024-11-25 12:19:23,067:INFO:Copying training dataset
2024-11-25 12:19:23,071:INFO:Defining folds
2024-11-25 12:19:23,071:INFO:Declaring metric variables
2024-11-25 12:19:23,075:INFO:Importing untrained model
2024-11-25 12:19:23,079:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:19:23,087:INFO:Starting cross validation
2024-11-25 12:19:23,091:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:23,590:INFO:Calculating mean and std
2024-11-25 12:19:23,590:INFO:Creating metrics dataframe
2024-11-25 12:19:23,590:INFO:Uploading results into container
2024-11-25 12:19:23,590:INFO:Uploading model into container now
2024-11-25 12:19:23,590:INFO:_master_model_container: 12
2024-11-25 12:19:23,590:INFO:_display_container: 2
2024-11-25 12:19:23,594:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:19:23,594:INFO:create_model() successfully completed......................................
2024-11-25 12:19:23,654:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:23,654:INFO:Creating metrics dataframe
2024-11-25 12:19:23,662:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:19:23,662:INFO:Total runtime is 0.18179348707199094 minutes
2024-11-25 12:19:23,666:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:23,666:INFO:Initializing create_model()
2024-11-25 12:19:23,666:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:23,666:INFO:Checking exceptions
2024-11-25 12:19:23,666:INFO:Importing libraries
2024-11-25 12:19:23,666:INFO:Copying training dataset
2024-11-25 12:19:23,670:INFO:Defining folds
2024-11-25 12:19:23,670:INFO:Declaring metric variables
2024-11-25 12:19:23,674:INFO:Importing untrained model
2024-11-25 12:19:23,678:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:19:23,690:INFO:Starting cross validation
2024-11-25 12:19:23,694:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:24,438:INFO:Calculating mean and std
2024-11-25 12:19:24,438:INFO:Creating metrics dataframe
2024-11-25 12:19:24,442:INFO:Uploading results into container
2024-11-25 12:19:24,442:INFO:Uploading model into container now
2024-11-25 12:19:24,442:INFO:_master_model_container: 13
2024-11-25 12:19:24,446:INFO:_display_container: 2
2024-11-25 12:19:24,446:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:19:24,446:INFO:create_model() successfully completed......................................
2024-11-25 12:19:24,542:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:24,542:INFO:Creating metrics dataframe
2024-11-25 12:19:24,554:INFO:Initializing Dummy Classifier
2024-11-25 12:19:24,554:INFO:Total runtime is 0.19665946165720619 minutes
2024-11-25 12:19:24,558:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:24,558:INFO:Initializing create_model()
2024-11-25 12:19:24,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF63A1F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:24,558:INFO:Checking exceptions
2024-11-25 12:19:24,558:INFO:Importing libraries
2024-11-25 12:19:24,558:INFO:Copying training dataset
2024-11-25 12:19:24,566:INFO:Defining folds
2024-11-25 12:19:24,566:INFO:Declaring metric variables
2024-11-25 12:19:24,570:INFO:Importing untrained model
2024-11-25 12:19:24,574:INFO:Dummy Classifier Imported successfully
2024-11-25 12:19:24,582:INFO:Starting cross validation
2024-11-25 12:19:24,586:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:24,785:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:24,789:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:24,809:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:24,813:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:24,821:INFO:Calculating mean and std
2024-11-25 12:19:24,821:INFO:Creating metrics dataframe
2024-11-25 12:19:24,821:INFO:Uploading results into container
2024-11-25 12:19:24,821:INFO:Uploading model into container now
2024-11-25 12:19:24,821:INFO:_master_model_container: 14
2024-11-25 12:19:24,821:INFO:_display_container: 2
2024-11-25 12:19:24,821:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:19:24,821:INFO:create_model() successfully completed......................................
2024-11-25 12:19:24,889:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:24,889:INFO:Creating metrics dataframe
2024-11-25 12:19:24,901:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:19:24,909:INFO:Initializing create_model()
2024-11-25 12:19:24,909:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:24,909:INFO:Checking exceptions
2024-11-25 12:19:24,913:INFO:Importing libraries
2024-11-25 12:19:24,913:INFO:Copying training dataset
2024-11-25 12:19:24,917:INFO:Defining folds
2024-11-25 12:19:24,917:INFO:Declaring metric variables
2024-11-25 12:19:24,917:INFO:Importing untrained model
2024-11-25 12:19:24,917:INFO:Declaring custom model
2024-11-25 12:19:24,917:INFO:Ridge Classifier Imported successfully
2024-11-25 12:19:24,917:INFO:Cross validation set to False
2024-11-25 12:19:24,917:INFO:Fitting Model
2024-11-25 12:19:25,021:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:19:25,021:INFO:create_model() successfully completed......................................
2024-11-25 12:19:25,111:INFO:_master_model_container: 14
2024-11-25 12:19:25,111:INFO:_display_container: 2
2024-11-25 12:19:25,111:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:19:25,111:INFO:compare_models() successfully completed......................................
2024-11-25 12:19:25,119:INFO:Initializing compare_models()
2024-11-25 12:19:25,119:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:19:25,119:INFO:Checking exceptions
2024-11-25 12:19:25,123:INFO:Preparing display monitor
2024-11-25 12:19:25,147:INFO:Initializing Logistic Regression
2024-11-25 12:19:25,147:INFO:Total runtime is 0.0 minutes
2024-11-25 12:19:25,155:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:25,155:INFO:Initializing create_model()
2024-11-25 12:19:25,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:25,155:INFO:Checking exceptions
2024-11-25 12:19:25,155:INFO:Importing libraries
2024-11-25 12:19:25,159:INFO:Copying training dataset
2024-11-25 12:19:25,159:INFO:Defining folds
2024-11-25 12:19:25,159:INFO:Declaring metric variables
2024-11-25 12:19:25,163:INFO:Importing untrained model
2024-11-25 12:19:25,167:INFO:Logistic Regression Imported successfully
2024-11-25 12:19:25,175:INFO:Starting cross validation
2024-11-25 12:19:25,179:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:25,580:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,584:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,599:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,604:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,608:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,611:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,631:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,780:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,782:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:25,800:INFO:Calculating mean and std
2024-11-25 12:19:25,800:INFO:Creating metrics dataframe
2024-11-25 12:19:25,802:INFO:Uploading results into container
2024-11-25 12:19:25,802:INFO:Uploading model into container now
2024-11-25 12:19:25,802:INFO:_master_model_container: 1
2024-11-25 12:19:25,802:INFO:_display_container: 2
2024-11-25 12:19:25,802:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:19:25,802:INFO:create_model() successfully completed......................................
2024-11-25 12:19:25,866:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:25,866:INFO:Creating metrics dataframe
2024-11-25 12:19:25,870:INFO:Initializing K Neighbors Classifier
2024-11-25 12:19:25,870:INFO:Total runtime is 0.012044791380564373 minutes
2024-11-25 12:19:25,874:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:25,874:INFO:Initializing create_model()
2024-11-25 12:19:25,874:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:25,874:INFO:Checking exceptions
2024-11-25 12:19:25,874:INFO:Importing libraries
2024-11-25 12:19:25,874:INFO:Copying training dataset
2024-11-25 12:19:25,878:INFO:Defining folds
2024-11-25 12:19:25,878:INFO:Declaring metric variables
2024-11-25 12:19:25,882:INFO:Importing untrained model
2024-11-25 12:19:25,886:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:19:25,894:INFO:Starting cross validation
2024-11-25 12:19:25,898:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:26,523:INFO:Calculating mean and std
2024-11-25 12:19:26,523:INFO:Creating metrics dataframe
2024-11-25 12:19:26,523:INFO:Uploading results into container
2024-11-25 12:19:26,523:INFO:Uploading model into container now
2024-11-25 12:19:26,527:INFO:_master_model_container: 2
2024-11-25 12:19:26,527:INFO:_display_container: 2
2024-11-25 12:19:26,527:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:19:26,527:INFO:create_model() successfully completed......................................
2024-11-25 12:19:26,590:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:26,590:INFO:Creating metrics dataframe
2024-11-25 12:19:26,594:INFO:Initializing Naive Bayes
2024-11-25 12:19:26,594:INFO:Total runtime is 0.02411086956659953 minutes
2024-11-25 12:19:26,598:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:26,598:INFO:Initializing create_model()
2024-11-25 12:19:26,598:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:26,598:INFO:Checking exceptions
2024-11-25 12:19:26,598:INFO:Importing libraries
2024-11-25 12:19:26,598:INFO:Copying training dataset
2024-11-25 12:19:26,606:INFO:Defining folds
2024-11-25 12:19:26,606:INFO:Declaring metric variables
2024-11-25 12:19:26,610:INFO:Importing untrained model
2024-11-25 12:19:26,614:INFO:Naive Bayes Imported successfully
2024-11-25 12:19:26,622:INFO:Starting cross validation
2024-11-25 12:19:26,626:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:27,183:INFO:Calculating mean and std
2024-11-25 12:19:27,183:INFO:Creating metrics dataframe
2024-11-25 12:19:27,183:INFO:Uploading results into container
2024-11-25 12:19:27,183:INFO:Uploading model into container now
2024-11-25 12:19:27,183:INFO:_master_model_container: 3
2024-11-25 12:19:27,183:INFO:_display_container: 2
2024-11-25 12:19:27,187:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:19:27,187:INFO:create_model() successfully completed......................................
2024-11-25 12:19:27,247:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:27,247:INFO:Creating metrics dataframe
2024-11-25 12:19:27,255:INFO:Initializing Decision Tree Classifier
2024-11-25 12:19:27,255:INFO:Total runtime is 0.035130572319030766 minutes
2024-11-25 12:19:27,259:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:27,259:INFO:Initializing create_model()
2024-11-25 12:19:27,259:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:27,259:INFO:Checking exceptions
2024-11-25 12:19:27,259:INFO:Importing libraries
2024-11-25 12:19:27,259:INFO:Copying training dataset
2024-11-25 12:19:27,267:INFO:Defining folds
2024-11-25 12:19:27,267:INFO:Declaring metric variables
2024-11-25 12:19:27,271:INFO:Importing untrained model
2024-11-25 12:19:27,275:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:19:27,279:INFO:Starting cross validation
2024-11-25 12:19:27,287:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:27,885:INFO:Calculating mean and std
2024-11-25 12:19:27,886:INFO:Creating metrics dataframe
2024-11-25 12:19:27,886:INFO:Uploading results into container
2024-11-25 12:19:27,886:INFO:Uploading model into container now
2024-11-25 12:19:27,886:INFO:_master_model_container: 4
2024-11-25 12:19:27,886:INFO:_display_container: 2
2024-11-25 12:19:27,886:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:19:27,886:INFO:create_model() successfully completed......................................
2024-11-25 12:19:27,951:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:27,951:INFO:Creating metrics dataframe
2024-11-25 12:19:27,959:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:19:27,959:INFO:Total runtime is 0.04685560862223308 minutes
2024-11-25 12:19:27,963:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:27,963:INFO:Initializing create_model()
2024-11-25 12:19:27,963:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:27,963:INFO:Checking exceptions
2024-11-25 12:19:27,963:INFO:Importing libraries
2024-11-25 12:19:27,963:INFO:Copying training dataset
2024-11-25 12:19:27,967:INFO:Defining folds
2024-11-25 12:19:27,967:INFO:Declaring metric variables
2024-11-25 12:19:27,975:INFO:Importing untrained model
2024-11-25 12:19:27,979:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:19:27,987:INFO:Starting cross validation
2024-11-25 12:19:27,987:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:28,380:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,384:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,384:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,394:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,394:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,406:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,416:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,575:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,579:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:28,591:INFO:Calculating mean and std
2024-11-25 12:19:28,591:INFO:Creating metrics dataframe
2024-11-25 12:19:28,591:INFO:Uploading results into container
2024-11-25 12:19:28,591:INFO:Uploading model into container now
2024-11-25 12:19:28,591:INFO:_master_model_container: 5
2024-11-25 12:19:28,595:INFO:_display_container: 2
2024-11-25 12:19:28,595:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:19:28,595:INFO:create_model() successfully completed......................................
2024-11-25 12:19:28,659:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:28,659:INFO:Creating metrics dataframe
2024-11-25 12:19:28,663:INFO:Initializing Ridge Classifier
2024-11-25 12:19:28,663:INFO:Total runtime is 0.05859018166859945 minutes
2024-11-25 12:19:28,667:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:28,667:INFO:Initializing create_model()
2024-11-25 12:19:28,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:28,667:INFO:Checking exceptions
2024-11-25 12:19:28,667:INFO:Importing libraries
2024-11-25 12:19:28,667:INFO:Copying training dataset
2024-11-25 12:19:28,675:INFO:Defining folds
2024-11-25 12:19:28,675:INFO:Declaring metric variables
2024-11-25 12:19:28,679:INFO:Importing untrained model
2024-11-25 12:19:28,683:INFO:Ridge Classifier Imported successfully
2024-11-25 12:19:28,691:INFO:Starting cross validation
2024-11-25 12:19:28,691:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:29,070:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,077:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,082:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,086:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,091:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,116:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,118:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,119:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,286:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,288:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:29,302:INFO:Calculating mean and std
2024-11-25 12:19:29,303:INFO:Creating metrics dataframe
2024-11-25 12:19:29,304:INFO:Uploading results into container
2024-11-25 12:19:29,305:INFO:Uploading model into container now
2024-11-25 12:19:29,305:INFO:_master_model_container: 6
2024-11-25 12:19:29,305:INFO:_display_container: 2
2024-11-25 12:19:29,306:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:19:29,306:INFO:create_model() successfully completed......................................
2024-11-25 12:19:29,370:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:29,370:INFO:Creating metrics dataframe
2024-11-25 12:19:29,377:INFO:Initializing Random Forest Classifier
2024-11-25 12:19:29,377:INFO:Total runtime is 0.07048808733622233 minutes
2024-11-25 12:19:29,381:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:29,382:INFO:Initializing create_model()
2024-11-25 12:19:29,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:29,382:INFO:Checking exceptions
2024-11-25 12:19:29,382:INFO:Importing libraries
2024-11-25 12:19:29,382:INFO:Copying training dataset
2024-11-25 12:19:29,387:INFO:Defining folds
2024-11-25 12:19:29,387:INFO:Declaring metric variables
2024-11-25 12:19:29,391:INFO:Importing untrained model
2024-11-25 12:19:29,394:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:19:29,402:INFO:Starting cross validation
2024-11-25 12:19:29,404:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:30,619:INFO:Calculating mean and std
2024-11-25 12:19:30,620:INFO:Creating metrics dataframe
2024-11-25 12:19:30,621:INFO:Uploading results into container
2024-11-25 12:19:30,622:INFO:Uploading model into container now
2024-11-25 12:19:30,622:INFO:_master_model_container: 7
2024-11-25 12:19:30,622:INFO:_display_container: 2
2024-11-25 12:19:30,623:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:19:30,623:INFO:create_model() successfully completed......................................
2024-11-25 12:19:30,688:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:30,688:INFO:Creating metrics dataframe
2024-11-25 12:19:30,695:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:19:30,695:INFO:Total runtime is 0.09245495796203614 minutes
2024-11-25 12:19:30,699:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:30,699:INFO:Initializing create_model()
2024-11-25 12:19:30,700:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:30,700:INFO:Checking exceptions
2024-11-25 12:19:30,700:INFO:Importing libraries
2024-11-25 12:19:30,700:INFO:Copying training dataset
2024-11-25 12:19:30,705:INFO:Defining folds
2024-11-25 12:19:30,705:INFO:Declaring metric variables
2024-11-25 12:19:30,710:INFO:Importing untrained model
2024-11-25 12:19:30,713:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:19:30,722:INFO:Starting cross validation
2024-11-25 12:19:30,724:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:30,990:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:30,996:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,006:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,006:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,007:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,010:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,022:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,093:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,098:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,106:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,108:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,111:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,124:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:19:31,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,281:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:31,295:INFO:Calculating mean and std
2024-11-25 12:19:31,296:INFO:Creating metrics dataframe
2024-11-25 12:19:31,298:INFO:Uploading results into container
2024-11-25 12:19:31,299:INFO:Uploading model into container now
2024-11-25 12:19:31,299:INFO:_master_model_container: 8
2024-11-25 12:19:31,299:INFO:_display_container: 2
2024-11-25 12:19:31,299:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:19:31,300:INFO:create_model() successfully completed......................................
2024-11-25 12:19:31,366:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:31,366:INFO:Creating metrics dataframe
2024-11-25 12:19:31,374:INFO:Initializing Ada Boost Classifier
2024-11-25 12:19:31,374:INFO:Total runtime is 0.10377701123555502 minutes
2024-11-25 12:19:31,379:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:31,379:INFO:Initializing create_model()
2024-11-25 12:19:31,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:31,379:INFO:Checking exceptions
2024-11-25 12:19:31,379:INFO:Importing libraries
2024-11-25 12:19:31,380:INFO:Copying training dataset
2024-11-25 12:19:31,384:INFO:Defining folds
2024-11-25 12:19:31,385:INFO:Declaring metric variables
2024-11-25 12:19:31,389:INFO:Importing untrained model
2024-11-25 12:19:31,394:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:19:31,402:INFO:Starting cross validation
2024-11-25 12:19:31,405:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:31,681:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,683:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,695:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,707:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,709:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,711:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,732:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:31,989:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,001:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,006:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,008:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,010:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,041:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,051:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,151:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:32,157:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:19:32,276:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,282:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:32,301:INFO:Calculating mean and std
2024-11-25 12:19:32,302:INFO:Creating metrics dataframe
2024-11-25 12:19:32,304:INFO:Uploading results into container
2024-11-25 12:19:32,304:INFO:Uploading model into container now
2024-11-25 12:19:32,304:INFO:_master_model_container: 9
2024-11-25 12:19:32,304:INFO:_display_container: 2
2024-11-25 12:19:32,304:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:19:32,304:INFO:create_model() successfully completed......................................
2024-11-25 12:19:32,377:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:32,377:INFO:Creating metrics dataframe
2024-11-25 12:19:32,388:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:19:32,388:INFO:Total runtime is 0.12068048318227133 minutes
2024-11-25 12:19:32,393:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:32,394:INFO:Initializing create_model()
2024-11-25 12:19:32,394:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:32,394:INFO:Checking exceptions
2024-11-25 12:19:32,394:INFO:Importing libraries
2024-11-25 12:19:32,394:INFO:Copying training dataset
2024-11-25 12:19:32,398:INFO:Defining folds
2024-11-25 12:19:32,398:INFO:Declaring metric variables
2024-11-25 12:19:32,403:INFO:Importing untrained model
2024-11-25 12:19:32,408:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:19:32,420:INFO:Starting cross validation
2024-11-25 12:19:32,424:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:33,481:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,492:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,506:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,513:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,513:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,522:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,543:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,563:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,983:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:33,988:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,002:INFO:Calculating mean and std
2024-11-25 12:19:34,002:INFO:Creating metrics dataframe
2024-11-25 12:19:34,005:INFO:Uploading results into container
2024-11-25 12:19:34,005:INFO:Uploading model into container now
2024-11-25 12:19:34,005:INFO:_master_model_container: 10
2024-11-25 12:19:34,006:INFO:_display_container: 2
2024-11-25 12:19:34,006:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:19:34,006:INFO:create_model() successfully completed......................................
2024-11-25 12:19:34,071:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:34,071:INFO:Creating metrics dataframe
2024-11-25 12:19:34,079:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:19:34,079:INFO:Total runtime is 0.1488608996073405 minutes
2024-11-25 12:19:34,084:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:34,084:INFO:Initializing create_model()
2024-11-25 12:19:34,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:34,084:INFO:Checking exceptions
2024-11-25 12:19:34,085:INFO:Importing libraries
2024-11-25 12:19:34,085:INFO:Copying training dataset
2024-11-25 12:19:34,090:INFO:Defining folds
2024-11-25 12:19:34,090:INFO:Declaring metric variables
2024-11-25 12:19:34,095:INFO:Importing untrained model
2024-11-25 12:19:34,099:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:19:34,106:INFO:Starting cross validation
2024-11-25 12:19:34,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:34,485:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,489:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,494:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,497:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,497:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,497:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,497:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,505:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,706:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,726:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:19:34,738:INFO:Calculating mean and std
2024-11-25 12:19:34,739:INFO:Creating metrics dataframe
2024-11-25 12:19:34,741:INFO:Uploading results into container
2024-11-25 12:19:34,741:INFO:Uploading model into container now
2024-11-25 12:19:34,742:INFO:_master_model_container: 11
2024-11-25 12:19:34,742:INFO:_display_container: 2
2024-11-25 12:19:34,742:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:19:34,742:INFO:create_model() successfully completed......................................
2024-11-25 12:19:34,824:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:34,825:INFO:Creating metrics dataframe
2024-11-25 12:19:34,836:INFO:Initializing Extra Trees Classifier
2024-11-25 12:19:34,837:INFO:Total runtime is 0.16148770650227864 minutes
2024-11-25 12:19:34,842:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:34,842:INFO:Initializing create_model()
2024-11-25 12:19:34,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:34,842:INFO:Checking exceptions
2024-11-25 12:19:34,842:INFO:Importing libraries
2024-11-25 12:19:34,842:INFO:Copying training dataset
2024-11-25 12:19:34,846:INFO:Defining folds
2024-11-25 12:19:34,846:INFO:Declaring metric variables
2024-11-25 12:19:34,851:INFO:Importing untrained model
2024-11-25 12:19:34,856:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:19:34,870:INFO:Starting cross validation
2024-11-25 12:19:34,876:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:35,997:INFO:Calculating mean and std
2024-11-25 12:19:35,998:INFO:Creating metrics dataframe
2024-11-25 12:19:36,000:INFO:Uploading results into container
2024-11-25 12:19:36,000:INFO:Uploading model into container now
2024-11-25 12:19:36,001:INFO:_master_model_container: 12
2024-11-25 12:19:36,001:INFO:_display_container: 2
2024-11-25 12:19:36,001:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:19:36,002:INFO:create_model() successfully completed......................................
2024-11-25 12:19:36,068:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:36,068:INFO:Creating metrics dataframe
2024-11-25 12:19:36,077:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:19:36,077:INFO:Total runtime is 0.1821589231491089 minutes
2024-11-25 12:19:36,080:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:36,080:INFO:Initializing create_model()
2024-11-25 12:19:36,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:36,080:INFO:Checking exceptions
2024-11-25 12:19:36,080:INFO:Importing libraries
2024-11-25 12:19:36,081:INFO:Copying training dataset
2024-11-25 12:19:36,085:INFO:Defining folds
2024-11-25 12:19:36,085:INFO:Declaring metric variables
2024-11-25 12:19:36,089:INFO:Importing untrained model
2024-11-25 12:19:36,095:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:19:36,104:INFO:Starting cross validation
2024-11-25 12:19:36,108:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:38,143:INFO:Calculating mean and std
2024-11-25 12:19:38,145:INFO:Creating metrics dataframe
2024-11-25 12:19:38,148:INFO:Uploading results into container
2024-11-25 12:19:38,149:INFO:Uploading model into container now
2024-11-25 12:19:38,150:INFO:_master_model_container: 13
2024-11-25 12:19:38,150:INFO:_display_container: 2
2024-11-25 12:19:38,151:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:19:38,151:INFO:create_model() successfully completed......................................
2024-11-25 12:19:38,247:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:38,248:INFO:Creating metrics dataframe
2024-11-25 12:19:38,260:INFO:Initializing Dummy Classifier
2024-11-25 12:19:38,260:INFO:Total runtime is 0.21854057709376018 minutes
2024-11-25 12:19:38,264:INFO:SubProcess create_model() called ==================================
2024-11-25 12:19:38,264:INFO:Initializing create_model()
2024-11-25 12:19:38,265:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001BF6344F250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:38,265:INFO:Checking exceptions
2024-11-25 12:19:38,265:INFO:Importing libraries
2024-11-25 12:19:38,265:INFO:Copying training dataset
2024-11-25 12:19:38,270:INFO:Defining folds
2024-11-25 12:19:38,270:INFO:Declaring metric variables
2024-11-25 12:19:38,275:INFO:Importing untrained model
2024-11-25 12:19:38,279:INFO:Dummy Classifier Imported successfully
2024-11-25 12:19:38,288:INFO:Starting cross validation
2024-11-25 12:19:38,291:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:19:38,683:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,686:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,688:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,690:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,698:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,706:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,708:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,711:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,865:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,866:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:19:38,873:INFO:Calculating mean and std
2024-11-25 12:19:38,874:INFO:Creating metrics dataframe
2024-11-25 12:19:38,876:INFO:Uploading results into container
2024-11-25 12:19:38,877:INFO:Uploading model into container now
2024-11-25 12:19:38,877:INFO:_master_model_container: 14
2024-11-25 12:19:38,877:INFO:_display_container: 2
2024-11-25 12:19:38,877:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:19:38,877:INFO:create_model() successfully completed......................................
2024-11-25 12:19:38,943:INFO:SubProcess create_model() end ==================================
2024-11-25 12:19:38,943:INFO:Creating metrics dataframe
2024-11-25 12:19:38,951:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:19:38,959:INFO:Initializing create_model()
2024-11-25 12:19:38,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF6350FF50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:38,960:INFO:Checking exceptions
2024-11-25 12:19:38,962:INFO:Importing libraries
2024-11-25 12:19:38,962:INFO:Copying training dataset
2024-11-25 12:19:38,967:INFO:Defining folds
2024-11-25 12:19:38,968:INFO:Declaring metric variables
2024-11-25 12:19:38,968:INFO:Importing untrained model
2024-11-25 12:19:38,968:INFO:Declaring custom model
2024-11-25 12:19:38,969:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:19:38,971:INFO:Cross validation set to False
2024-11-25 12:19:38,971:INFO:Fitting Model
2024-11-25 12:19:39,081:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:19:39,082:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000165 seconds.
2024-11-25 12:19:39,082:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-25 12:19:39,083:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:19:39,083:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:19:39,083:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:19:39,084:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:19:39,084:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:19:39,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:19:39,209:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:19:39,209:INFO:create_model() successfully completed......................................
2024-11-25 12:19:39,344:INFO:_master_model_container: 14
2024-11-25 12:19:39,345:INFO:_display_container: 2
2024-11-25 12:19:39,345:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:19:39,345:INFO:compare_models() successfully completed......................................
2024-11-25 12:19:39,350:INFO:Initializing evaluate_model()
2024-11-25 12:19:39,351:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:19:39,363:INFO:Initializing plot_model()
2024-11-25 12:19:39,363:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:19:39,363:INFO:Checking exceptions
2024-11-25 12:19:39,365:INFO:Preloading libraries
2024-11-25 12:19:39,366:INFO:Copying training dataset
2024-11-25 12:19:39,366:INFO:Plot type: pipeline
2024-11-25 12:19:39,506:INFO:Visual Rendered Successfully
2024-11-25 12:19:39,572:INFO:plot_model() successfully completed......................................
2024-11-25 12:19:39,582:INFO:Initializing plot_model()
2024-11-25 12:19:39,582:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:19:39,582:INFO:Checking exceptions
2024-11-25 12:19:39,587:INFO:Preloading libraries
2024-11-25 12:19:39,588:INFO:Copying training dataset
2024-11-25 12:19:39,588:INFO:Plot type: confusion_matrix
2024-11-25 12:19:39,807:INFO:Fitting Model
2024-11-25 12:19:39,808:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:19:39,809:INFO:Scoring test/hold-out set
2024-11-25 12:19:39,927:INFO:Visual Rendered Successfully
2024-11-25 12:19:39,992:INFO:plot_model() successfully completed......................................
2024-11-25 12:19:39,997:INFO:Initializing predict_model()
2024-11-25 12:19:39,997:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001BF6366F560>)
2024-11-25 12:19:39,998:INFO:Checking exceptions
2024-11-25 12:19:39,998:INFO:Preloading libraries
2024-11-25 12:19:40,000:INFO:Set up data.
2024-11-25 12:19:40,007:INFO:Set up index.
2024-11-25 12:19:40,095:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,101:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,105:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,109:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,112:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,115:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

  warnings.warn(traceback.format_exc())

2024-11-25 12:19:40,199:INFO:Initializing finalize_model()
2024-11-25 12:19:40,200:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:19:40,200:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:19:40,204:INFO:Initializing create_model()
2024-11-25 12:19:40,205:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BF63364B10>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:19:40,205:INFO:Checking exceptions
2024-11-25 12:19:40,207:INFO:Importing libraries
2024-11-25 12:19:40,207:INFO:Copying training dataset
2024-11-25 12:19:40,207:INFO:Defining folds
2024-11-25 12:19:40,207:INFO:Declaring metric variables
2024-11-25 12:19:40,207:INFO:Importing untrained model
2024-11-25 12:19:40,207:INFO:Declaring custom model
2024-11-25 12:19:40,208:INFO:Ridge Classifier Imported successfully
2024-11-25 12:19:40,209:INFO:Cross validation set to False
2024-11-25 12:19:40,209:INFO:Fitting Model
2024-11-25 12:19:40,325:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:19:40,325:INFO:create_model() successfully completed......................................
2024-11-25 12:19:40,390:INFO:_master_model_container: 14
2024-11-25 12:19:40,391:INFO:_display_container: 3
2024-11-25 12:19:40,396:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:19:40,396:INFO:finalize_model() successfully completed......................................
2024-11-25 12:19:40,479:INFO:Initializing save_model()
2024-11-25 12:19:40,479:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-25 12:19:40,479:INFO:Adding model into prep_pipe
2024-11-25 12:19:40,494:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-25 12:19:40,502:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:19:40,502:INFO:save_model() successfully completed......................................
2024-11-25 12:22:41,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_07c7a0521fa042eb800d97026dca3fde
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_169ece3b7f4d47798649750b480bf145_59d87e14761944e2b529d4fd87992a39
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_2cbc96bbccdd41d6bba538953a956cf6
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7516171c95754fc891780660a879a34b_a64603e8b4bc410ba912b8d31cde7ebe
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_b1e85f1fff4b4226b8f180d305e5fb39
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_569703add4fd4c93be501c2c681a4887_5736d6aeeb024575b4e9cced368f92f7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_3b3d862ad7a34113a1ee7a677c92b6b3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_b4998654a1e84a4a959b8c17ed899a80_a6a6ead1ddbc40e5afbe29f79dc42c38
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_20e287227055446a8985295a81fcd91a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_a8edc294f549420c96f661244dcdc359_7ae707e99788444bad1caea36e2867da
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_6d4ed599c5914a03be7033ed52024834
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_a3087978d1554f5fb3802a3f5fb02b7d_826e3ff3ffce4ed38e0dbefb0f98773d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_825366488ef147b39ede7042a29933e8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_28eadc00666245ad8eb0960e218c7b2b_b12ff78e85874eafa5e9f1f979cdb982
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_a97b86ed8689426a9ffdd06b0f30dfd2
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_d64a66b763c246b8852d6454d57410a8_c3163a50fe06452ea17d516e18064d37
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_2eb618b49bc849c9a59647f629dac3a5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_5f801c0027a7418c92882758c37cf9a4_2add8863e2854f679732f42349e8efdd
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_c63c0e0a1faf447887d65b11d86dbbeb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_68a85ec4b90049a2acef5a4153188c86_62bf2906be194fec82e434cd234e12ea
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_8601df8f9fe949d3a3368eeee7daa5ae
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,242:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_4adfd992c67f4db7866cefbe555028f1_7850f82b287b46adbb0e5e932d511ccd
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_77fe4d0cd5184f74a2844ddafafebee8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_e1246e90fec64effb79955c56692471a_50083e5313ae4e5189b3bf4e358ca63c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_28e119676e664a778aab64485950bd9a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_8c085469d5b84e87b3cb0dbc336daf96_f84690f593134c58b1570edfa73ce849
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_3a7722713c1f4ed4bfec4db9610ffeac
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_4e2fc24fde964e599653c8dce334acf3_169433ceed334b66b7044bfb31328715
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_1ce04f3079c4429ab39d050b0d077273
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_b69b397dcd154a17b990915f9daa6db2_6bc3fc89d9b949139f5ceb99a86433dd
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,243:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_60066f8a944f436fb2317061210d6798
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_20afba7b8adc4318b8ce6ff9140a1c1b_76c9b4b7b7ab43c193874a21ddc2c2cf
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_0d11447ee6db4a2d9600dd8ee616e095
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_e84a0d7f1f7d4f7fb75cf0949f5f0377_68ec51af4b7046679038d669bea5bbe5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_83604cab21ce4e7ea82d089364852efa
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_4a46aac73ad548878618e2610b8faeaf_0048b44928ed437e9d8dffb8687dd9b7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_f087a5a5e2a2490aabfb239d7cb28225
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_0e78f885cc9140219811d6a206ff2e86_425b35503d514228b9e5c2c973d86c39
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,244:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_f15273d671d04b43875111fc9092ff9a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_68ba3f43691144f1964365f0f160c87f_2c58b213ce9f4f7da88527ebb5ae2214
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_4e8395d2ba8846d3bbd989c3d7a22685
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_8e57865c17ff41918485ab0ea2dccd47_9d1af753556d47c7af351807cf1a5c73
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_b81867ffeb854113a87aebe01e8b3cd9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_77517519ae8b4812973cc4e17b8c0fec_4441a63f6d7d4d35bcd9c7e9ff481191
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_e8182ffb06d24077823162160f7f5fe1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_da3d9be951f74018ab2b4201aa1b3ae4_31227cb821c74339a9eaf03a4e0b5960
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_887b7cd344414501800159693d0473f8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_335bfb9002754092ad0c2c2f1c0f62fd_a640a9861dd940debd6e779b4801b574
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,245:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_ea1b475788034eee9f6c392cd38ab5fa
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_4437bb2c08b645f8b008d90720f3ea2d_8c02fbbffadd472597506259748ce680
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_5eb1f18976564fc08b394f653a9cc2b8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7f556dfddb804adf95001689a3ef61a4_4320fc31e8c74987b3e3a125122a2d2c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_6bbcd5e0d6d94629ba30fb96d18ee863
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_9a16f0970ce14d2eb10ce8a777c762bb_919d4c9f73b84fc3a544483bd39987e4
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_43e7a756f22f43a7932a04ee364798e1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:41,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_7184_7099357856ec465e85b3642c17a4720b_78334bb790dc46db9b36644590ddb019
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:22:51,125:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:22:51,125:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:22:51,125:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:22:51,126:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:22:51,593:INFO:PyCaret ClassificationExperiment
2024-11-25 12:22:51,593:INFO:Logging name: clf-default-name
2024-11-25 12:22:51,593:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:22:51,593:INFO:version 3.3.2
2024-11-25 12:22:51,593:INFO:Initializing setup()
2024-11-25 12:22:51,593:INFO:self.USI: 7c14
2024-11-25 12:22:51,593:INFO:self._variable_keys: {'target_param', 'y_train', 'idx', 'gpu_param', 'n_jobs_param', 'X', 'USI', 'exp_id', 'X_test', 'seed', 'pipeline', 'gpu_n_jobs_param', 'html_param', 'y', '_ml_usecase', 'memory', 'logging_param', '_available_plots', 'data', 'fold_shuffle_param', 'exp_name_log', 'X_train', 'y_test', 'is_multiclass', 'fix_imbalance', 'fold_generator', 'fold_groups_param', 'log_plots_param'}
2024-11-25 12:22:51,593:INFO:Checking environment
2024-11-25 12:22:51,593:INFO:python_version: 3.11.9
2024-11-25 12:22:51,593:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:22:51,593:INFO:machine: AMD64
2024-11-25 12:22:51,593:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:22:51,593:INFO:Memory: svmem(total=25525833728, available=14714875904, percent=42.4, used=10810957824, free=14714875904)
2024-11-25 12:22:51,593:INFO:Physical Core: 4
2024-11-25 12:22:51,593:INFO:Logical Core: 8
2024-11-25 12:22:51,593:INFO:Checking libraries
2024-11-25 12:22:51,593:INFO:System:
2024-11-25 12:22:51,593:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:22:51,593:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:22:51,593:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:22:51,593:INFO:PyCaret required dependencies:
2024-11-25 12:22:51,619:INFO:                 pip: 24.0
2024-11-25 12:22:51,619:INFO:          setuptools: 65.5.0
2024-11-25 12:22:51,619:INFO:             pycaret: 3.3.2
2024-11-25 12:22:51,619:INFO:             IPython: 8.29.0
2024-11-25 12:22:51,619:INFO:          ipywidgets: 8.1.5
2024-11-25 12:22:51,619:INFO:                tqdm: 4.66.6
2024-11-25 12:22:51,619:INFO:               numpy: 1.26.4
2024-11-25 12:22:51,619:INFO:              pandas: 2.1.4
2024-11-25 12:22:51,619:INFO:              jinja2: 3.1.4
2024-11-25 12:22:51,619:INFO:               scipy: 1.11.4
2024-11-25 12:22:51,619:INFO:              joblib: 1.3.2
2024-11-25 12:22:51,619:INFO:             sklearn: 1.4.2
2024-11-25 12:22:51,619:INFO:                pyod: 2.0.2
2024-11-25 12:22:51,619:INFO:            imblearn: 0.12.4
2024-11-25 12:22:51,619:INFO:   category_encoders: 2.6.4
2024-11-25 12:22:51,619:INFO:            lightgbm: 4.5.0
2024-11-25 12:22:51,619:INFO:               numba: 0.60.0
2024-11-25 12:22:51,619:INFO:            requests: 2.32.3
2024-11-25 12:22:51,619:INFO:          matplotlib: 3.7.5
2024-11-25 12:22:51,619:INFO:          scikitplot: 0.3.7
2024-11-25 12:22:51,619:INFO:         yellowbrick: 1.5
2024-11-25 12:22:51,619:INFO:              plotly: 5.24.1
2024-11-25 12:22:51,619:INFO:    plotly-resampler: Not installed
2024-11-25 12:22:51,619:INFO:             kaleido: 0.2.1
2024-11-25 12:22:51,619:INFO:           schemdraw: 0.15
2024-11-25 12:22:51,619:INFO:         statsmodels: 0.14.4
2024-11-25 12:22:51,619:INFO:              sktime: 0.26.0
2024-11-25 12:22:51,619:INFO:               tbats: 1.1.3
2024-11-25 12:22:51,619:INFO:            pmdarima: 2.0.4
2024-11-25 12:22:51,619:INFO:              psutil: 6.1.0
2024-11-25 12:22:51,619:INFO:          markupsafe: 3.0.2
2024-11-25 12:22:51,619:INFO:             pickle5: Not installed
2024-11-25 12:22:51,619:INFO:         cloudpickle: 3.1.0
2024-11-25 12:22:51,619:INFO:         deprecation: 2.1.0
2024-11-25 12:22:51,619:INFO:              xxhash: 3.5.0
2024-11-25 12:22:51,619:INFO:           wurlitzer: 3.1.1
2024-11-25 12:22:51,619:INFO:PyCaret optional dependencies:
2024-11-25 12:22:51,619:INFO:                shap: Not installed
2024-11-25 12:22:51,619:INFO:           interpret: Not installed
2024-11-25 12:22:51,619:INFO:                umap: Not installed
2024-11-25 12:22:51,619:INFO:     ydata_profiling: Not installed
2024-11-25 12:22:51,619:INFO:  explainerdashboard: Not installed
2024-11-25 12:22:51,619:INFO:             autoviz: Not installed
2024-11-25 12:22:51,619:INFO:           fairlearn: Not installed
2024-11-25 12:22:51,619:INFO:          deepchecks: Not installed
2024-11-25 12:22:51,619:INFO:             xgboost: Not installed
2024-11-25 12:22:51,619:INFO:            catboost: Not installed
2024-11-25 12:22:51,619:INFO:              kmodes: Not installed
2024-11-25 12:22:51,619:INFO:             mlxtend: Not installed
2024-11-25 12:22:51,619:INFO:       statsforecast: Not installed
2024-11-25 12:22:51,619:INFO:        tune_sklearn: Not installed
2024-11-25 12:22:51,619:INFO:                 ray: Not installed
2024-11-25 12:22:51,619:INFO:            hyperopt: Not installed
2024-11-25 12:22:51,619:INFO:              optuna: Not installed
2024-11-25 12:22:51,619:INFO:               skopt: Not installed
2024-11-25 12:22:51,619:INFO:              mlflow: Not installed
2024-11-25 12:22:51,619:INFO:              gradio: Not installed
2024-11-25 12:22:51,619:INFO:             fastapi: Not installed
2024-11-25 12:22:51,619:INFO:             uvicorn: Not installed
2024-11-25 12:22:51,619:INFO:              m2cgen: Not installed
2024-11-25 12:22:51,619:INFO:           evidently: Not installed
2024-11-25 12:22:51,619:INFO:               fugue: Not installed
2024-11-25 12:22:51,619:INFO:           streamlit: Not installed
2024-11-25 12:22:51,619:INFO:             prophet: Not installed
2024-11-25 12:22:51,619:INFO:None
2024-11-25 12:22:51,634:INFO:Set up data.
2024-11-25 12:22:51,634:INFO:Set up folding strategy.
2024-11-25 12:22:51,634:INFO:Set up train/test split.
2024-11-25 12:22:51,634:INFO:Set up index.
2024-11-25 12:22:51,634:INFO:Assigning column types.
2024-11-25 12:22:51,634:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:22:51,681:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,681:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,726:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,726:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,768:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,768:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,797:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,797:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:22:51,840:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,902:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:51,933:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,933:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:51,933:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:22:52,003:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,003:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,077:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,077:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,077:INFO:Preparing preprocessing pipeline...
2024-11-25 12:22:52,077:INFO:Set up simple imputation.
2024-11-25 12:22:52,077:INFO:Set up encoding of categorical features.
2024-11-25 12:22:52,077:INFO:Set up column name cleaning.
2024-11-25 12:22:52,213:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:22:52,217:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:22:52,217:INFO:Creating final display dataframe.
2024-11-25 12:22:52,403:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              7c14
2024-11-25 12:22:52,483:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,483:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,551:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,551:INFO:setup() successfully completed in 0.96s...............
2024-11-25 12:22:52,569:INFO:PyCaret ClassificationExperiment
2024-11-25 12:22:52,569:INFO:Logging name: clf-default-name
2024-11-25 12:22:52,569:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:22:52,569:INFO:version 3.3.2
2024-11-25 12:22:52,569:INFO:Initializing setup()
2024-11-25 12:22:52,569:INFO:self.USI: 7f91
2024-11-25 12:22:52,569:INFO:self._variable_keys: {'target_param', 'y_train', 'idx', 'gpu_param', 'n_jobs_param', 'X', 'USI', 'exp_id', 'X_test', 'seed', 'pipeline', 'gpu_n_jobs_param', 'html_param', 'y', '_ml_usecase', 'memory', 'logging_param', '_available_plots', 'data', 'fold_shuffle_param', 'exp_name_log', 'X_train', 'y_test', 'is_multiclass', 'fix_imbalance', 'fold_generator', 'fold_groups_param', 'log_plots_param'}
2024-11-25 12:22:52,569:INFO:Checking environment
2024-11-25 12:22:52,569:INFO:python_version: 3.11.9
2024-11-25 12:22:52,569:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:22:52,569:INFO:machine: AMD64
2024-11-25 12:22:52,569:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:22:52,573:INFO:Memory: svmem(total=25525833728, available=14718758912, percent=42.3, used=10807074816, free=14718758912)
2024-11-25 12:22:52,573:INFO:Physical Core: 4
2024-11-25 12:22:52,573:INFO:Logical Core: 8
2024-11-25 12:22:52,573:INFO:Checking libraries
2024-11-25 12:22:52,573:INFO:System:
2024-11-25 12:22:52,573:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:22:52,573:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:22:52,573:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:22:52,573:INFO:PyCaret required dependencies:
2024-11-25 12:22:52,573:INFO:                 pip: 24.0
2024-11-25 12:22:52,573:INFO:          setuptools: 65.5.0
2024-11-25 12:22:52,573:INFO:             pycaret: 3.3.2
2024-11-25 12:22:52,573:INFO:             IPython: 8.29.0
2024-11-25 12:22:52,573:INFO:          ipywidgets: 8.1.5
2024-11-25 12:22:52,573:INFO:                tqdm: 4.66.6
2024-11-25 12:22:52,573:INFO:               numpy: 1.26.4
2024-11-25 12:22:52,573:INFO:              pandas: 2.1.4
2024-11-25 12:22:52,573:INFO:              jinja2: 3.1.4
2024-11-25 12:22:52,573:INFO:               scipy: 1.11.4
2024-11-25 12:22:52,573:INFO:              joblib: 1.3.2
2024-11-25 12:22:52,573:INFO:             sklearn: 1.4.2
2024-11-25 12:22:52,573:INFO:                pyod: 2.0.2
2024-11-25 12:22:52,573:INFO:            imblearn: 0.12.4
2024-11-25 12:22:52,577:INFO:   category_encoders: 2.6.4
2024-11-25 12:22:52,577:INFO:            lightgbm: 4.5.0
2024-11-25 12:22:52,577:INFO:               numba: 0.60.0
2024-11-25 12:22:52,577:INFO:            requests: 2.32.3
2024-11-25 12:22:52,577:INFO:          matplotlib: 3.7.5
2024-11-25 12:22:52,577:INFO:          scikitplot: 0.3.7
2024-11-25 12:22:52,577:INFO:         yellowbrick: 1.5
2024-11-25 12:22:52,577:INFO:              plotly: 5.24.1
2024-11-25 12:22:52,577:INFO:    plotly-resampler: Not installed
2024-11-25 12:22:52,577:INFO:             kaleido: 0.2.1
2024-11-25 12:22:52,577:INFO:           schemdraw: 0.15
2024-11-25 12:22:52,577:INFO:         statsmodels: 0.14.4
2024-11-25 12:22:52,577:INFO:              sktime: 0.26.0
2024-11-25 12:22:52,577:INFO:               tbats: 1.1.3
2024-11-25 12:22:52,577:INFO:            pmdarima: 2.0.4
2024-11-25 12:22:52,577:INFO:              psutil: 6.1.0
2024-11-25 12:22:52,577:INFO:          markupsafe: 3.0.2
2024-11-25 12:22:52,577:INFO:             pickle5: Not installed
2024-11-25 12:22:52,577:INFO:         cloudpickle: 3.1.0
2024-11-25 12:22:52,577:INFO:         deprecation: 2.1.0
2024-11-25 12:22:52,577:INFO:              xxhash: 3.5.0
2024-11-25 12:22:52,577:INFO:           wurlitzer: 3.1.1
2024-11-25 12:22:52,577:INFO:PyCaret optional dependencies:
2024-11-25 12:22:52,577:INFO:                shap: Not installed
2024-11-25 12:22:52,577:INFO:           interpret: Not installed
2024-11-25 12:22:52,577:INFO:                umap: Not installed
2024-11-25 12:22:52,577:INFO:     ydata_profiling: Not installed
2024-11-25 12:22:52,577:INFO:  explainerdashboard: Not installed
2024-11-25 12:22:52,577:INFO:             autoviz: Not installed
2024-11-25 12:22:52,577:INFO:           fairlearn: Not installed
2024-11-25 12:22:52,577:INFO:          deepchecks: Not installed
2024-11-25 12:22:52,577:INFO:             xgboost: Not installed
2024-11-25 12:22:52,577:INFO:            catboost: Not installed
2024-11-25 12:22:52,577:INFO:              kmodes: Not installed
2024-11-25 12:22:52,577:INFO:             mlxtend: Not installed
2024-11-25 12:22:52,577:INFO:       statsforecast: Not installed
2024-11-25 12:22:52,577:INFO:        tune_sklearn: Not installed
2024-11-25 12:22:52,577:INFO:                 ray: Not installed
2024-11-25 12:22:52,577:INFO:            hyperopt: Not installed
2024-11-25 12:22:52,577:INFO:              optuna: Not installed
2024-11-25 12:22:52,577:INFO:               skopt: Not installed
2024-11-25 12:22:52,577:INFO:              mlflow: Not installed
2024-11-25 12:22:52,577:INFO:              gradio: Not installed
2024-11-25 12:22:52,577:INFO:             fastapi: Not installed
2024-11-25 12:22:52,577:INFO:             uvicorn: Not installed
2024-11-25 12:22:52,577:INFO:              m2cgen: Not installed
2024-11-25 12:22:52,577:INFO:           evidently: Not installed
2024-11-25 12:22:52,577:INFO:               fugue: Not installed
2024-11-25 12:22:52,577:INFO:           streamlit: Not installed
2024-11-25 12:22:52,577:INFO:             prophet: Not installed
2024-11-25 12:22:52,577:INFO:None
2024-11-25 12:22:52,577:INFO:Set up data.
2024-11-25 12:22:52,585:INFO:Set up folding strategy.
2024-11-25 12:22:52,585:INFO:Set up train/test split.
2024-11-25 12:22:52,593:INFO:Set up index.
2024-11-25 12:22:52,593:INFO:Assigning column types.
2024-11-25 12:22:52,593:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:22:52,640:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,640:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,713:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,717:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,745:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,745:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,745:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:22:52,793:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,821:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,821:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,869:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:22:52,893:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,893:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:22:52,960:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:52,960:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,028:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,028:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,032:INFO:Preparing preprocessing pipeline...
2024-11-25 12:22:53,032:INFO:Set up simple imputation.
2024-11-25 12:22:53,032:INFO:Set up encoding of categorical features.
2024-11-25 12:22:53,036:INFO:Set up column name cleaning.
2024-11-25 12:22:53,160:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:22:53,164:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:22:53,164:INFO:Creating final display dataframe.
2024-11-25 12:22:53,369:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              7f91
2024-11-25 12:22:53,457:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,529:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,529:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:22:53,529:INFO:setup() successfully completed in 0.96s...............
2024-11-25 12:22:53,535:INFO:Initializing compare_models()
2024-11-25 12:22:53,535:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:22:53,535:INFO:Checking exceptions
2024-11-25 12:22:53,539:INFO:Preparing display monitor
2024-11-25 12:22:53,567:INFO:Initializing Logistic Regression
2024-11-25 12:22:53,567:INFO:Total runtime is 0.0 minutes
2024-11-25 12:22:53,575:INFO:SubProcess create_model() called ==================================
2024-11-25 12:22:53,575:INFO:Initializing create_model()
2024-11-25 12:22:53,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:22:53,575:INFO:Checking exceptions
2024-11-25 12:22:53,575:INFO:Importing libraries
2024-11-25 12:22:53,575:INFO:Copying training dataset
2024-11-25 12:22:53,583:INFO:Defining folds
2024-11-25 12:22:53,583:INFO:Declaring metric variables
2024-11-25 12:22:53,587:INFO:Importing untrained model
2024-11-25 12:22:53,591:INFO:Logistic Regression Imported successfully
2024-11-25 12:22:53,599:INFO:Starting cross validation
2024-11-25 12:22:53,603:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:22:56,853:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:22:56,857:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:22:56,877:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:22:56,933:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:22:56,945:INFO:Calculating mean and std
2024-11-25 12:22:56,945:INFO:Creating metrics dataframe
2024-11-25 12:22:56,949:INFO:Uploading results into container
2024-11-25 12:22:56,949:INFO:Uploading model into container now
2024-11-25 12:22:56,949:INFO:_master_model_container: 1
2024-11-25 12:22:56,949:INFO:_display_container: 2
2024-11-25 12:22:56,949:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:22:56,953:INFO:create_model() successfully completed......................................
2024-11-25 12:22:57,037:INFO:SubProcess create_model() end ==================================
2024-11-25 12:22:57,037:INFO:Creating metrics dataframe
2024-11-25 12:22:57,041:INFO:Initializing K Neighbors Classifier
2024-11-25 12:22:57,041:INFO:Total runtime is 0.05790736675262451 minutes
2024-11-25 12:22:57,045:INFO:SubProcess create_model() called ==================================
2024-11-25 12:22:57,045:INFO:Initializing create_model()
2024-11-25 12:22:57,049:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:22:57,049:INFO:Checking exceptions
2024-11-25 12:22:57,049:INFO:Importing libraries
2024-11-25 12:22:57,049:INFO:Copying training dataset
2024-11-25 12:22:57,054:INFO:Defining folds
2024-11-25 12:22:57,054:INFO:Declaring metric variables
2024-11-25 12:22:57,060:INFO:Importing untrained model
2024-11-25 12:22:57,068:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:22:57,082:INFO:Starting cross validation
2024-11-25 12:22:57,084:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:22:59,794:INFO:Calculating mean and std
2024-11-25 12:22:59,797:INFO:Creating metrics dataframe
2024-11-25 12:22:59,801:INFO:Uploading results into container
2024-11-25 12:22:59,802:INFO:Uploading model into container now
2024-11-25 12:22:59,803:INFO:_master_model_container: 2
2024-11-25 12:22:59,803:INFO:_display_container: 2
2024-11-25 12:22:59,804:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:22:59,804:INFO:create_model() successfully completed......................................
2024-11-25 12:22:59,890:INFO:SubProcess create_model() end ==================================
2024-11-25 12:22:59,890:INFO:Creating metrics dataframe
2024-11-25 12:22:59,897:INFO:Initializing Naive Bayes
2024-11-25 12:22:59,897:INFO:Total runtime is 0.1055077075958252 minutes
2024-11-25 12:22:59,900:INFO:SubProcess create_model() called ==================================
2024-11-25 12:22:59,901:INFO:Initializing create_model()
2024-11-25 12:22:59,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:22:59,901:INFO:Checking exceptions
2024-11-25 12:22:59,901:INFO:Importing libraries
2024-11-25 12:22:59,901:INFO:Copying training dataset
2024-11-25 12:22:59,907:INFO:Defining folds
2024-11-25 12:22:59,908:INFO:Declaring metric variables
2024-11-25 12:22:59,912:INFO:Importing untrained model
2024-11-25 12:22:59,916:INFO:Naive Bayes Imported successfully
2024-11-25 12:22:59,927:INFO:Starting cross validation
2024-11-25 12:22:59,930:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:00,238:INFO:Calculating mean and std
2024-11-25 12:23:00,239:INFO:Creating metrics dataframe
2024-11-25 12:23:00,242:INFO:Uploading results into container
2024-11-25 12:23:00,242:INFO:Uploading model into container now
2024-11-25 12:23:00,243:INFO:_master_model_container: 3
2024-11-25 12:23:00,243:INFO:_display_container: 2
2024-11-25 12:23:00,244:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:23:00,244:INFO:create_model() successfully completed......................................
2024-11-25 12:23:00,315:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:00,315:INFO:Creating metrics dataframe
2024-11-25 12:23:00,322:INFO:Initializing Decision Tree Classifier
2024-11-25 12:23:00,322:INFO:Total runtime is 0.11258154312769572 minutes
2024-11-25 12:23:00,325:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:00,325:INFO:Initializing create_model()
2024-11-25 12:23:00,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:00,326:INFO:Checking exceptions
2024-11-25 12:23:00,326:INFO:Importing libraries
2024-11-25 12:23:00,326:INFO:Copying training dataset
2024-11-25 12:23:00,332:INFO:Defining folds
2024-11-25 12:23:00,332:INFO:Declaring metric variables
2024-11-25 12:23:00,337:INFO:Importing untrained model
2024-11-25 12:23:00,341:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:23:00,354:INFO:Starting cross validation
2024-11-25 12:23:00,359:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:00,607:INFO:Calculating mean and std
2024-11-25 12:23:00,607:INFO:Creating metrics dataframe
2024-11-25 12:23:00,610:INFO:Uploading results into container
2024-11-25 12:23:00,610:INFO:Uploading model into container now
2024-11-25 12:23:00,610:INFO:_master_model_container: 4
2024-11-25 12:23:00,610:INFO:_display_container: 2
2024-11-25 12:23:00,611:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:23:00,611:INFO:create_model() successfully completed......................................
2024-11-25 12:23:00,677:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:00,677:INFO:Creating metrics dataframe
2024-11-25 12:23:00,689:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:23:00,689:INFO:Total runtime is 0.11870195865631104 minutes
2024-11-25 12:23:00,694:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:00,695:INFO:Initializing create_model()
2024-11-25 12:23:00,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:00,695:INFO:Checking exceptions
2024-11-25 12:23:00,695:INFO:Importing libraries
2024-11-25 12:23:00,695:INFO:Copying training dataset
2024-11-25 12:23:00,700:INFO:Defining folds
2024-11-25 12:23:00,701:INFO:Declaring metric variables
2024-11-25 12:23:00,705:INFO:Importing untrained model
2024-11-25 12:23:00,710:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:23:00,718:INFO:Starting cross validation
2024-11-25 12:23:00,722:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:00,961:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:00,965:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:00,966:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:00,974:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:00,985:INFO:Calculating mean and std
2024-11-25 12:23:00,985:INFO:Creating metrics dataframe
2024-11-25 12:23:00,988:INFO:Uploading results into container
2024-11-25 12:23:00,988:INFO:Uploading model into container now
2024-11-25 12:23:00,989:INFO:_master_model_container: 5
2024-11-25 12:23:00,989:INFO:_display_container: 2
2024-11-25 12:23:00,989:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:23:00,989:INFO:create_model() successfully completed......................................
2024-11-25 12:23:01,058:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:01,059:INFO:Creating metrics dataframe
2024-11-25 12:23:01,065:INFO:Initializing Ridge Classifier
2024-11-25 12:23:01,065:INFO:Total runtime is 0.1249603033065796 minutes
2024-11-25 12:23:01,068:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:01,068:INFO:Initializing create_model()
2024-11-25 12:23:01,068:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:01,069:INFO:Checking exceptions
2024-11-25 12:23:01,069:INFO:Importing libraries
2024-11-25 12:23:01,069:INFO:Copying training dataset
2024-11-25 12:23:01,073:INFO:Defining folds
2024-11-25 12:23:01,073:INFO:Declaring metric variables
2024-11-25 12:23:01,076:INFO:Importing untrained model
2024-11-25 12:23:01,081:INFO:Ridge Classifier Imported successfully
2024-11-25 12:23:01,089:INFO:Starting cross validation
2024-11-25 12:23:01,093:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:01,298:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:01,299:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:01,307:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:01,326:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:01,340:INFO:Calculating mean and std
2024-11-25 12:23:01,341:INFO:Creating metrics dataframe
2024-11-25 12:23:01,343:INFO:Uploading results into container
2024-11-25 12:23:01,344:INFO:Uploading model into container now
2024-11-25 12:23:01,344:INFO:_master_model_container: 6
2024-11-25 12:23:01,345:INFO:_display_container: 2
2024-11-25 12:23:01,345:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:23:01,345:INFO:create_model() successfully completed......................................
2024-11-25 12:23:01,414:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:01,414:INFO:Creating metrics dataframe
2024-11-25 12:23:01,420:INFO:Initializing Random Forest Classifier
2024-11-25 12:23:01,420:INFO:Total runtime is 0.13088569243748985 minutes
2024-11-25 12:23:01,424:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:01,424:INFO:Initializing create_model()
2024-11-25 12:23:01,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:01,425:INFO:Checking exceptions
2024-11-25 12:23:01,425:INFO:Importing libraries
2024-11-25 12:23:01,425:INFO:Copying training dataset
2024-11-25 12:23:01,429:INFO:Defining folds
2024-11-25 12:23:01,429:INFO:Declaring metric variables
2024-11-25 12:23:01,433:INFO:Importing untrained model
2024-11-25 12:23:01,438:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:23:01,448:INFO:Starting cross validation
2024-11-25 12:23:01,451:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:01,964:INFO:Calculating mean and std
2024-11-25 12:23:01,965:INFO:Creating metrics dataframe
2024-11-25 12:23:01,967:INFO:Uploading results into container
2024-11-25 12:23:01,967:INFO:Uploading model into container now
2024-11-25 12:23:01,967:INFO:_master_model_container: 7
2024-11-25 12:23:01,967:INFO:_display_container: 2
2024-11-25 12:23:01,968:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:23:01,968:INFO:create_model() successfully completed......................................
2024-11-25 12:23:02,037:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:02,037:INFO:Creating metrics dataframe
2024-11-25 12:23:02,045:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:23:02,045:INFO:Total runtime is 0.14129592577616376 minutes
2024-11-25 12:23:02,049:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:02,049:INFO:Initializing create_model()
2024-11-25 12:23:02,049:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:02,050:INFO:Checking exceptions
2024-11-25 12:23:02,050:INFO:Importing libraries
2024-11-25 12:23:02,050:INFO:Copying training dataset
2024-11-25 12:23:02,053:INFO:Defining folds
2024-11-25 12:23:02,054:INFO:Declaring metric variables
2024-11-25 12:23:02,058:INFO:Importing untrained model
2024-11-25 12:23:02,062:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:23:02,070:INFO:Starting cross validation
2024-11-25 12:23:02,074:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:02,237:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:02,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:02,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:02,287:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,289:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,292:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,312:INFO:Calculating mean and std
2024-11-25 12:23:02,313:INFO:Creating metrics dataframe
2024-11-25 12:23:02,315:INFO:Uploading results into container
2024-11-25 12:23:02,315:INFO:Uploading model into container now
2024-11-25 12:23:02,316:INFO:_master_model_container: 8
2024-11-25 12:23:02,316:INFO:_display_container: 2
2024-11-25 12:23:02,316:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:23:02,316:INFO:create_model() successfully completed......................................
2024-11-25 12:23:02,383:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:02,383:INFO:Creating metrics dataframe
2024-11-25 12:23:02,396:INFO:Initializing Ada Boost Classifier
2024-11-25 12:23:02,396:INFO:Total runtime is 0.14714817206064862 minutes
2024-11-25 12:23:02,399:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:02,400:INFO:Initializing create_model()
2024-11-25 12:23:02,400:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:02,400:INFO:Checking exceptions
2024-11-25 12:23:02,400:INFO:Importing libraries
2024-11-25 12:23:02,400:INFO:Copying training dataset
2024-11-25 12:23:02,404:INFO:Defining folds
2024-11-25 12:23:02,404:INFO:Declaring metric variables
2024-11-25 12:23:02,409:INFO:Importing untrained model
2024-11-25 12:23:02,414:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:23:02,423:INFO:Starting cross validation
2024-11-25 12:23:02,427:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:02,585:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:02,586:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:02,589:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:02,605:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:02,730:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,734:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,740:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:02,774:INFO:Calculating mean and std
2024-11-25 12:23:02,774:INFO:Creating metrics dataframe
2024-11-25 12:23:02,776:INFO:Uploading results into container
2024-11-25 12:23:02,777:INFO:Uploading model into container now
2024-11-25 12:23:02,777:INFO:_master_model_container: 9
2024-11-25 12:23:02,778:INFO:_display_container: 2
2024-11-25 12:23:02,778:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:23:02,778:INFO:create_model() successfully completed......................................
2024-11-25 12:23:02,853:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:02,853:INFO:Creating metrics dataframe
2024-11-25 12:23:02,861:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:23:02,861:INFO:Total runtime is 0.15490071376164757 minutes
2024-11-25 12:23:02,864:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:02,865:INFO:Initializing create_model()
2024-11-25 12:23:02,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:02,865:INFO:Checking exceptions
2024-11-25 12:23:02,865:INFO:Importing libraries
2024-11-25 12:23:02,865:INFO:Copying training dataset
2024-11-25 12:23:02,869:INFO:Defining folds
2024-11-25 12:23:02,870:INFO:Declaring metric variables
2024-11-25 12:23:02,873:INFO:Importing untrained model
2024-11-25 12:23:02,877:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:23:02,885:INFO:Starting cross validation
2024-11-25 12:23:02,888:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:03,408:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,429:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,433:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,460:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,470:INFO:Calculating mean and std
2024-11-25 12:23:03,470:INFO:Creating metrics dataframe
2024-11-25 12:23:03,473:INFO:Uploading results into container
2024-11-25 12:23:03,473:INFO:Uploading model into container now
2024-11-25 12:23:03,473:INFO:_master_model_container: 10
2024-11-25 12:23:03,473:INFO:_display_container: 2
2024-11-25 12:23:03,474:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:23:03,474:INFO:create_model() successfully completed......................................
2024-11-25 12:23:03,544:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:03,545:INFO:Creating metrics dataframe
2024-11-25 12:23:03,554:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:23:03,554:INFO:Total runtime is 0.16645748615264896 minutes
2024-11-25 12:23:03,559:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:03,559:INFO:Initializing create_model()
2024-11-25 12:23:03,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:03,560:INFO:Checking exceptions
2024-11-25 12:23:03,560:INFO:Importing libraries
2024-11-25 12:23:03,560:INFO:Copying training dataset
2024-11-25 12:23:03,564:INFO:Defining folds
2024-11-25 12:23:03,564:INFO:Declaring metric variables
2024-11-25 12:23:03,567:INFO:Importing untrained model
2024-11-25 12:23:03,572:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:23:03,579:INFO:Starting cross validation
2024-11-25 12:23:03,583:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:03,797:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,808:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,830:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,836:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:03,849:INFO:Calculating mean and std
2024-11-25 12:23:03,850:INFO:Creating metrics dataframe
2024-11-25 12:23:03,852:INFO:Uploading results into container
2024-11-25 12:23:03,853:INFO:Uploading model into container now
2024-11-25 12:23:03,853:INFO:_master_model_container: 11
2024-11-25 12:23:03,853:INFO:_display_container: 2
2024-11-25 12:23:03,854:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:23:03,854:INFO:create_model() successfully completed......................................
2024-11-25 12:23:03,925:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:03,925:INFO:Creating metrics dataframe
2024-11-25 12:23:03,933:INFO:Initializing Extra Trees Classifier
2024-11-25 12:23:03,933:INFO:Total runtime is 0.17276112635930382 minutes
2024-11-25 12:23:03,937:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:03,937:INFO:Initializing create_model()
2024-11-25 12:23:03,938:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:03,938:INFO:Checking exceptions
2024-11-25 12:23:03,938:INFO:Importing libraries
2024-11-25 12:23:03,938:INFO:Copying training dataset
2024-11-25 12:23:03,941:INFO:Defining folds
2024-11-25 12:23:03,942:INFO:Declaring metric variables
2024-11-25 12:23:03,945:INFO:Importing untrained model
2024-11-25 12:23:03,949:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:23:03,957:INFO:Starting cross validation
2024-11-25 12:23:03,961:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:04,393:INFO:Calculating mean and std
2024-11-25 12:23:04,394:INFO:Creating metrics dataframe
2024-11-25 12:23:04,397:INFO:Uploading results into container
2024-11-25 12:23:04,397:INFO:Uploading model into container now
2024-11-25 12:23:04,398:INFO:_master_model_container: 12
2024-11-25 12:23:04,398:INFO:_display_container: 2
2024-11-25 12:23:04,399:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:23:04,399:INFO:create_model() successfully completed......................................
2024-11-25 12:23:04,469:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:04,469:INFO:Creating metrics dataframe
2024-11-25 12:23:04,478:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:23:04,478:INFO:Total runtime is 0.1818552136421204 minutes
2024-11-25 12:23:04,480:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:04,481:INFO:Initializing create_model()
2024-11-25 12:23:04,481:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:04,481:INFO:Checking exceptions
2024-11-25 12:23:04,481:INFO:Importing libraries
2024-11-25 12:23:04,481:INFO:Copying training dataset
2024-11-25 12:23:04,485:INFO:Defining folds
2024-11-25 12:23:04,485:INFO:Declaring metric variables
2024-11-25 12:23:04,489:INFO:Importing untrained model
2024-11-25 12:23:04,493:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:23:04,499:INFO:Starting cross validation
2024-11-25 12:23:04,503:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:05,213:INFO:Calculating mean and std
2024-11-25 12:23:05,215:INFO:Creating metrics dataframe
2024-11-25 12:23:05,217:INFO:Uploading results into container
2024-11-25 12:23:05,217:INFO:Uploading model into container now
2024-11-25 12:23:05,218:INFO:_master_model_container: 13
2024-11-25 12:23:05,218:INFO:_display_container: 2
2024-11-25 12:23:05,219:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:23:05,220:INFO:create_model() successfully completed......................................
2024-11-25 12:23:05,311:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:05,312:INFO:Creating metrics dataframe
2024-11-25 12:23:05,321:INFO:Initializing Dummy Classifier
2024-11-25 12:23:05,321:INFO:Total runtime is 0.19589902559916184 minutes
2024-11-25 12:23:05,323:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:05,324:INFO:Initializing create_model()
2024-11-25 12:23:05,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD04D190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:05,324:INFO:Checking exceptions
2024-11-25 12:23:05,324:INFO:Importing libraries
2024-11-25 12:23:05,324:INFO:Copying training dataset
2024-11-25 12:23:05,328:INFO:Defining folds
2024-11-25 12:23:05,329:INFO:Declaring metric variables
2024-11-25 12:23:05,332:INFO:Importing untrained model
2024-11-25 12:23:05,336:INFO:Dummy Classifier Imported successfully
2024-11-25 12:23:05,345:INFO:Starting cross validation
2024-11-25 12:23:05,348:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:05,555:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:05,558:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:05,560:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:05,561:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:05,572:INFO:Calculating mean and std
2024-11-25 12:23:05,573:INFO:Creating metrics dataframe
2024-11-25 12:23:05,574:INFO:Uploading results into container
2024-11-25 12:23:05,575:INFO:Uploading model into container now
2024-11-25 12:23:05,575:INFO:_master_model_container: 14
2024-11-25 12:23:05,575:INFO:_display_container: 2
2024-11-25 12:23:05,575:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:23:05,576:INFO:create_model() successfully completed......................................
2024-11-25 12:23:05,646:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:05,647:INFO:Creating metrics dataframe
2024-11-25 12:23:05,657:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:23:05,665:INFO:Initializing create_model()
2024-11-25 12:23:05,665:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:05,666:INFO:Checking exceptions
2024-11-25 12:23:05,667:INFO:Importing libraries
2024-11-25 12:23:05,667:INFO:Copying training dataset
2024-11-25 12:23:05,675:INFO:Defining folds
2024-11-25 12:23:05,675:INFO:Declaring metric variables
2024-11-25 12:23:05,676:INFO:Importing untrained model
2024-11-25 12:23:05,676:INFO:Declaring custom model
2024-11-25 12:23:05,677:INFO:Ridge Classifier Imported successfully
2024-11-25 12:23:05,680:INFO:Cross validation set to False
2024-11-25 12:23:05,680:INFO:Fitting Model
2024-11-25 12:23:05,786:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:23:05,787:INFO:create_model() successfully completed......................................
2024-11-25 12:23:05,880:INFO:_master_model_container: 14
2024-11-25 12:23:05,880:INFO:_display_container: 2
2024-11-25 12:23:05,880:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:23:05,880:INFO:compare_models() successfully completed......................................
2024-11-25 12:23:05,888:INFO:Initializing compare_models()
2024-11-25 12:23:05,888:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:23:05,888:INFO:Checking exceptions
2024-11-25 12:23:05,894:INFO:Preparing display monitor
2024-11-25 12:23:05,930:INFO:Initializing Logistic Regression
2024-11-25 12:23:05,930:INFO:Total runtime is 0.0 minutes
2024-11-25 12:23:05,935:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:05,935:INFO:Initializing create_model()
2024-11-25 12:23:05,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:05,936:INFO:Checking exceptions
2024-11-25 12:23:05,936:INFO:Importing libraries
2024-11-25 12:23:05,936:INFO:Copying training dataset
2024-11-25 12:23:05,945:INFO:Defining folds
2024-11-25 12:23:05,945:INFO:Declaring metric variables
2024-11-25 12:23:05,950:INFO:Importing untrained model
2024-11-25 12:23:05,954:INFO:Logistic Regression Imported successfully
2024-11-25 12:23:05,968:INFO:Starting cross validation
2024-11-25 12:23:05,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:06,407:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,414:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,414:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,427:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,473:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,480:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,480:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,484:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,616:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,619:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:06,637:INFO:Calculating mean and std
2024-11-25 12:23:06,637:INFO:Creating metrics dataframe
2024-11-25 12:23:06,639:INFO:Uploading results into container
2024-11-25 12:23:06,639:INFO:Uploading model into container now
2024-11-25 12:23:06,640:INFO:_master_model_container: 1
2024-11-25 12:23:06,640:INFO:_display_container: 2
2024-11-25 12:23:06,640:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:23:06,640:INFO:create_model() successfully completed......................................
2024-11-25 12:23:06,704:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:06,704:INFO:Creating metrics dataframe
2024-11-25 12:23:06,710:INFO:Initializing K Neighbors Classifier
2024-11-25 12:23:06,710:INFO:Total runtime is 0.012998708089192708 minutes
2024-11-25 12:23:06,713:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:06,714:INFO:Initializing create_model()
2024-11-25 12:23:06,714:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:06,714:INFO:Checking exceptions
2024-11-25 12:23:06,714:INFO:Importing libraries
2024-11-25 12:23:06,714:INFO:Copying training dataset
2024-11-25 12:23:06,718:INFO:Defining folds
2024-11-25 12:23:06,718:INFO:Declaring metric variables
2024-11-25 12:23:06,722:INFO:Importing untrained model
2024-11-25 12:23:06,726:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:23:06,734:INFO:Starting cross validation
2024-11-25 12:23:06,740:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:07,386:INFO:Calculating mean and std
2024-11-25 12:23:07,387:INFO:Creating metrics dataframe
2024-11-25 12:23:07,388:INFO:Uploading results into container
2024-11-25 12:23:07,389:INFO:Uploading model into container now
2024-11-25 12:23:07,389:INFO:_master_model_container: 2
2024-11-25 12:23:07,389:INFO:_display_container: 2
2024-11-25 12:23:07,389:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:23:07,390:INFO:create_model() successfully completed......................................
2024-11-25 12:23:07,457:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:07,457:INFO:Creating metrics dataframe
2024-11-25 12:23:07,463:INFO:Initializing Naive Bayes
2024-11-25 12:23:07,463:INFO:Total runtime is 0.025551795959472656 minutes
2024-11-25 12:23:07,468:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:07,469:INFO:Initializing create_model()
2024-11-25 12:23:07,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:07,469:INFO:Checking exceptions
2024-11-25 12:23:07,469:INFO:Importing libraries
2024-11-25 12:23:07,469:INFO:Copying training dataset
2024-11-25 12:23:07,475:INFO:Defining folds
2024-11-25 12:23:07,475:INFO:Declaring metric variables
2024-11-25 12:23:07,479:INFO:Importing untrained model
2024-11-25 12:23:07,483:INFO:Naive Bayes Imported successfully
2024-11-25 12:23:07,493:INFO:Starting cross validation
2024-11-25 12:23:07,497:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:08,120:INFO:Calculating mean and std
2024-11-25 12:23:08,121:INFO:Creating metrics dataframe
2024-11-25 12:23:08,123:INFO:Uploading results into container
2024-11-25 12:23:08,124:INFO:Uploading model into container now
2024-11-25 12:23:08,125:INFO:_master_model_container: 3
2024-11-25 12:23:08,125:INFO:_display_container: 2
2024-11-25 12:23:08,125:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:23:08,125:INFO:create_model() successfully completed......................................
2024-11-25 12:23:08,200:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:08,200:INFO:Creating metrics dataframe
2024-11-25 12:23:08,207:INFO:Initializing Decision Tree Classifier
2024-11-25 12:23:08,208:INFO:Total runtime is 0.037961097558339436 minutes
2024-11-25 12:23:08,212:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:08,212:INFO:Initializing create_model()
2024-11-25 12:23:08,213:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:08,213:INFO:Checking exceptions
2024-11-25 12:23:08,213:INFO:Importing libraries
2024-11-25 12:23:08,213:INFO:Copying training dataset
2024-11-25 12:23:08,218:INFO:Defining folds
2024-11-25 12:23:08,218:INFO:Declaring metric variables
2024-11-25 12:23:08,222:INFO:Importing untrained model
2024-11-25 12:23:08,228:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:23:08,237:INFO:Starting cross validation
2024-11-25 12:23:08,242:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:08,858:INFO:Calculating mean and std
2024-11-25 12:23:08,859:INFO:Creating metrics dataframe
2024-11-25 12:23:08,861:INFO:Uploading results into container
2024-11-25 12:23:08,862:INFO:Uploading model into container now
2024-11-25 12:23:08,862:INFO:_master_model_container: 4
2024-11-25 12:23:08,862:INFO:_display_container: 2
2024-11-25 12:23:08,863:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:23:08,863:INFO:create_model() successfully completed......................................
2024-11-25 12:23:08,945:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:08,945:INFO:Creating metrics dataframe
2024-11-25 12:23:08,953:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:23:08,953:INFO:Total runtime is 0.05038687785466512 minutes
2024-11-25 12:23:08,957:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:08,958:INFO:Initializing create_model()
2024-11-25 12:23:08,958:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:08,958:INFO:Checking exceptions
2024-11-25 12:23:08,958:INFO:Importing libraries
2024-11-25 12:23:08,958:INFO:Copying training dataset
2024-11-25 12:23:08,961:INFO:Defining folds
2024-11-25 12:23:08,961:INFO:Declaring metric variables
2024-11-25 12:23:08,965:INFO:Importing untrained model
2024-11-25 12:23:08,968:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:23:08,977:INFO:Starting cross validation
2024-11-25 12:23:08,981:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:09,389:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,390:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,393:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,395:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,400:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,400:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,422:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,423:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,582:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,582:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:09,600:INFO:Calculating mean and std
2024-11-25 12:23:09,600:INFO:Creating metrics dataframe
2024-11-25 12:23:09,602:INFO:Uploading results into container
2024-11-25 12:23:09,603:INFO:Uploading model into container now
2024-11-25 12:23:09,604:INFO:_master_model_container: 5
2024-11-25 12:23:09,605:INFO:_display_container: 2
2024-11-25 12:23:09,606:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:23:09,606:INFO:create_model() successfully completed......................................
2024-11-25 12:23:09,681:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:09,682:INFO:Creating metrics dataframe
2024-11-25 12:23:09,689:INFO:Initializing Ridge Classifier
2024-11-25 12:23:09,690:INFO:Total runtime is 0.06266522407531738 minutes
2024-11-25 12:23:09,694:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:09,694:INFO:Initializing create_model()
2024-11-25 12:23:09,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:09,694:INFO:Checking exceptions
2024-11-25 12:23:09,695:INFO:Importing libraries
2024-11-25 12:23:09,695:INFO:Copying training dataset
2024-11-25 12:23:09,700:INFO:Defining folds
2024-11-25 12:23:09,700:INFO:Declaring metric variables
2024-11-25 12:23:09,703:INFO:Importing untrained model
2024-11-25 12:23:09,709:INFO:Ridge Classifier Imported successfully
2024-11-25 12:23:09,718:INFO:Starting cross validation
2024-11-25 12:23:09,721:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:10,135:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,143:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,154:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,155:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,162:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,180:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,187:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,350:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,350:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:10,370:INFO:Calculating mean and std
2024-11-25 12:23:10,370:INFO:Creating metrics dataframe
2024-11-25 12:23:10,373:INFO:Uploading results into container
2024-11-25 12:23:10,374:INFO:Uploading model into container now
2024-11-25 12:23:10,374:INFO:_master_model_container: 6
2024-11-25 12:23:10,375:INFO:_display_container: 2
2024-11-25 12:23:10,375:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:23:10,375:INFO:create_model() successfully completed......................................
2024-11-25 12:23:10,450:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:10,451:INFO:Creating metrics dataframe
2024-11-25 12:23:10,459:INFO:Initializing Random Forest Classifier
2024-11-25 12:23:10,459:INFO:Total runtime is 0.07547549406687419 minutes
2024-11-25 12:23:10,464:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:10,464:INFO:Initializing create_model()
2024-11-25 12:23:10,464:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:10,464:INFO:Checking exceptions
2024-11-25 12:23:10,465:INFO:Importing libraries
2024-11-25 12:23:10,465:INFO:Copying training dataset
2024-11-25 12:23:10,470:INFO:Defining folds
2024-11-25 12:23:10,470:INFO:Declaring metric variables
2024-11-25 12:23:10,475:INFO:Importing untrained model
2024-11-25 12:23:10,479:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:23:10,488:INFO:Starting cross validation
2024-11-25 12:23:10,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:11,792:INFO:Calculating mean and std
2024-11-25 12:23:11,792:INFO:Creating metrics dataframe
2024-11-25 12:23:11,794:INFO:Uploading results into container
2024-11-25 12:23:11,795:INFO:Uploading model into container now
2024-11-25 12:23:11,795:INFO:_master_model_container: 7
2024-11-25 12:23:11,796:INFO:_display_container: 2
2024-11-25 12:23:11,796:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:23:11,796:INFO:create_model() successfully completed......................................
2024-11-25 12:23:11,865:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:11,865:INFO:Creating metrics dataframe
2024-11-25 12:23:11,873:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:23:11,873:INFO:Total runtime is 0.0990553061167399 minutes
2024-11-25 12:23:11,876:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:11,876:INFO:Initializing create_model()
2024-11-25 12:23:11,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:11,877:INFO:Checking exceptions
2024-11-25 12:23:11,877:INFO:Importing libraries
2024-11-25 12:23:11,877:INFO:Copying training dataset
2024-11-25 12:23:11,882:INFO:Defining folds
2024-11-25 12:23:11,882:INFO:Declaring metric variables
2024-11-25 12:23:11,884:INFO:Importing untrained model
2024-11-25 12:23:11,889:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:23:11,896:INFO:Starting cross validation
2024-11-25 12:23:11,900:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:12,186:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,189:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,190:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,213:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,224:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,296:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,302:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,312:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,319:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,321:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,323:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,327:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,352:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,447:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,448:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:23:12,487:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,488:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:12,499:INFO:Calculating mean and std
2024-11-25 12:23:12,500:INFO:Creating metrics dataframe
2024-11-25 12:23:12,501:INFO:Uploading results into container
2024-11-25 12:23:12,502:INFO:Uploading model into container now
2024-11-25 12:23:12,502:INFO:_master_model_container: 8
2024-11-25 12:23:12,502:INFO:_display_container: 2
2024-11-25 12:23:12,502:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:23:12,502:INFO:create_model() successfully completed......................................
2024-11-25 12:23:12,569:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:12,570:INFO:Creating metrics dataframe
2024-11-25 12:23:12,581:INFO:Initializing Ada Boost Classifier
2024-11-25 12:23:12,581:INFO:Total runtime is 0.11084696054458618 minutes
2024-11-25 12:23:12,584:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:12,585:INFO:Initializing create_model()
2024-11-25 12:23:12,585:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:12,585:INFO:Checking exceptions
2024-11-25 12:23:12,585:INFO:Importing libraries
2024-11-25 12:23:12,585:INFO:Copying training dataset
2024-11-25 12:23:12,591:INFO:Defining folds
2024-11-25 12:23:12,591:INFO:Declaring metric variables
2024-11-25 12:23:12,595:INFO:Importing untrained model
2024-11-25 12:23:12,599:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:23:12,607:INFO:Starting cross validation
2024-11-25 12:23:12,611:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:12,904:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,908:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,908:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,914:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,914:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,917:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,921:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:12,940:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:13,207:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,220:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,221:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,241:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,257:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,268:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,382:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:13,384:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:23:13,504:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,505:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:13,519:INFO:Calculating mean and std
2024-11-25 12:23:13,520:INFO:Creating metrics dataframe
2024-11-25 12:23:13,522:INFO:Uploading results into container
2024-11-25 12:23:13,522:INFO:Uploading model into container now
2024-11-25 12:23:13,522:INFO:_master_model_container: 9
2024-11-25 12:23:13,522:INFO:_display_container: 2
2024-11-25 12:23:13,523:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:23:13,523:INFO:create_model() successfully completed......................................
2024-11-25 12:23:13,590:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:13,591:INFO:Creating metrics dataframe
2024-11-25 12:23:13,599:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:23:13,600:INFO:Total runtime is 0.1278345505396525 minutes
2024-11-25 12:23:13,606:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:13,607:INFO:Initializing create_model()
2024-11-25 12:23:13,607:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:13,607:INFO:Checking exceptions
2024-11-25 12:23:13,607:INFO:Importing libraries
2024-11-25 12:23:13,607:INFO:Copying training dataset
2024-11-25 12:23:13,611:INFO:Defining folds
2024-11-25 12:23:13,612:INFO:Declaring metric variables
2024-11-25 12:23:13,615:INFO:Importing untrained model
2024-11-25 12:23:13,618:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:23:13,628:INFO:Starting cross validation
2024-11-25 12:23:13,632:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:14,753:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,783:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,794:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,799:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,825:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,834:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,860:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:14,863:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,275:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,281:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,298:INFO:Calculating mean and std
2024-11-25 12:23:15,299:INFO:Creating metrics dataframe
2024-11-25 12:23:15,301:INFO:Uploading results into container
2024-11-25 12:23:15,301:INFO:Uploading model into container now
2024-11-25 12:23:15,301:INFO:_master_model_container: 10
2024-11-25 12:23:15,301:INFO:_display_container: 2
2024-11-25 12:23:15,302:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:23:15,302:INFO:create_model() successfully completed......................................
2024-11-25 12:23:15,373:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:15,373:INFO:Creating metrics dataframe
2024-11-25 12:23:15,383:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:23:15,383:INFO:Total runtime is 0.15754028161366782 minutes
2024-11-25 12:23:15,387:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:15,387:INFO:Initializing create_model()
2024-11-25 12:23:15,388:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:15,388:INFO:Checking exceptions
2024-11-25 12:23:15,388:INFO:Importing libraries
2024-11-25 12:23:15,388:INFO:Copying training dataset
2024-11-25 12:23:15,393:INFO:Defining folds
2024-11-25 12:23:15,393:INFO:Declaring metric variables
2024-11-25 12:23:15,396:INFO:Importing untrained model
2024-11-25 12:23:15,400:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:23:15,417:INFO:Starting cross validation
2024-11-25 12:23:15,424:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:15,830:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,851:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,865:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,876:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,888:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,892:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,913:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:15,922:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:16,047:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:16,057:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:23:16,076:INFO:Calculating mean and std
2024-11-25 12:23:16,076:INFO:Creating metrics dataframe
2024-11-25 12:23:16,078:INFO:Uploading results into container
2024-11-25 12:23:16,079:INFO:Uploading model into container now
2024-11-25 12:23:16,079:INFO:_master_model_container: 11
2024-11-25 12:23:16,079:INFO:_display_container: 2
2024-11-25 12:23:16,079:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:23:16,079:INFO:create_model() successfully completed......................................
2024-11-25 12:23:16,142:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:16,143:INFO:Creating metrics dataframe
2024-11-25 12:23:16,156:INFO:Initializing Extra Trees Classifier
2024-11-25 12:23:16,156:INFO:Total runtime is 0.17042246262232463 minutes
2024-11-25 12:23:16,161:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:16,161:INFO:Initializing create_model()
2024-11-25 12:23:16,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:16,161:INFO:Checking exceptions
2024-11-25 12:23:16,161:INFO:Importing libraries
2024-11-25 12:23:16,161:INFO:Copying training dataset
2024-11-25 12:23:16,167:INFO:Defining folds
2024-11-25 12:23:16,167:INFO:Declaring metric variables
2024-11-25 12:23:16,171:INFO:Importing untrained model
2024-11-25 12:23:16,176:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:23:16,186:INFO:Starting cross validation
2024-11-25 12:23:16,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:17,283:INFO:Calculating mean and std
2024-11-25 12:23:17,284:INFO:Creating metrics dataframe
2024-11-25 12:23:17,286:INFO:Uploading results into container
2024-11-25 12:23:17,286:INFO:Uploading model into container now
2024-11-25 12:23:17,287:INFO:_master_model_container: 12
2024-11-25 12:23:17,287:INFO:_display_container: 2
2024-11-25 12:23:17,287:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:23:17,287:INFO:create_model() successfully completed......................................
2024-11-25 12:23:17,352:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:17,352:INFO:Creating metrics dataframe
2024-11-25 12:23:17,362:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:23:17,362:INFO:Total runtime is 0.19052791198094687 minutes
2024-11-25 12:23:17,366:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:17,367:INFO:Initializing create_model()
2024-11-25 12:23:17,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:17,367:INFO:Checking exceptions
2024-11-25 12:23:17,367:INFO:Importing libraries
2024-11-25 12:23:17,367:INFO:Copying training dataset
2024-11-25 12:23:17,374:INFO:Defining folds
2024-11-25 12:23:17,374:INFO:Declaring metric variables
2024-11-25 12:23:17,379:INFO:Importing untrained model
2024-11-25 12:23:17,383:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:23:17,393:INFO:Starting cross validation
2024-11-25 12:23:17,396:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:19,482:INFO:Calculating mean and std
2024-11-25 12:23:19,484:INFO:Creating metrics dataframe
2024-11-25 12:23:19,487:INFO:Uploading results into container
2024-11-25 12:23:19,488:INFO:Uploading model into container now
2024-11-25 12:23:19,489:INFO:_master_model_container: 13
2024-11-25 12:23:19,489:INFO:_display_container: 2
2024-11-25 12:23:19,490:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:23:19,490:INFO:create_model() successfully completed......................................
2024-11-25 12:23:19,587:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:19,587:INFO:Creating metrics dataframe
2024-11-25 12:23:19,595:INFO:Initializing Dummy Classifier
2024-11-25 12:23:19,596:INFO:Total runtime is 0.22776320775349937 minutes
2024-11-25 12:23:19,599:INFO:SubProcess create_model() called ==================================
2024-11-25 12:23:19,599:INFO:Initializing create_model()
2024-11-25 12:23:19,599:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024DFD318210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:19,599:INFO:Checking exceptions
2024-11-25 12:23:19,599:INFO:Importing libraries
2024-11-25 12:23:19,599:INFO:Copying training dataset
2024-11-25 12:23:19,604:INFO:Defining folds
2024-11-25 12:23:19,604:INFO:Declaring metric variables
2024-11-25 12:23:19,608:INFO:Importing untrained model
2024-11-25 12:23:19,612:INFO:Dummy Classifier Imported successfully
2024-11-25 12:23:19,620:INFO:Starting cross validation
2024-11-25 12:23:19,625:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:23:20,026:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,032:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,036:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,043:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,044:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,047:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,051:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,061:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,210:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,214:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:23:20,223:INFO:Calculating mean and std
2024-11-25 12:23:20,224:INFO:Creating metrics dataframe
2024-11-25 12:23:20,226:INFO:Uploading results into container
2024-11-25 12:23:20,227:INFO:Uploading model into container now
2024-11-25 12:23:20,227:INFO:_master_model_container: 14
2024-11-25 12:23:20,227:INFO:_display_container: 2
2024-11-25 12:23:20,227:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:23:20,227:INFO:create_model() successfully completed......................................
2024-11-25 12:23:20,302:INFO:SubProcess create_model() end ==================================
2024-11-25 12:23:20,302:INFO:Creating metrics dataframe
2024-11-25 12:23:20,316:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:23:20,327:INFO:Initializing create_model()
2024-11-25 12:23:20,327:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCFEFF50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:20,327:INFO:Checking exceptions
2024-11-25 12:23:20,329:INFO:Importing libraries
2024-11-25 12:23:20,329:INFO:Copying training dataset
2024-11-25 12:23:20,333:INFO:Defining folds
2024-11-25 12:23:20,333:INFO:Declaring metric variables
2024-11-25 12:23:20,334:INFO:Importing untrained model
2024-11-25 12:23:20,334:INFO:Declaring custom model
2024-11-25 12:23:20,334:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:23:20,337:INFO:Cross validation set to False
2024-11-25 12:23:20,337:INFO:Fitting Model
2024-11-25 12:23:20,455:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:23:20,456:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000056 seconds.
2024-11-25 12:23:20,456:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-25 12:23:20,456:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-25 12:23:20,457:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:23:20,457:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:23:20,457:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:23:20,458:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:23:20,458:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:23:20,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:23:20,560:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:23:20,560:INFO:create_model() successfully completed......................................
2024-11-25 12:23:20,698:INFO:_master_model_container: 14
2024-11-25 12:23:20,698:INFO:_display_container: 2
2024-11-25 12:23:20,699:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:23:20,699:INFO:compare_models() successfully completed......................................
2024-11-25 12:23:20,705:INFO:Initializing evaluate_model()
2024-11-25 12:23:20,706:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:23:20,718:INFO:Initializing plot_model()
2024-11-25 12:23:20,719:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:23:20,719:INFO:Checking exceptions
2024-11-25 12:23:20,721:INFO:Preloading libraries
2024-11-25 12:23:20,722:INFO:Copying training dataset
2024-11-25 12:23:20,722:INFO:Plot type: pipeline
2024-11-25 12:23:20,893:INFO:Visual Rendered Successfully
2024-11-25 12:23:20,958:INFO:plot_model() successfully completed......................................
2024-11-25 12:23:20,966:INFO:Initializing plot_model()
2024-11-25 12:23:20,966:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:23:20,966:INFO:Checking exceptions
2024-11-25 12:23:20,971:INFO:Preloading libraries
2024-11-25 12:23:20,971:INFO:Copying training dataset
2024-11-25 12:23:20,971:INFO:Plot type: confusion_matrix
2024-11-25 12:23:21,199:INFO:Fitting Model
2024-11-25 12:23:21,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:23:21,200:INFO:Scoring test/hold-out set
2024-11-25 12:23:21,324:INFO:Visual Rendered Successfully
2024-11-25 12:23:21,391:INFO:plot_model() successfully completed......................................
2024-11-25 12:23:21,399:INFO:Initializing predict_model()
2024-11-25 12:23:21,399:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024DFD368F40>)
2024-11-25 12:23:21,399:INFO:Checking exceptions
2024-11-25 12:23:21,399:INFO:Preloading libraries
2024-11-25 12:23:21,402:INFO:Set up data.
2024-11-25 12:23:21,411:INFO:Set up index.
2024-11-25 12:23:21,504:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,509:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,513:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,517:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,520:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,523:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

  warnings.warn(traceback.format_exc())

2024-11-25 12:23:21,607:INFO:Initializing finalize_model()
2024-11-25 12:23:21,607:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:23:21,607:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:23:21,611:INFO:Initializing create_model()
2024-11-25 12:23:21,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:23:21,612:INFO:Checking exceptions
2024-11-25 12:23:21,614:INFO:Importing libraries
2024-11-25 12:23:21,614:INFO:Copying training dataset
2024-11-25 12:23:21,614:INFO:Defining folds
2024-11-25 12:23:21,614:INFO:Declaring metric variables
2024-11-25 12:23:21,615:INFO:Importing untrained model
2024-11-25 12:23:21,615:INFO:Declaring custom model
2024-11-25 12:23:21,615:INFO:Ridge Classifier Imported successfully
2024-11-25 12:23:21,618:INFO:Cross validation set to False
2024-11-25 12:23:21,618:INFO:Fitting Model
2024-11-25 12:23:21,721:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:23:21,721:INFO:create_model() successfully completed......................................
2024-11-25 12:23:21,787:INFO:_master_model_container: 14
2024-11-25 12:23:21,787:INFO:_display_container: 3
2024-11-25 12:23:21,792:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:23:21,792:INFO:finalize_model() successfully completed......................................
2024-11-25 12:23:21,866:INFO:Initializing save_model()
2024-11-25 12:23:21,866:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-25 12:23:21,867:INFO:Adding model into prep_pipe
2024-11-25 12:23:21,881:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-25 12:23:21,887:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:23:21,887:INFO:save_model() successfully completed......................................
2024-11-25 12:24:07,599:INFO:Initializing predict_model()
2024-11-25 12:24:07,599:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024DE9440EA0>)
2024-11-25 12:24:07,599:INFO:Checking exceptions
2024-11-25 12:24:07,599:INFO:Preloading libraries
2024-11-25 12:24:07,601:INFO:Set up data.
2024-11-25 12:24:07,609:INFO:Set up index.
2024-11-25 12:24:07,710:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:24:07,714:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:24:07,718:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:24:07,721:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:24:07,723:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:24:07,725:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

  warnings.warn(traceback.format_exc())

2024-11-25 12:27:20,344:INFO:Initializing plot_model()
2024-11-25 12:27:20,344:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=gain, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:20,344:INFO:Checking exceptions
2024-11-25 12:27:20,347:INFO:Preloading libraries
2024-11-25 12:27:20,348:INFO:Copying training dataset
2024-11-25 12:27:20,348:INFO:Plot type: gain
2024-11-25 12:27:20,348:INFO:Generating predictions / predict_proba on X_test
2024-11-25 12:27:22,179:INFO:Initializing plot_model()
2024-11-25 12:27:22,349:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=tree, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:22,349:INFO:Checking exceptions
2024-11-25 12:27:24,760:INFO:Initializing plot_model()
2024-11-25 12:27:24,760:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=lift, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:24,760:INFO:Checking exceptions
2024-11-25 12:27:24,762:INFO:Preloading libraries
2024-11-25 12:27:24,762:INFO:Copying training dataset
2024-11-25 12:27:24,762:INFO:Plot type: lift
2024-11-25 12:27:24,762:INFO:Generating predictions / predict_proba on X_test
2024-11-25 12:27:26,117:INFO:Initializing plot_model()
2024-11-25 12:27:26,117:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:26,117:INFO:Checking exceptions
2024-11-25 12:27:26,119:INFO:Preloading libraries
2024-11-25 12:27:26,119:INFO:Copying training dataset
2024-11-25 12:27:26,119:INFO:Plot type: boundary
2024-11-25 12:27:26,214:INFO:Fitting StandardScaler()
2024-11-25 12:27:26,218:INFO:Fitting PCA()
2024-11-25 12:27:26,312:INFO:Fitting Model
2024-11-25 12:27:27,248:INFO:Visual Rendered Successfully
2024-11-25 12:27:27,382:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:27,402:INFO:Initializing plot_model()
2024-11-25 12:27:27,402:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:27,402:INFO:Checking exceptions
2024-11-25 12:27:27,405:INFO:Preloading libraries
2024-11-25 12:27:27,405:INFO:Copying training dataset
2024-11-25 12:27:27,405:INFO:Plot type: pipeline
2024-11-25 12:27:27,525:INFO:Visual Rendered Successfully
2024-11-25 12:27:27,597:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:29,761:INFO:Initializing plot_model()
2024-11-25 12:27:29,761:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:29,765:INFO:Checking exceptions
2024-11-25 12:27:29,765:INFO:Preloading libraries
2024-11-25 12:27:29,765:INFO:Copying training dataset
2024-11-25 12:27:29,765:INFO:Plot type: parameter
2024-11-25 12:27:29,769:INFO:Visual Rendered Successfully
2024-11-25 12:27:29,841:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:31,253:INFO:Initializing plot_model()
2024-11-25 12:27:31,253:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:31,253:INFO:Checking exceptions
2024-11-25 12:27:32,489:INFO:Initializing plot_model()
2024-11-25 12:27:32,489:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:32,489:INFO:Checking exceptions
2024-11-25 12:27:32,496:INFO:Preloading libraries
2024-11-25 12:27:32,496:INFO:Copying training dataset
2024-11-25 12:27:32,496:INFO:Plot type: confusion_matrix
2024-11-25 12:27:32,692:INFO:Fitting Model
2024-11-25 12:27:32,692:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:27:32,692:INFO:Scoring test/hold-out set
2024-11-25 12:27:32,793:INFO:Visual Rendered Successfully
2024-11-25 12:27:32,876:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:36,806:INFO:Initializing plot_model()
2024-11-25 12:27:36,807:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=threshold, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:36,807:INFO:Checking exceptions
2024-11-25 12:27:37,532:INFO:Initializing plot_model()
2024-11-25 12:27:37,532:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:37,533:INFO:Checking exceptions
2024-11-25 12:27:37,534:INFO:Preloading libraries
2024-11-25 12:27:37,534:INFO:Copying training dataset
2024-11-25 12:27:37,534:INFO:Plot type: pr
2024-11-25 12:27:37,753:INFO:Fitting Model
2024-11-25 12:27:37,762:INFO:Scoring test/hold-out set
2024-11-25 12:27:37,946:INFO:Visual Rendered Successfully
2024-11-25 12:27:38,030:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:42,235:INFO:Initializing plot_model()
2024-11-25 12:27:42,235:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=error, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:42,235:INFO:Checking exceptions
2024-11-25 12:27:42,237:INFO:Preloading libraries
2024-11-25 12:27:42,238:INFO:Copying training dataset
2024-11-25 12:27:42,238:INFO:Plot type: error
2024-11-25 12:27:42,432:INFO:Fitting Model
2024-11-25 12:27:42,432:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:27:42,432:INFO:Scoring test/hold-out set
2024-11-25 12:27:42,598:INFO:Visual Rendered Successfully
2024-11-25 12:27:42,684:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:45,490:INFO:Initializing plot_model()
2024-11-25 12:27:45,491:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=class_report, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:45,491:INFO:Checking exceptions
2024-11-25 12:27:45,493:INFO:Preloading libraries
2024-11-25 12:27:45,493:INFO:Copying training dataset
2024-11-25 12:27:45,493:INFO:Plot type: class_report
2024-11-25 12:27:45,679:INFO:Fitting Model
2024-11-25 12:27:45,679:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:27:45,679:INFO:Scoring test/hold-out set
2024-11-25 12:27:45,848:INFO:Visual Rendered Successfully
2024-11-25 12:27:45,918:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:57,258:INFO:Initializing plot_model()
2024-11-25 12:27:57,259:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:57,259:INFO:Checking exceptions
2024-11-25 12:27:58,368:INFO:Initializing plot_model()
2024-11-25 12:27:58,368:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:58,368:INFO:Checking exceptions
2024-11-25 12:27:58,370:INFO:Preloading libraries
2024-11-25 12:27:58,370:INFO:Copying training dataset
2024-11-25 12:27:58,370:INFO:Plot type: learning
2024-11-25 12:27:58,558:INFO:Fitting Model
2024-11-25 12:27:58,814:INFO:Visual Rendered Successfully
2024-11-25 12:27:58,894:INFO:plot_model() successfully completed......................................
2024-11-25 12:27:59,972:INFO:Initializing plot_model()
2024-11-25 12:27:59,973:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=manifold, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:27:59,973:INFO:Checking exceptions
2024-11-25 12:28:00,970:INFO:Initializing plot_model()
2024-11-25 12:28:00,970:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:00,970:INFO:Checking exceptions
2024-11-25 12:28:02,680:INFO:Initializing plot_model()
2024-11-25 12:28:02,680:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=vc, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:02,680:INFO:Checking exceptions
2024-11-25 12:28:02,683:INFO:Preloading libraries
2024-11-25 12:28:02,684:INFO:Copying training dataset
2024-11-25 12:28:02,684:INFO:Plot type: vc
2024-11-25 12:28:02,684:INFO:Determining param_name
2024-11-25 12:28:02,684:INFO:param_name: alpha
2024-11-25 12:28:02,877:INFO:Fitting Model
2024-11-25 12:28:03,158:INFO:Visual Rendered Successfully
2024-11-25 12:28:03,250:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:03,820:INFO:Initializing plot_model()
2024-11-25 12:28:03,820:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=dimension, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:03,821:INFO:Checking exceptions
2024-11-25 12:28:03,824:INFO:Preloading libraries
2024-11-25 12:28:03,824:INFO:Copying training dataset
2024-11-25 12:28:03,824:INFO:Plot type: dimension
2024-11-25 12:28:03,860:INFO:Fitting StandardScaler()
2024-11-25 12:28:03,879:INFO:Fitting PCA()
2024-11-25 12:28:04,081:INFO:Fitting & Transforming Model
2024-11-25 12:28:04,232:INFO:Visual Rendered Successfully
2024-11-25 12:28:04,319:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:05,253:INFO:Initializing plot_model()
2024-11-25 12:28:05,254:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:05,254:INFO:Checking exceptions
2024-11-25 12:28:05,255:INFO:Preloading libraries
2024-11-25 12:28:05,255:INFO:Copying training dataset
2024-11-25 12:28:05,255:INFO:Plot type: feature
2024-11-25 12:28:05,436:INFO:Visual Rendered Successfully
2024-11-25 12:28:05,508:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:06,196:INFO:Initializing plot_model()
2024-11-25 12:28:06,196:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=dimension, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:06,196:INFO:Checking exceptions
2024-11-25 12:28:06,198:INFO:Preloading libraries
2024-11-25 12:28:06,198:INFO:Copying training dataset
2024-11-25 12:28:06,198:INFO:Plot type: dimension
2024-11-25 12:28:06,224:INFO:Fitting StandardScaler()
2024-11-25 12:28:06,247:INFO:Fitting PCA()
2024-11-25 12:28:06,439:INFO:Fitting & Transforming Model
2024-11-25 12:28:06,560:INFO:Visual Rendered Successfully
2024-11-25 12:28:06,640:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:07,916:INFO:Initializing plot_model()
2024-11-25 12:28:07,916:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:07,917:INFO:Checking exceptions
2024-11-25 12:28:07,919:INFO:Preloading libraries
2024-11-25 12:28:07,919:INFO:Copying training dataset
2024-11-25 12:28:07,919:INFO:Plot type: feature
2024-11-25 12:28:08,119:INFO:Visual Rendered Successfully
2024-11-25 12:28:08,205:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:08,643:INFO:Initializing plot_model()
2024-11-25 12:28:08,645:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:08,645:INFO:Checking exceptions
2024-11-25 12:28:08,646:INFO:Preloading libraries
2024-11-25 12:28:08,646:INFO:Copying training dataset
2024-11-25 12:28:08,646:INFO:Plot type: feature_all
2024-11-25 12:28:08,984:INFO:Visual Rendered Successfully
2024-11-25 12:28:09,056:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:11,861:INFO:Initializing plot_model()
2024-11-25 12:28:11,862:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:11,862:INFO:Checking exceptions
2024-11-25 12:28:11,864:INFO:Preloading libraries
2024-11-25 12:28:11,865:INFO:Copying training dataset
2024-11-25 12:28:11,865:INFO:Plot type: feature
2024-11-25 12:28:12,045:INFO:Visual Rendered Successfully
2024-11-25 12:28:12,119:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:13,840:INFO:Initializing plot_model()
2024-11-25 12:28:13,841:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:13,841:INFO:Checking exceptions
2024-11-25 12:28:13,843:INFO:Preloading libraries
2024-11-25 12:28:13,843:INFO:Copying training dataset
2024-11-25 12:28:13,843:INFO:Plot type: feature_all
2024-11-25 12:28:14,192:INFO:Visual Rendered Successfully
2024-11-25 12:28:14,277:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:18,499:INFO:Initializing plot_model()
2024-11-25 12:28:18,500:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:18,500:INFO:Checking exceptions
2024-11-25 12:28:18,501:INFO:Preloading libraries
2024-11-25 12:28:18,501:INFO:Copying training dataset
2024-11-25 12:28:18,501:INFO:Plot type: boundary
2024-11-25 12:28:18,588:INFO:Fitting StandardScaler()
2024-11-25 12:28:18,591:INFO:Fitting PCA()
2024-11-25 12:28:18,682:INFO:Fitting Model
2024-11-25 12:28:19,484:INFO:Visual Rendered Successfully
2024-11-25 12:28:19,610:INFO:plot_model() successfully completed......................................
2024-11-25 12:28:20,876:INFO:Initializing plot_model()
2024-11-25 12:28:20,876:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=lift, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:20,877:INFO:Checking exceptions
2024-11-25 12:28:20,878:INFO:Preloading libraries
2024-11-25 12:28:20,879:INFO:Copying training dataset
2024-11-25 12:28:20,879:INFO:Plot type: lift
2024-11-25 12:28:20,879:INFO:Generating predictions / predict_proba on X_test
2024-11-25 12:28:21,971:INFO:Initializing plot_model()
2024-11-25 12:28:21,971:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=gain, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:21,971:INFO:Checking exceptions
2024-11-25 12:28:21,973:INFO:Preloading libraries
2024-11-25 12:28:21,973:INFO:Copying training dataset
2024-11-25 12:28:21,973:INFO:Plot type: gain
2024-11-25 12:28:21,974:INFO:Generating predictions / predict_proba on X_test
2024-11-25 12:28:23,007:INFO:Initializing plot_model()
2024-11-25 12:28:23,007:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=tree, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:23,007:INFO:Checking exceptions
2024-11-25 12:28:24,334:INFO:Initializing plot_model()
2024-11-25 12:28:24,334:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024DFCE44410>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=ks, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:28:24,335:INFO:Checking exceptions
2024-11-25 12:28:24,338:INFO:Preloading libraries
2024-11-25 12:28:24,338:INFO:Copying training dataset
2024-11-25 12:28:24,338:INFO:Plot type: ks
2024-11-25 12:28:24,338:INFO:Generating predictions / predict_proba on X_test
2024-11-25 12:29:30,753:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_c9a10a764df045f6a88f348c4b9e4d8e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,754:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_644545506362499fa6c9bf3e642fc2d0_26bb65e475a34edd9b60bc36de359da3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,757:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_fc7361304571448f80051ffb9bf28e8b
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,757:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_2af60ca6c77b43af9dcb73a788ace5b3_11550976fca241fc8355f4b7178bbd51
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,757:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_f3b2bcea81504abe802622cc787700b4
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_0e50bf840fee47e8994d735a198754e7_8a113d715ebc423fa108e3decbde3a25
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_e5a3aa8cc93a4647805f4ed318af5e7f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_4e871b8404544317b49c4c65776e20ec_ca5f4a4d408742d2ac75fa4f05d5a65c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_88a83f7724744d34a79faf7f040abe36
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_8574c194118d4c978bf8df24b1db668a_a5a30cc914d14b2f92576973e812f9f7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_c855e7c03a1c461d8e2a16de22db02e3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_7f6f1d7c9d5f4458ab19bd36eb629899_43888f08734a43838ef36a957b2118b5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_f5432f75e11541a2adaefc7accd50fd6
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_31403fdb5d754ffc8403da666b726e82_8763842769324017923aac8bb5f2adf9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_fa823bf7f2e74817b4eec80a9c703937
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_3f35aafd15344176a4f5cca909cc83c0_5ae56c2d55724abd9dec2a8230ea9b50
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_6d0d6d38a9ac44d88ea8c96a52da7a62
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_4f2ac353255a4a6eacd2ea250ac35397_3d2167091d6e46bcaebc1d0d0735520e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_b1922cee331044c89f2d4bdb9de63138
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_b25c104e96a44b96a47ffb6d3277cdb2_3bf661f1a4844edeab06f62800423432
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_ef1ccee75f4543deb0933f9a81d46753
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_4440b61fe31e467c821ec2c152d2ce20_28e5881601004feeb8d9439cc10028bb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_fa3eebfc08f74214812f387c26082df6
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_448b698bd1a54fcebc6d9f0cf6a3353b_9cc72180715e4fb3b66c456f586ef2ef
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,759:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_c759d77ed38046959175d18f682264c7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_1c0fab012e414657b34a76444877b125_ba827089e2b445a7ad94a7b44a79d21d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_b79d35ac212f45fd83d51f8c2ab6625f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_354eb52248d546cebd015b19b82267e5_d72991e48a604f588d2c22d789da40d9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_25932a3c3d2e4a629ca0d4a0d3ee9439
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_6cc36fb9752c4e5e9fffbef5fd883d73_e72d6397bba2445b990a4e02763d65b9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_6d62a326c00f4d59aeed4028f54a9e5a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_295bcdd86a614c7dbfca26017923c7dc_e731c8950934473399ef94822ec605ae
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_949ba4567cf0443aaae0d4d030acc671
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_20b4ff93254045c986be3a3342e1a4ba_4212e7d9abdd40f4b3433de169e7e20f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,760:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_7a008e2427de45f78ddbae3af3aa8f66
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_f9651ac13ecf41cabda62dba1ee41c12_5e4d149e309e4d99afc2d63d66fdae13
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_ead7cc3697644345a33e440d2cfebc74
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_f7f24be630fd49e68d33e7c42b4761b2_339f10e5c95649af86c27bba8351727d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_3fd86cdd28784bcbb176fffa88eddb4b
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_d08a208b29ee4255a800b8ddb0aee3b2_1db35a8b711a4c4db7827b661d5800ea
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_2fde9b620fed478f8f4790ba8cb5f2bb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_718079976b644ee28a8b319dd26c0311_1dc6c394aad34a62ab22cfbdc699d277
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_c0210b0c727741a2b9a7137a9f2f5268
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_8c6153162f2f47f188a3adbd39be2029_0afc23cf8c0444a7b4f6e3059ca0dec3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_3f4ae2a795d54a4f883fc8ff7557d433
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_268c9d2fa1e84a97a7ef3ff23adaa9be_aa03406149a6493798f364332b366f3c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_873ab9ad70a046fc88fd6b66a6a6fe7d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_119e8202a25d4501bef603d25c49733a_279e92fbbb2c4199a3b0442e28a270ad
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_c6d53d7c0f074d6b9dd28f93f4a022d5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_a8f18be88d6b49a48e59f460c1ac33be_41b59ae4af7c4defb641609f43a7f967
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_a286ee774d014b25b1bf358b306953bc
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_f9cda723442944bda2aec6bcf8a66596_15d1d3e4120242398b705742732bded3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_085fa9a5168b4603b23536e2cea0d0c1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,762:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_51348063ad364660a4445ff0220c7b27_5c3c6d2b94084be3b340677636f74830
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_18c8ee406a1f4f809b0ea1414d72bb71
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_7357ba208e8648578e49e9eb194530c2_9bce4d853a00496789cebfa21f93ed66
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_9c140ea36e694ce894baad516b09da21
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_154cbfec64ad4e49a0993e46344be0ed_0e8993e43ec946d6b9304ec9ef033560
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_b0f3f4224380424bb31564ece145346c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:30,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9636_5fc30ead89704d0ab8e5137cf8216276_44ce4a29c10849e6b006b50855ddf252
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:29:40,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:29:40,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:29:40,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:29:40,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:29:40,815:INFO:PyCaret ClassificationExperiment
2024-11-25 12:29:40,815:INFO:Logging name: clf-default-name
2024-11-25 12:29:40,815:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:29:40,815:INFO:version 3.3.2
2024-11-25 12:29:40,815:INFO:Initializing setup()
2024-11-25 12:29:40,815:INFO:self.USI: a4c5
2024-11-25 12:29:40,815:INFO:self._variable_keys: {'seed', 'fold_shuffle_param', 'fold_groups_param', 'exp_id', 'exp_name_log', '_available_plots', 'n_jobs_param', 'data', 'fix_imbalance', 'y', 'logging_param', 'USI', 'idx', 'gpu_param', 'y_train', 'pipeline', 'is_multiclass', 'html_param', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', '_ml_usecase', 'y_test', 'X_test', 'fold_generator', 'X', 'X_train', 'memory'}
2024-11-25 12:29:40,815:INFO:Checking environment
2024-11-25 12:29:40,815:INFO:python_version: 3.11.9
2024-11-25 12:29:40,816:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:29:40,816:INFO:machine: AMD64
2024-11-25 12:29:40,816:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:29:40,819:INFO:Memory: svmem(total=25525833728, available=14629699584, percent=42.7, used=10896134144, free=14629699584)
2024-11-25 12:29:40,819:INFO:Physical Core: 4
2024-11-25 12:29:40,819:INFO:Logical Core: 8
2024-11-25 12:29:40,820:INFO:Checking libraries
2024-11-25 12:29:40,820:INFO:System:
2024-11-25 12:29:40,820:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:29:40,820:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:29:40,820:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:29:40,820:INFO:PyCaret required dependencies:
2024-11-25 12:29:40,840:INFO:                 pip: 24.0
2024-11-25 12:29:40,841:INFO:          setuptools: 65.5.0
2024-11-25 12:29:40,841:INFO:             pycaret: 3.3.2
2024-11-25 12:29:40,841:INFO:             IPython: 8.29.0
2024-11-25 12:29:40,841:INFO:          ipywidgets: 8.1.5
2024-11-25 12:29:40,841:INFO:                tqdm: 4.66.6
2024-11-25 12:29:40,841:INFO:               numpy: 1.26.4
2024-11-25 12:29:40,841:INFO:              pandas: 2.1.4
2024-11-25 12:29:40,841:INFO:              jinja2: 3.1.4
2024-11-25 12:29:40,841:INFO:               scipy: 1.11.4
2024-11-25 12:29:40,841:INFO:              joblib: 1.3.2
2024-11-25 12:29:40,841:INFO:             sklearn: 1.4.2
2024-11-25 12:29:40,841:INFO:                pyod: 2.0.2
2024-11-25 12:29:40,841:INFO:            imblearn: 0.12.4
2024-11-25 12:29:40,841:INFO:   category_encoders: 2.6.4
2024-11-25 12:29:40,841:INFO:            lightgbm: 4.5.0
2024-11-25 12:29:40,841:INFO:               numba: 0.60.0
2024-11-25 12:29:40,841:INFO:            requests: 2.32.3
2024-11-25 12:29:40,841:INFO:          matplotlib: 3.7.5
2024-11-25 12:29:40,841:INFO:          scikitplot: 0.3.7
2024-11-25 12:29:40,841:INFO:         yellowbrick: 1.5
2024-11-25 12:29:40,841:INFO:              plotly: 5.24.1
2024-11-25 12:29:40,841:INFO:    plotly-resampler: Not installed
2024-11-25 12:29:40,842:INFO:             kaleido: 0.2.1
2024-11-25 12:29:40,842:INFO:           schemdraw: 0.15
2024-11-25 12:29:40,842:INFO:         statsmodels: 0.14.4
2024-11-25 12:29:40,842:INFO:              sktime: 0.26.0
2024-11-25 12:29:40,842:INFO:               tbats: 1.1.3
2024-11-25 12:29:40,842:INFO:            pmdarima: 2.0.4
2024-11-25 12:29:40,842:INFO:              psutil: 6.1.0
2024-11-25 12:29:40,842:INFO:          markupsafe: 3.0.2
2024-11-25 12:29:40,842:INFO:             pickle5: Not installed
2024-11-25 12:29:40,842:INFO:         cloudpickle: 3.1.0
2024-11-25 12:29:40,842:INFO:         deprecation: 2.1.0
2024-11-25 12:29:40,842:INFO:              xxhash: 3.5.0
2024-11-25 12:29:40,842:INFO:           wurlitzer: 3.1.1
2024-11-25 12:29:40,842:INFO:PyCaret optional dependencies:
2024-11-25 12:29:40,860:INFO:                shap: Not installed
2024-11-25 12:29:40,860:INFO:           interpret: Not installed
2024-11-25 12:29:40,860:INFO:                umap: Not installed
2024-11-25 12:29:40,860:INFO:     ydata_profiling: Not installed
2024-11-25 12:29:40,860:INFO:  explainerdashboard: Not installed
2024-11-25 12:29:40,860:INFO:             autoviz: Not installed
2024-11-25 12:29:40,860:INFO:           fairlearn: Not installed
2024-11-25 12:29:40,860:INFO:          deepchecks: Not installed
2024-11-25 12:29:40,860:INFO:             xgboost: Not installed
2024-11-25 12:29:40,860:INFO:            catboost: Not installed
2024-11-25 12:29:40,860:INFO:              kmodes: Not installed
2024-11-25 12:29:40,860:INFO:             mlxtend: Not installed
2024-11-25 12:29:40,860:INFO:       statsforecast: Not installed
2024-11-25 12:29:40,860:INFO:        tune_sklearn: Not installed
2024-11-25 12:29:40,860:INFO:                 ray: Not installed
2024-11-25 12:29:40,860:INFO:            hyperopt: Not installed
2024-11-25 12:29:40,860:INFO:              optuna: Not installed
2024-11-25 12:29:40,860:INFO:               skopt: Not installed
2024-11-25 12:29:40,860:INFO:              mlflow: Not installed
2024-11-25 12:29:40,860:INFO:              gradio: Not installed
2024-11-25 12:29:40,860:INFO:             fastapi: Not installed
2024-11-25 12:29:40,860:INFO:             uvicorn: Not installed
2024-11-25 12:29:40,860:INFO:              m2cgen: Not installed
2024-11-25 12:29:40,860:INFO:           evidently: Not installed
2024-11-25 12:29:40,860:INFO:               fugue: Not installed
2024-11-25 12:29:40,860:INFO:           streamlit: Not installed
2024-11-25 12:29:40,860:INFO:             prophet: Not installed
2024-11-25 12:29:40,860:INFO:None
2024-11-25 12:29:40,860:INFO:Set up data.
2024-11-25 12:29:40,868:INFO:Set up folding strategy.
2024-11-25 12:29:40,868:INFO:Set up train/test split.
2024-11-25 12:29:40,875:INFO:Set up index.
2024-11-25 12:29:40,876:INFO:Assigning column types.
2024-11-25 12:29:40,879:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:29:40,923:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:29:40,927:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:40,959:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:40,959:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,001:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,002:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,028:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,029:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,029:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:29:41,071:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,097:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,097:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,142:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,169:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,169:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,169:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:29:41,238:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,238:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,307:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,309:INFO:Preparing preprocessing pipeline...
2024-11-25 12:29:41,310:INFO:Set up simple imputation.
2024-11-25 12:29:41,312:INFO:Set up encoding of categorical features.
2024-11-25 12:29:41,313:INFO:Set up column name cleaning.
2024-11-25 12:29:41,459:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:29:41,465:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:29:41,465:INFO:Creating final display dataframe.
2024-11-25 12:29:41,689:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              a4c5
2024-11-25 12:29:41,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,840:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,842:INFO:setup() successfully completed in 1.03s...............
2024-11-25 12:29:41,856:INFO:PyCaret ClassificationExperiment
2024-11-25 12:29:41,856:INFO:Logging name: clf-default-name
2024-11-25 12:29:41,856:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:29:41,856:INFO:version 3.3.2
2024-11-25 12:29:41,856:INFO:Initializing setup()
2024-11-25 12:29:41,856:INFO:self.USI: 07a6
2024-11-25 12:29:41,856:INFO:self._variable_keys: {'seed', 'fold_shuffle_param', 'fold_groups_param', 'exp_id', 'exp_name_log', '_available_plots', 'n_jobs_param', 'data', 'fix_imbalance', 'y', 'logging_param', 'USI', 'idx', 'gpu_param', 'y_train', 'pipeline', 'is_multiclass', 'html_param', 'gpu_n_jobs_param', 'log_plots_param', 'target_param', '_ml_usecase', 'y_test', 'X_test', 'fold_generator', 'X', 'X_train', 'memory'}
2024-11-25 12:29:41,856:INFO:Checking environment
2024-11-25 12:29:41,856:INFO:python_version: 3.11.9
2024-11-25 12:29:41,856:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:29:41,856:INFO:machine: AMD64
2024-11-25 12:29:41,856:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:29:41,859:INFO:Memory: svmem(total=25525833728, available=14627344384, percent=42.7, used=10898489344, free=14627344384)
2024-11-25 12:29:41,859:INFO:Physical Core: 4
2024-11-25 12:29:41,860:INFO:Logical Core: 8
2024-11-25 12:29:41,860:INFO:Checking libraries
2024-11-25 12:29:41,860:INFO:System:
2024-11-25 12:29:41,860:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:29:41,860:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:29:41,860:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:29:41,860:INFO:PyCaret required dependencies:
2024-11-25 12:29:41,860:INFO:                 pip: 24.0
2024-11-25 12:29:41,860:INFO:          setuptools: 65.5.0
2024-11-25 12:29:41,860:INFO:             pycaret: 3.3.2
2024-11-25 12:29:41,860:INFO:             IPython: 8.29.0
2024-11-25 12:29:41,860:INFO:          ipywidgets: 8.1.5
2024-11-25 12:29:41,860:INFO:                tqdm: 4.66.6
2024-11-25 12:29:41,860:INFO:               numpy: 1.26.4
2024-11-25 12:29:41,860:INFO:              pandas: 2.1.4
2024-11-25 12:29:41,860:INFO:              jinja2: 3.1.4
2024-11-25 12:29:41,860:INFO:               scipy: 1.11.4
2024-11-25 12:29:41,860:INFO:              joblib: 1.3.2
2024-11-25 12:29:41,860:INFO:             sklearn: 1.4.2
2024-11-25 12:29:41,860:INFO:                pyod: 2.0.2
2024-11-25 12:29:41,860:INFO:            imblearn: 0.12.4
2024-11-25 12:29:41,861:INFO:   category_encoders: 2.6.4
2024-11-25 12:29:41,861:INFO:            lightgbm: 4.5.0
2024-11-25 12:29:41,861:INFO:               numba: 0.60.0
2024-11-25 12:29:41,861:INFO:            requests: 2.32.3
2024-11-25 12:29:41,861:INFO:          matplotlib: 3.7.5
2024-11-25 12:29:41,861:INFO:          scikitplot: 0.3.7
2024-11-25 12:29:41,861:INFO:         yellowbrick: 1.5
2024-11-25 12:29:41,861:INFO:              plotly: 5.24.1
2024-11-25 12:29:41,861:INFO:    plotly-resampler: Not installed
2024-11-25 12:29:41,861:INFO:             kaleido: 0.2.1
2024-11-25 12:29:41,861:INFO:           schemdraw: 0.15
2024-11-25 12:29:41,861:INFO:         statsmodels: 0.14.4
2024-11-25 12:29:41,861:INFO:              sktime: 0.26.0
2024-11-25 12:29:41,861:INFO:               tbats: 1.1.3
2024-11-25 12:29:41,861:INFO:            pmdarima: 2.0.4
2024-11-25 12:29:41,861:INFO:              psutil: 6.1.0
2024-11-25 12:29:41,861:INFO:          markupsafe: 3.0.2
2024-11-25 12:29:41,861:INFO:             pickle5: Not installed
2024-11-25 12:29:41,861:INFO:         cloudpickle: 3.1.0
2024-11-25 12:29:41,861:INFO:         deprecation: 2.1.0
2024-11-25 12:29:41,861:INFO:              xxhash: 3.5.0
2024-11-25 12:29:41,861:INFO:           wurlitzer: 3.1.1
2024-11-25 12:29:41,861:INFO:PyCaret optional dependencies:
2024-11-25 12:29:41,861:INFO:                shap: Not installed
2024-11-25 12:29:41,861:INFO:           interpret: Not installed
2024-11-25 12:29:41,861:INFO:                umap: Not installed
2024-11-25 12:29:41,861:INFO:     ydata_profiling: Not installed
2024-11-25 12:29:41,861:INFO:  explainerdashboard: Not installed
2024-11-25 12:29:41,861:INFO:             autoviz: Not installed
2024-11-25 12:29:41,861:INFO:           fairlearn: Not installed
2024-11-25 12:29:41,861:INFO:          deepchecks: Not installed
2024-11-25 12:29:41,862:INFO:             xgboost: Not installed
2024-11-25 12:29:41,862:INFO:            catboost: Not installed
2024-11-25 12:29:41,862:INFO:              kmodes: Not installed
2024-11-25 12:29:41,862:INFO:             mlxtend: Not installed
2024-11-25 12:29:41,862:INFO:       statsforecast: Not installed
2024-11-25 12:29:41,862:INFO:        tune_sklearn: Not installed
2024-11-25 12:29:41,862:INFO:                 ray: Not installed
2024-11-25 12:29:41,862:INFO:            hyperopt: Not installed
2024-11-25 12:29:41,862:INFO:              optuna: Not installed
2024-11-25 12:29:41,862:INFO:               skopt: Not installed
2024-11-25 12:29:41,862:INFO:              mlflow: Not installed
2024-11-25 12:29:41,862:INFO:              gradio: Not installed
2024-11-25 12:29:41,862:INFO:             fastapi: Not installed
2024-11-25 12:29:41,862:INFO:             uvicorn: Not installed
2024-11-25 12:29:41,862:INFO:              m2cgen: Not installed
2024-11-25 12:29:41,862:INFO:           evidently: Not installed
2024-11-25 12:29:41,862:INFO:               fugue: Not installed
2024-11-25 12:29:41,862:INFO:           streamlit: Not installed
2024-11-25 12:29:41,862:INFO:             prophet: Not installed
2024-11-25 12:29:41,862:INFO:None
2024-11-25 12:29:41,862:INFO:Set up data.
2024-11-25 12:29:41,867:INFO:Set up folding strategy.
2024-11-25 12:29:41,867:INFO:Set up train/test split.
2024-11-25 12:29:41,873:INFO:Set up index.
2024-11-25 12:29:41,873:INFO:Assigning column types.
2024-11-25 12:29:41,875:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:29:41,919:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,920:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,946:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,946:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:41,990:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:29:41,991:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:42,017:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,017:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,017:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:29:42,060:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:42,085:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,128:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:29:42,154:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,154:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,155:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:29:42,223:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,223:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,292:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,293:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,294:INFO:Preparing preprocessing pipeline...
2024-11-25 12:29:42,294:INFO:Set up simple imputation.
2024-11-25 12:29:42,296:INFO:Set up encoding of categorical features.
2024-11-25 12:29:42,297:INFO:Set up column name cleaning.
2024-11-25 12:29:42,433:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:29:42,440:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:29:42,440:INFO:Creating final display dataframe.
2024-11-25 12:29:42,672:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              07a6
2024-11-25 12:29:42,753:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,753:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,828:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,828:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:29:42,829:INFO:setup() successfully completed in 0.98s...............
2024-11-25 12:29:42,837:INFO:Initializing compare_models()
2024-11-25 12:29:42,837:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:29:42,837:INFO:Checking exceptions
2024-11-25 12:29:42,843:INFO:Preparing display monitor
2024-11-25 12:29:42,874:INFO:Initializing Logistic Regression
2024-11-25 12:29:42,874:INFO:Total runtime is 0.0 minutes
2024-11-25 12:29:42,879:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:42,879:INFO:Initializing create_model()
2024-11-25 12:29:42,880:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:42,880:INFO:Checking exceptions
2024-11-25 12:29:42,880:INFO:Importing libraries
2024-11-25 12:29:42,880:INFO:Copying training dataset
2024-11-25 12:29:42,884:INFO:Defining folds
2024-11-25 12:29:42,884:INFO:Declaring metric variables
2024-11-25 12:29:42,891:INFO:Importing untrained model
2024-11-25 12:29:42,896:INFO:Logistic Regression Imported successfully
2024-11-25 12:29:42,906:INFO:Starting cross validation
2024-11-25 12:29:42,910:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:46,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:46,209:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:46,213:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:46,269:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:46,289:INFO:Calculating mean and std
2024-11-25 12:29:46,289:INFO:Creating metrics dataframe
2024-11-25 12:29:46,293:INFO:Uploading results into container
2024-11-25 12:29:46,293:INFO:Uploading model into container now
2024-11-25 12:29:46,293:INFO:_master_model_container: 1
2024-11-25 12:29:46,293:INFO:_display_container: 2
2024-11-25 12:29:46,293:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:29:46,293:INFO:create_model() successfully completed......................................
2024-11-25 12:29:46,373:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:46,373:INFO:Creating metrics dataframe
2024-11-25 12:29:46,377:INFO:Initializing K Neighbors Classifier
2024-11-25 12:29:46,377:INFO:Total runtime is 0.05837840636571248 minutes
2024-11-25 12:29:46,381:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:46,381:INFO:Initializing create_model()
2024-11-25 12:29:46,381:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:46,381:INFO:Checking exceptions
2024-11-25 12:29:46,381:INFO:Importing libraries
2024-11-25 12:29:46,381:INFO:Copying training dataset
2024-11-25 12:29:46,389:INFO:Defining folds
2024-11-25 12:29:46,389:INFO:Declaring metric variables
2024-11-25 12:29:46,393:INFO:Importing untrained model
2024-11-25 12:29:46,397:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:29:46,409:INFO:Starting cross validation
2024-11-25 12:29:46,409:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:49,277:INFO:Calculating mean and std
2024-11-25 12:29:49,277:INFO:Creating metrics dataframe
2024-11-25 12:29:49,281:INFO:Uploading results into container
2024-11-25 12:29:49,281:INFO:Uploading model into container now
2024-11-25 12:29:49,281:INFO:_master_model_container: 2
2024-11-25 12:29:49,281:INFO:_display_container: 2
2024-11-25 12:29:49,281:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:29:49,281:INFO:create_model() successfully completed......................................
2024-11-25 12:29:49,369:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:49,369:INFO:Creating metrics dataframe
2024-11-25 12:29:49,373:INFO:Initializing Naive Bayes
2024-11-25 12:29:49,373:INFO:Total runtime is 0.10831234455108643 minutes
2024-11-25 12:29:49,377:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:49,377:INFO:Initializing create_model()
2024-11-25 12:29:49,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:49,377:INFO:Checking exceptions
2024-11-25 12:29:49,377:INFO:Importing libraries
2024-11-25 12:29:49,377:INFO:Copying training dataset
2024-11-25 12:29:49,381:INFO:Defining folds
2024-11-25 12:29:49,381:INFO:Declaring metric variables
2024-11-25 12:29:49,389:INFO:Importing untrained model
2024-11-25 12:29:49,393:INFO:Naive Bayes Imported successfully
2024-11-25 12:29:49,397:INFO:Starting cross validation
2024-11-25 12:29:49,405:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:49,655:INFO:Calculating mean and std
2024-11-25 12:29:49,655:INFO:Creating metrics dataframe
2024-11-25 12:29:49,655:INFO:Uploading results into container
2024-11-25 12:29:49,655:INFO:Uploading model into container now
2024-11-25 12:29:49,659:INFO:_master_model_container: 3
2024-11-25 12:29:49,659:INFO:_display_container: 2
2024-11-25 12:29:49,659:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:29:49,659:INFO:create_model() successfully completed......................................
2024-11-25 12:29:49,723:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:49,723:INFO:Creating metrics dataframe
2024-11-25 12:29:49,727:INFO:Initializing Decision Tree Classifier
2024-11-25 12:29:49,727:INFO:Total runtime is 0.11422122716903688 minutes
2024-11-25 12:29:49,731:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:49,731:INFO:Initializing create_model()
2024-11-25 12:29:49,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:49,731:INFO:Checking exceptions
2024-11-25 12:29:49,735:INFO:Importing libraries
2024-11-25 12:29:49,735:INFO:Copying training dataset
2024-11-25 12:29:49,735:INFO:Defining folds
2024-11-25 12:29:49,735:INFO:Declaring metric variables
2024-11-25 12:29:49,739:INFO:Importing untrained model
2024-11-25 12:29:49,743:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:29:49,751:INFO:Starting cross validation
2024-11-25 12:29:49,755:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:49,991:INFO:Calculating mean and std
2024-11-25 12:29:49,991:INFO:Creating metrics dataframe
2024-11-25 12:29:49,991:INFO:Uploading results into container
2024-11-25 12:29:49,995:INFO:Uploading model into container now
2024-11-25 12:29:49,995:INFO:_master_model_container: 4
2024-11-25 12:29:49,995:INFO:_display_container: 2
2024-11-25 12:29:49,995:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:29:49,995:INFO:create_model() successfully completed......................................
2024-11-25 12:29:50,069:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:50,069:INFO:Creating metrics dataframe
2024-11-25 12:29:50,077:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:29:50,077:INFO:Total runtime is 0.12004097302754721 minutes
2024-11-25 12:29:50,081:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:50,081:INFO:Initializing create_model()
2024-11-25 12:29:50,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:50,081:INFO:Checking exceptions
2024-11-25 12:29:50,081:INFO:Importing libraries
2024-11-25 12:29:50,081:INFO:Copying training dataset
2024-11-25 12:29:50,085:INFO:Defining folds
2024-11-25 12:29:50,085:INFO:Declaring metric variables
2024-11-25 12:29:50,089:INFO:Importing untrained model
2024-11-25 12:29:50,093:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:29:50,101:INFO:Starting cross validation
2024-11-25 12:29:50,105:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:50,326:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,334:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,334:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,334:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,354:INFO:Calculating mean and std
2024-11-25 12:29:50,354:INFO:Creating metrics dataframe
2024-11-25 12:29:50,354:INFO:Uploading results into container
2024-11-25 12:29:50,354:INFO:Uploading model into container now
2024-11-25 12:29:50,354:INFO:_master_model_container: 5
2024-11-25 12:29:50,354:INFO:_display_container: 2
2024-11-25 12:29:50,358:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:29:50,358:INFO:create_model() successfully completed......................................
2024-11-25 12:29:50,426:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:50,430:INFO:Creating metrics dataframe
2024-11-25 12:29:50,438:INFO:Initializing Ridge Classifier
2024-11-25 12:29:50,438:INFO:Total runtime is 0.12606825828552248 minutes
2024-11-25 12:29:50,442:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:50,442:INFO:Initializing create_model()
2024-11-25 12:29:50,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:50,442:INFO:Checking exceptions
2024-11-25 12:29:50,442:INFO:Importing libraries
2024-11-25 12:29:50,442:INFO:Copying training dataset
2024-11-25 12:29:50,446:INFO:Defining folds
2024-11-25 12:29:50,446:INFO:Declaring metric variables
2024-11-25 12:29:50,450:INFO:Importing untrained model
2024-11-25 12:29:50,454:INFO:Ridge Classifier Imported successfully
2024-11-25 12:29:50,462:INFO:Starting cross validation
2024-11-25 12:29:50,466:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:50,657:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,667:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,693:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,708:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:50,728:INFO:Calculating mean and std
2024-11-25 12:29:50,728:INFO:Creating metrics dataframe
2024-11-25 12:29:50,728:INFO:Uploading results into container
2024-11-25 12:29:50,732:INFO:Uploading model into container now
2024-11-25 12:29:50,732:INFO:_master_model_container: 6
2024-11-25 12:29:50,732:INFO:_display_container: 2
2024-11-25 12:29:50,732:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:29:50,732:INFO:create_model() successfully completed......................................
2024-11-25 12:29:50,800:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:50,800:INFO:Creating metrics dataframe
2024-11-25 12:29:50,808:INFO:Initializing Random Forest Classifier
2024-11-25 12:29:50,808:INFO:Total runtime is 0.13223145008087162 minutes
2024-11-25 12:29:50,812:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:50,812:INFO:Initializing create_model()
2024-11-25 12:29:50,812:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:50,812:INFO:Checking exceptions
2024-11-25 12:29:50,812:INFO:Importing libraries
2024-11-25 12:29:50,812:INFO:Copying training dataset
2024-11-25 12:29:50,820:INFO:Defining folds
2024-11-25 12:29:50,820:INFO:Declaring metric variables
2024-11-25 12:29:50,824:INFO:Importing untrained model
2024-11-25 12:29:50,824:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:29:50,832:INFO:Starting cross validation
2024-11-25 12:29:50,836:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:51,327:INFO:Calculating mean and std
2024-11-25 12:29:51,327:INFO:Creating metrics dataframe
2024-11-25 12:29:51,327:INFO:Uploading results into container
2024-11-25 12:29:51,327:INFO:Uploading model into container now
2024-11-25 12:29:51,331:INFO:_master_model_container: 7
2024-11-25 12:29:51,331:INFO:_display_container: 2
2024-11-25 12:29:51,331:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:29:51,331:INFO:create_model() successfully completed......................................
2024-11-25 12:29:51,399:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:51,399:INFO:Creating metrics dataframe
2024-11-25 12:29:51,407:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:29:51,407:INFO:Total runtime is 0.1422135710716248 minutes
2024-11-25 12:29:51,407:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:51,411:INFO:Initializing create_model()
2024-11-25 12:29:51,411:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:51,411:INFO:Checking exceptions
2024-11-25 12:29:51,411:INFO:Importing libraries
2024-11-25 12:29:51,411:INFO:Copying training dataset
2024-11-25 12:29:51,415:INFO:Defining folds
2024-11-25 12:29:51,415:INFO:Declaring metric variables
2024-11-25 12:29:51,419:INFO:Importing untrained model
2024-11-25 12:29:51,423:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:29:51,431:INFO:Starting cross validation
2024-11-25 12:29:51,435:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:51,582:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:29:51,586:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:29:51,598:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:29:51,614:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:29:51,634:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:51,642:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:51,646:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:51,660:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:51,678:INFO:Calculating mean and std
2024-11-25 12:29:51,678:INFO:Creating metrics dataframe
2024-11-25 12:29:51,680:INFO:Uploading results into container
2024-11-25 12:29:51,680:INFO:Uploading model into container now
2024-11-25 12:29:51,680:INFO:_master_model_container: 8
2024-11-25 12:29:51,680:INFO:_display_container: 2
2024-11-25 12:29:51,680:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:29:51,680:INFO:create_model() successfully completed......................................
2024-11-25 12:29:51,752:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:51,752:INFO:Creating metrics dataframe
2024-11-25 12:29:51,760:INFO:Initializing Ada Boost Classifier
2024-11-25 12:29:51,760:INFO:Total runtime is 0.14809989134470627 minutes
2024-11-25 12:29:51,764:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:51,764:INFO:Initializing create_model()
2024-11-25 12:29:51,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:51,764:INFO:Checking exceptions
2024-11-25 12:29:51,764:INFO:Importing libraries
2024-11-25 12:29:51,764:INFO:Copying training dataset
2024-11-25 12:29:51,772:INFO:Defining folds
2024-11-25 12:29:51,772:INFO:Declaring metric variables
2024-11-25 12:29:51,772:INFO:Importing untrained model
2024-11-25 12:29:51,776:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:29:51,784:INFO:Starting cross validation
2024-11-25 12:29:51,792:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:51,944:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:29:51,948:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:29:51,950:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:29:51,954:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:29:52,081:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,110:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,118:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,124:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,136:INFO:Calculating mean and std
2024-11-25 12:29:52,136:INFO:Creating metrics dataframe
2024-11-25 12:29:52,136:INFO:Uploading results into container
2024-11-25 12:29:52,140:INFO:Uploading model into container now
2024-11-25 12:29:52,140:INFO:_master_model_container: 9
2024-11-25 12:29:52,140:INFO:_display_container: 2
2024-11-25 12:29:52,140:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:29:52,140:INFO:create_model() successfully completed......................................
2024-11-25 12:29:52,208:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:52,208:INFO:Creating metrics dataframe
2024-11-25 12:29:52,219:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:29:52,220:INFO:Total runtime is 0.15575640996297208 minutes
2024-11-25 12:29:52,223:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:52,224:INFO:Initializing create_model()
2024-11-25 12:29:52,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:52,224:INFO:Checking exceptions
2024-11-25 12:29:52,224:INFO:Importing libraries
2024-11-25 12:29:52,224:INFO:Copying training dataset
2024-11-25 12:29:52,229:INFO:Defining folds
2024-11-25 12:29:52,230:INFO:Declaring metric variables
2024-11-25 12:29:52,234:INFO:Importing untrained model
2024-11-25 12:29:52,241:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:29:52,253:INFO:Starting cross validation
2024-11-25 12:29:52,258:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:52,841:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,855:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,892:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,914:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:52,934:INFO:Calculating mean and std
2024-11-25 12:29:52,934:INFO:Creating metrics dataframe
2024-11-25 12:29:52,937:INFO:Uploading results into container
2024-11-25 12:29:52,937:INFO:Uploading model into container now
2024-11-25 12:29:52,937:INFO:_master_model_container: 10
2024-11-25 12:29:52,937:INFO:_display_container: 2
2024-11-25 12:29:52,937:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:29:52,937:INFO:create_model() successfully completed......................................
2024-11-25 12:29:53,003:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:53,003:INFO:Creating metrics dataframe
2024-11-25 12:29:53,009:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:29:53,009:INFO:Total runtime is 0.16891880035400397 minutes
2024-11-25 12:29:53,016:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:53,016:INFO:Initializing create_model()
2024-11-25 12:29:53,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:53,016:INFO:Checking exceptions
2024-11-25 12:29:53,016:INFO:Importing libraries
2024-11-25 12:29:53,016:INFO:Copying training dataset
2024-11-25 12:29:53,020:INFO:Defining folds
2024-11-25 12:29:53,020:INFO:Declaring metric variables
2024-11-25 12:29:53,024:INFO:Importing untrained model
2024-11-25 12:29:53,028:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:29:53,040:INFO:Starting cross validation
2024-11-25 12:29:53,044:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:53,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:53,252:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:53,264:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:53,266:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:53,290:INFO:Calculating mean and std
2024-11-25 12:29:53,290:INFO:Creating metrics dataframe
2024-11-25 12:29:53,290:INFO:Uploading results into container
2024-11-25 12:29:53,290:INFO:Uploading model into container now
2024-11-25 12:29:53,294:INFO:_master_model_container: 11
2024-11-25 12:29:53,294:INFO:_display_container: 2
2024-11-25 12:29:53,294:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:29:53,294:INFO:create_model() successfully completed......................................
2024-11-25 12:29:53,358:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:53,362:INFO:Creating metrics dataframe
2024-11-25 12:29:53,370:INFO:Initializing Extra Trees Classifier
2024-11-25 12:29:53,370:INFO:Total runtime is 0.1749337553977967 minutes
2024-11-25 12:29:53,370:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:53,374:INFO:Initializing create_model()
2024-11-25 12:29:53,374:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:53,374:INFO:Checking exceptions
2024-11-25 12:29:53,374:INFO:Importing libraries
2024-11-25 12:29:53,374:INFO:Copying training dataset
2024-11-25 12:29:53,374:INFO:Defining folds
2024-11-25 12:29:53,374:INFO:Declaring metric variables
2024-11-25 12:29:53,378:INFO:Importing untrained model
2024-11-25 12:29:53,382:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:29:53,394:INFO:Starting cross validation
2024-11-25 12:29:53,394:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:53,901:INFO:Calculating mean and std
2024-11-25 12:29:53,902:INFO:Creating metrics dataframe
2024-11-25 12:29:53,904:INFO:Uploading results into container
2024-11-25 12:29:53,905:INFO:Uploading model into container now
2024-11-25 12:29:53,905:INFO:_master_model_container: 12
2024-11-25 12:29:53,905:INFO:_display_container: 2
2024-11-25 12:29:53,906:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:29:53,906:INFO:create_model() successfully completed......................................
2024-11-25 12:29:53,974:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:53,974:INFO:Creating metrics dataframe
2024-11-25 12:29:53,982:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:29:53,982:INFO:Total runtime is 0.18512853781382252 minutes
2024-11-25 12:29:53,985:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:53,986:INFO:Initializing create_model()
2024-11-25 12:29:53,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:53,986:INFO:Checking exceptions
2024-11-25 12:29:53,986:INFO:Importing libraries
2024-11-25 12:29:53,987:INFO:Copying training dataset
2024-11-25 12:29:53,990:INFO:Defining folds
2024-11-25 12:29:53,990:INFO:Declaring metric variables
2024-11-25 12:29:53,994:INFO:Importing untrained model
2024-11-25 12:29:53,998:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:29:54,007:INFO:Starting cross validation
2024-11-25 12:29:54,010:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:54,771:INFO:Calculating mean and std
2024-11-25 12:29:54,773:INFO:Creating metrics dataframe
2024-11-25 12:29:54,775:INFO:Uploading results into container
2024-11-25 12:29:54,776:INFO:Uploading model into container now
2024-11-25 12:29:54,777:INFO:_master_model_container: 13
2024-11-25 12:29:54,777:INFO:_display_container: 2
2024-11-25 12:29:54,778:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:29:54,778:INFO:create_model() successfully completed......................................
2024-11-25 12:29:54,873:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:54,873:INFO:Creating metrics dataframe
2024-11-25 12:29:54,893:INFO:Initializing Dummy Classifier
2024-11-25 12:29:54,893:INFO:Total runtime is 0.2003131111462912 minutes
2024-11-25 12:29:54,900:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:54,901:INFO:Initializing create_model()
2024-11-25 12:29:54,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C768D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:54,902:INFO:Checking exceptions
2024-11-25 12:29:54,902:INFO:Importing libraries
2024-11-25 12:29:54,902:INFO:Copying training dataset
2024-11-25 12:29:54,912:INFO:Defining folds
2024-11-25 12:29:54,912:INFO:Declaring metric variables
2024-11-25 12:29:54,919:INFO:Importing untrained model
2024-11-25 12:29:54,927:INFO:Dummy Classifier Imported successfully
2024-11-25 12:29:54,938:INFO:Starting cross validation
2024-11-25 12:29:54,943:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:55,213:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:29:55,224:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:29:55,227:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:29:55,246:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:29:55,258:INFO:Calculating mean and std
2024-11-25 12:29:55,259:INFO:Creating metrics dataframe
2024-11-25 12:29:55,262:INFO:Uploading results into container
2024-11-25 12:29:55,263:INFO:Uploading model into container now
2024-11-25 12:29:55,263:INFO:_master_model_container: 14
2024-11-25 12:29:55,263:INFO:_display_container: 2
2024-11-25 12:29:55,264:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:29:55,264:INFO:create_model() successfully completed......................................
2024-11-25 12:29:55,342:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:55,342:INFO:Creating metrics dataframe
2024-11-25 12:29:55,353:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:29:55,362:INFO:Initializing create_model()
2024-11-25 12:29:55,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:55,363:INFO:Checking exceptions
2024-11-25 12:29:55,364:INFO:Importing libraries
2024-11-25 12:29:55,365:INFO:Copying training dataset
2024-11-25 12:29:55,370:INFO:Defining folds
2024-11-25 12:29:55,370:INFO:Declaring metric variables
2024-11-25 12:29:55,370:INFO:Importing untrained model
2024-11-25 12:29:55,370:INFO:Declaring custom model
2024-11-25 12:29:55,371:INFO:Ridge Classifier Imported successfully
2024-11-25 12:29:55,374:INFO:Cross validation set to False
2024-11-25 12:29:55,374:INFO:Fitting Model
2024-11-25 12:29:55,495:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:29:55,495:INFO:create_model() successfully completed......................................
2024-11-25 12:29:55,587:INFO:_master_model_container: 14
2024-11-25 12:29:55,587:INFO:_display_container: 2
2024-11-25 12:29:55,588:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:29:55,589:INFO:compare_models() successfully completed......................................
2024-11-25 12:29:55,594:INFO:Initializing compare_models()
2024-11-25 12:29:55,594:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:29:55,594:INFO:Checking exceptions
2024-11-25 12:29:55,598:INFO:Preparing display monitor
2024-11-25 12:29:55,631:INFO:Initializing Logistic Regression
2024-11-25 12:29:55,631:INFO:Total runtime is 1.6701221466064452e-05 minutes
2024-11-25 12:29:55,636:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:55,637:INFO:Initializing create_model()
2024-11-25 12:29:55,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:55,637:INFO:Checking exceptions
2024-11-25 12:29:55,637:INFO:Importing libraries
2024-11-25 12:29:55,637:INFO:Copying training dataset
2024-11-25 12:29:55,641:INFO:Defining folds
2024-11-25 12:29:55,641:INFO:Declaring metric variables
2024-11-25 12:29:55,644:INFO:Importing untrained model
2024-11-25 12:29:55,648:INFO:Logistic Regression Imported successfully
2024-11-25 12:29:55,656:INFO:Starting cross validation
2024-11-25 12:29:55,661:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:56,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,071:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,072:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,093:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,100:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,107:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,111:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,131:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,269:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,273:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:56,284:INFO:Calculating mean and std
2024-11-25 12:29:56,285:INFO:Creating metrics dataframe
2024-11-25 12:29:56,287:INFO:Uploading results into container
2024-11-25 12:29:56,287:INFO:Uploading model into container now
2024-11-25 12:29:56,287:INFO:_master_model_container: 1
2024-11-25 12:29:56,287:INFO:_display_container: 2
2024-11-25 12:29:56,288:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:29:56,288:INFO:create_model() successfully completed......................................
2024-11-25 12:29:56,355:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:56,355:INFO:Creating metrics dataframe
2024-11-25 12:29:56,360:INFO:Initializing K Neighbors Classifier
2024-11-25 12:29:56,360:INFO:Total runtime is 0.012163821856180828 minutes
2024-11-25 12:29:56,363:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:56,363:INFO:Initializing create_model()
2024-11-25 12:29:56,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:56,364:INFO:Checking exceptions
2024-11-25 12:29:56,364:INFO:Importing libraries
2024-11-25 12:29:56,364:INFO:Copying training dataset
2024-11-25 12:29:56,368:INFO:Defining folds
2024-11-25 12:29:56,368:INFO:Declaring metric variables
2024-11-25 12:29:56,372:INFO:Importing untrained model
2024-11-25 12:29:56,376:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:29:56,383:INFO:Starting cross validation
2024-11-25 12:29:56,388:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:57,132:INFO:Calculating mean and std
2024-11-25 12:29:57,133:INFO:Creating metrics dataframe
2024-11-25 12:29:57,137:INFO:Uploading results into container
2024-11-25 12:29:57,137:INFO:Uploading model into container now
2024-11-25 12:29:57,138:INFO:_master_model_container: 2
2024-11-25 12:29:57,138:INFO:_display_container: 2
2024-11-25 12:29:57,138:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:29:57,138:INFO:create_model() successfully completed......................................
2024-11-25 12:29:57,213:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:57,213:INFO:Creating metrics dataframe
2024-11-25 12:29:57,220:INFO:Initializing Naive Bayes
2024-11-25 12:29:57,220:INFO:Total runtime is 0.02649458646774292 minutes
2024-11-25 12:29:57,224:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:57,225:INFO:Initializing create_model()
2024-11-25 12:29:57,225:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:57,225:INFO:Checking exceptions
2024-11-25 12:29:57,225:INFO:Importing libraries
2024-11-25 12:29:57,225:INFO:Copying training dataset
2024-11-25 12:29:57,230:INFO:Defining folds
2024-11-25 12:29:57,230:INFO:Declaring metric variables
2024-11-25 12:29:57,234:INFO:Importing untrained model
2024-11-25 12:29:57,239:INFO:Naive Bayes Imported successfully
2024-11-25 12:29:57,248:INFO:Starting cross validation
2024-11-25 12:29:57,250:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:57,839:INFO:Calculating mean and std
2024-11-25 12:29:57,839:INFO:Creating metrics dataframe
2024-11-25 12:29:57,842:INFO:Uploading results into container
2024-11-25 12:29:57,842:INFO:Uploading model into container now
2024-11-25 12:29:57,843:INFO:_master_model_container: 3
2024-11-25 12:29:57,843:INFO:_display_container: 2
2024-11-25 12:29:57,843:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:29:57,843:INFO:create_model() successfully completed......................................
2024-11-25 12:29:57,909:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:57,910:INFO:Creating metrics dataframe
2024-11-25 12:29:57,917:INFO:Initializing Decision Tree Classifier
2024-11-25 12:29:57,918:INFO:Total runtime is 0.03812717199325562 minutes
2024-11-25 12:29:57,922:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:57,922:INFO:Initializing create_model()
2024-11-25 12:29:57,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:57,923:INFO:Checking exceptions
2024-11-25 12:29:57,923:INFO:Importing libraries
2024-11-25 12:29:57,923:INFO:Copying training dataset
2024-11-25 12:29:57,926:INFO:Defining folds
2024-11-25 12:29:57,926:INFO:Declaring metric variables
2024-11-25 12:29:57,930:INFO:Importing untrained model
2024-11-25 12:29:57,934:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:29:57,943:INFO:Starting cross validation
2024-11-25 12:29:57,945:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:58,517:INFO:Calculating mean and std
2024-11-25 12:29:58,519:INFO:Creating metrics dataframe
2024-11-25 12:29:58,521:INFO:Uploading results into container
2024-11-25 12:29:58,521:INFO:Uploading model into container now
2024-11-25 12:29:58,521:INFO:_master_model_container: 4
2024-11-25 12:29:58,521:INFO:_display_container: 2
2024-11-25 12:29:58,521:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:29:58,521:INFO:create_model() successfully completed......................................
2024-11-25 12:29:58,590:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:58,590:INFO:Creating metrics dataframe
2024-11-25 12:29:58,597:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:29:58,597:INFO:Total runtime is 0.04944941600163778 minutes
2024-11-25 12:29:58,600:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:58,602:INFO:Initializing create_model()
2024-11-25 12:29:58,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:58,602:INFO:Checking exceptions
2024-11-25 12:29:58,602:INFO:Importing libraries
2024-11-25 12:29:58,602:INFO:Copying training dataset
2024-11-25 12:29:58,606:INFO:Defining folds
2024-11-25 12:29:58,606:INFO:Declaring metric variables
2024-11-25 12:29:58,609:INFO:Importing untrained model
2024-11-25 12:29:58,613:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:29:58,621:INFO:Starting cross validation
2024-11-25 12:29:58,625:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:59,053:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,053:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,053:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,060:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,062:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,068:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,081:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,085:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,256:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,264:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,282:INFO:Calculating mean and std
2024-11-25 12:29:59,283:INFO:Creating metrics dataframe
2024-11-25 12:29:59,285:INFO:Uploading results into container
2024-11-25 12:29:59,286:INFO:Uploading model into container now
2024-11-25 12:29:59,287:INFO:_master_model_container: 5
2024-11-25 12:29:59,287:INFO:_display_container: 2
2024-11-25 12:29:59,287:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:29:59,287:INFO:create_model() successfully completed......................................
2024-11-25 12:29:59,356:INFO:SubProcess create_model() end ==================================
2024-11-25 12:29:59,356:INFO:Creating metrics dataframe
2024-11-25 12:29:59,364:INFO:Initializing Ridge Classifier
2024-11-25 12:29:59,364:INFO:Total runtime is 0.0622279167175293 minutes
2024-11-25 12:29:59,368:INFO:SubProcess create_model() called ==================================
2024-11-25 12:29:59,369:INFO:Initializing create_model()
2024-11-25 12:29:59,369:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:29:59,370:INFO:Checking exceptions
2024-11-25 12:29:59,370:INFO:Importing libraries
2024-11-25 12:29:59,370:INFO:Copying training dataset
2024-11-25 12:29:59,374:INFO:Defining folds
2024-11-25 12:29:59,374:INFO:Declaring metric variables
2024-11-25 12:29:59,378:INFO:Importing untrained model
2024-11-25 12:29:59,381:INFO:Ridge Classifier Imported successfully
2024-11-25 12:29:59,390:INFO:Starting cross validation
2024-11-25 12:29:59,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:29:59,768:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,772:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,782:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,784:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,785:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,802:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,802:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,815:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,956:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,958:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:29:59,971:INFO:Calculating mean and std
2024-11-25 12:29:59,972:INFO:Creating metrics dataframe
2024-11-25 12:29:59,974:INFO:Uploading results into container
2024-11-25 12:29:59,974:INFO:Uploading model into container now
2024-11-25 12:29:59,974:INFO:_master_model_container: 6
2024-11-25 12:29:59,974:INFO:_display_container: 2
2024-11-25 12:29:59,975:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:29:59,975:INFO:create_model() successfully completed......................................
2024-11-25 12:30:00,046:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:00,046:INFO:Creating metrics dataframe
2024-11-25 12:30:00,053:INFO:Initializing Random Forest Classifier
2024-11-25 12:30:00,054:INFO:Total runtime is 0.07372408310572307 minutes
2024-11-25 12:30:00,057:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:00,058:INFO:Initializing create_model()
2024-11-25 12:30:00,058:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:00,058:INFO:Checking exceptions
2024-11-25 12:30:00,058:INFO:Importing libraries
2024-11-25 12:30:00,058:INFO:Copying training dataset
2024-11-25 12:30:00,063:INFO:Defining folds
2024-11-25 12:30:00,064:INFO:Declaring metric variables
2024-11-25 12:30:00,067:INFO:Importing untrained model
2024-11-25 12:30:00,071:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:30:00,078:INFO:Starting cross validation
2024-11-25 12:30:00,080:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:01,318:INFO:Calculating mean and std
2024-11-25 12:30:01,319:INFO:Creating metrics dataframe
2024-11-25 12:30:01,322:INFO:Uploading results into container
2024-11-25 12:30:01,323:INFO:Uploading model into container now
2024-11-25 12:30:01,324:INFO:_master_model_container: 7
2024-11-25 12:30:01,324:INFO:_display_container: 2
2024-11-25 12:30:01,325:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:30:01,325:INFO:create_model() successfully completed......................................
2024-11-25 12:30:01,394:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:01,394:INFO:Creating metrics dataframe
2024-11-25 12:30:01,401:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:30:01,401:INFO:Total runtime is 0.0961796522140503 minutes
2024-11-25 12:30:01,405:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:01,405:INFO:Initializing create_model()
2024-11-25 12:30:01,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:01,406:INFO:Checking exceptions
2024-11-25 12:30:01,406:INFO:Importing libraries
2024-11-25 12:30:01,406:INFO:Copying training dataset
2024-11-25 12:30:01,410:INFO:Defining folds
2024-11-25 12:30:01,410:INFO:Declaring metric variables
2024-11-25 12:30:01,413:INFO:Importing untrained model
2024-11-25 12:30:01,418:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:30:01,425:INFO:Starting cross validation
2024-11-25 12:30:01,428:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:01,745:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,756:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,765:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,779:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,783:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,785:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,792:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,802:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:01,852:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,860:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,874:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,888:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,889:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,898:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:01,912:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,007:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:02,015:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:30:02,048:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,057:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,071:INFO:Calculating mean and std
2024-11-25 12:30:02,072:INFO:Creating metrics dataframe
2024-11-25 12:30:02,073:INFO:Uploading results into container
2024-11-25 12:30:02,074:INFO:Uploading model into container now
2024-11-25 12:30:02,075:INFO:_master_model_container: 8
2024-11-25 12:30:02,075:INFO:_display_container: 2
2024-11-25 12:30:02,075:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:30:02,075:INFO:create_model() successfully completed......................................
2024-11-25 12:30:02,144:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:02,144:INFO:Creating metrics dataframe
2024-11-25 12:30:02,151:INFO:Initializing Ada Boost Classifier
2024-11-25 12:30:02,151:INFO:Total runtime is 0.10867193539937338 minutes
2024-11-25 12:30:02,156:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:02,156:INFO:Initializing create_model()
2024-11-25 12:30:02,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:02,156:INFO:Checking exceptions
2024-11-25 12:30:02,156:INFO:Importing libraries
2024-11-25 12:30:02,157:INFO:Copying training dataset
2024-11-25 12:30:02,160:INFO:Defining folds
2024-11-25 12:30:02,160:INFO:Declaring metric variables
2024-11-25 12:30:02,164:INFO:Importing untrained model
2024-11-25 12:30:02,169:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:30:02,176:INFO:Starting cross validation
2024-11-25 12:30:02,180:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:02,453:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,461:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,464:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,469:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,475:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,478:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,482:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,491:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,757:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,781:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,787:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,789:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,795:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,811:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,828:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,830:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:02,922:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:02,933:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:30:03,047:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:03,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:03,075:INFO:Calculating mean and std
2024-11-25 12:30:03,076:INFO:Creating metrics dataframe
2024-11-25 12:30:03,078:INFO:Uploading results into container
2024-11-25 12:30:03,078:INFO:Uploading model into container now
2024-11-25 12:30:03,078:INFO:_master_model_container: 9
2024-11-25 12:30:03,078:INFO:_display_container: 2
2024-11-25 12:30:03,079:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:30:03,079:INFO:create_model() successfully completed......................................
2024-11-25 12:30:03,148:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:03,148:INFO:Creating metrics dataframe
2024-11-25 12:30:03,156:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:30:03,156:INFO:Total runtime is 0.12542659044265747 minutes
2024-11-25 12:30:03,160:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:03,161:INFO:Initializing create_model()
2024-11-25 12:30:03,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:03,161:INFO:Checking exceptions
2024-11-25 12:30:03,161:INFO:Importing libraries
2024-11-25 12:30:03,161:INFO:Copying training dataset
2024-11-25 12:30:03,165:INFO:Defining folds
2024-11-25 12:30:03,166:INFO:Declaring metric variables
2024-11-25 12:30:03,169:INFO:Importing untrained model
2024-11-25 12:30:03,174:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:30:03,181:INFO:Starting cross validation
2024-11-25 12:30:03,184:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:04,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,280:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,288:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,299:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,350:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,369:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,375:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,376:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,801:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:04,823:INFO:Calculating mean and std
2024-11-25 12:30:04,823:INFO:Creating metrics dataframe
2024-11-25 12:30:04,823:INFO:Uploading results into container
2024-11-25 12:30:04,827:INFO:Uploading model into container now
2024-11-25 12:30:04,827:INFO:_master_model_container: 10
2024-11-25 12:30:04,827:INFO:_display_container: 2
2024-11-25 12:30:04,827:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:30:04,827:INFO:create_model() successfully completed......................................
2024-11-25 12:30:04,891:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:04,891:INFO:Creating metrics dataframe
2024-11-25 12:30:04,899:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:30:04,899:INFO:Total runtime is 0.15447906255722046 minutes
2024-11-25 12:30:04,903:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:04,903:INFO:Initializing create_model()
2024-11-25 12:30:04,903:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:04,903:INFO:Checking exceptions
2024-11-25 12:30:04,903:INFO:Importing libraries
2024-11-25 12:30:04,903:INFO:Copying training dataset
2024-11-25 12:30:04,911:INFO:Defining folds
2024-11-25 12:30:04,911:INFO:Declaring metric variables
2024-11-25 12:30:04,915:INFO:Importing untrained model
2024-11-25 12:30:04,919:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:30:04,927:INFO:Starting cross validation
2024-11-25 12:30:04,931:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:05,325:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,331:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,335:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,351:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,399:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,407:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,418:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,426:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,579:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,583:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:30:05,605:INFO:Calculating mean and std
2024-11-25 12:30:05,605:INFO:Creating metrics dataframe
2024-11-25 12:30:05,605:INFO:Uploading results into container
2024-11-25 12:30:05,609:INFO:Uploading model into container now
2024-11-25 12:30:05,609:INFO:_master_model_container: 11
2024-11-25 12:30:05,609:INFO:_display_container: 2
2024-11-25 12:30:05,609:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:30:05,609:INFO:create_model() successfully completed......................................
2024-11-25 12:30:05,689:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:05,689:INFO:Creating metrics dataframe
2024-11-25 12:30:05,697:INFO:Initializing Extra Trees Classifier
2024-11-25 12:30:05,697:INFO:Total runtime is 0.16776803334554036 minutes
2024-11-25 12:30:05,701:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:05,701:INFO:Initializing create_model()
2024-11-25 12:30:05,701:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:05,701:INFO:Checking exceptions
2024-11-25 12:30:05,701:INFO:Importing libraries
2024-11-25 12:30:05,701:INFO:Copying training dataset
2024-11-25 12:30:05,705:INFO:Defining folds
2024-11-25 12:30:05,705:INFO:Declaring metric variables
2024-11-25 12:30:05,705:INFO:Importing untrained model
2024-11-25 12:30:05,709:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:30:05,717:INFO:Starting cross validation
2024-11-25 12:30:05,721:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:06,883:INFO:Calculating mean and std
2024-11-25 12:30:06,883:INFO:Creating metrics dataframe
2024-11-25 12:30:06,883:INFO:Uploading results into container
2024-11-25 12:30:06,883:INFO:Uploading model into container now
2024-11-25 12:30:06,883:INFO:_master_model_container: 12
2024-11-25 12:30:06,883:INFO:_display_container: 2
2024-11-25 12:30:06,883:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:30:06,883:INFO:create_model() successfully completed......................................
2024-11-25 12:30:06,948:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:06,948:INFO:Creating metrics dataframe
2024-11-25 12:30:06,948:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:30:06,948:INFO:Total runtime is 0.18862417538960774 minutes
2024-11-25 12:30:06,963:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:06,963:INFO:Initializing create_model()
2024-11-25 12:30:06,963:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:06,963:INFO:Checking exceptions
2024-11-25 12:30:06,963:INFO:Importing libraries
2024-11-25 12:30:06,963:INFO:Copying training dataset
2024-11-25 12:30:06,963:INFO:Defining folds
2024-11-25 12:30:06,963:INFO:Declaring metric variables
2024-11-25 12:30:06,963:INFO:Importing untrained model
2024-11-25 12:30:06,963:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:30:06,978:INFO:Starting cross validation
2024-11-25 12:30:06,978:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:09,032:INFO:Calculating mean and std
2024-11-25 12:30:09,034:INFO:Creating metrics dataframe
2024-11-25 12:30:09,034:INFO:Uploading results into container
2024-11-25 12:30:09,034:INFO:Uploading model into container now
2024-11-25 12:30:09,034:INFO:_master_model_container: 13
2024-11-25 12:30:09,034:INFO:_display_container: 2
2024-11-25 12:30:09,034:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:30:09,034:INFO:create_model() successfully completed......................................
2024-11-25 12:30:09,137:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:09,137:INFO:Creating metrics dataframe
2024-11-25 12:30:09,153:INFO:Initializing Dummy Classifier
2024-11-25 12:30:09,153:INFO:Total runtime is 0.2253784894943237 minutes
2024-11-25 12:30:09,158:INFO:SubProcess create_model() called ==================================
2024-11-25 12:30:09,158:INFO:Initializing create_model()
2024-11-25 12:30:09,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B93C7F1D90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:09,159:INFO:Checking exceptions
2024-11-25 12:30:09,159:INFO:Importing libraries
2024-11-25 12:30:09,159:INFO:Copying training dataset
2024-11-25 12:30:09,165:INFO:Defining folds
2024-11-25 12:30:09,166:INFO:Declaring metric variables
2024-11-25 12:30:09,171:INFO:Importing untrained model
2024-11-25 12:30:09,176:INFO:Dummy Classifier Imported successfully
2024-11-25 12:30:09,187:INFO:Starting cross validation
2024-11-25 12:30:09,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:30:09,596:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,602:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,615:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,616:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,617:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,621:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,633:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,645:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,827:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,830:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:30:09,847:INFO:Calculating mean and std
2024-11-25 12:30:09,849:INFO:Creating metrics dataframe
2024-11-25 12:30:09,851:INFO:Uploading results into container
2024-11-25 12:30:09,852:INFO:Uploading model into container now
2024-11-25 12:30:09,852:INFO:_master_model_container: 14
2024-11-25 12:30:09,852:INFO:_display_container: 2
2024-11-25 12:30:09,853:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:30:09,853:INFO:create_model() successfully completed......................................
2024-11-25 12:30:09,930:INFO:SubProcess create_model() end ==================================
2024-11-25 12:30:09,931:INFO:Creating metrics dataframe
2024-11-25 12:30:09,940:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:30:09,951:INFO:Initializing create_model()
2024-11-25 12:30:09,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364310>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:30:09,952:INFO:Checking exceptions
2024-11-25 12:30:09,954:INFO:Importing libraries
2024-11-25 12:30:09,954:INFO:Copying training dataset
2024-11-25 12:30:09,959:INFO:Defining folds
2024-11-25 12:30:09,959:INFO:Declaring metric variables
2024-11-25 12:30:09,959:INFO:Importing untrained model
2024-11-25 12:30:09,959:INFO:Declaring custom model
2024-11-25 12:30:09,960:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:30:09,964:INFO:Cross validation set to False
2024-11-25 12:30:09,964:INFO:Fitting Model
2024-11-25 12:30:10,102:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:30:10,103:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.
2024-11-25 12:30:10,103:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-25 12:30:10,103:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-25 12:30:10,103:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:30:10,103:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:30:10,104:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:30:10,104:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:30:10,104:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:30:10,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:30:10,262:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:30:10,263:INFO:create_model() successfully completed......................................
2024-11-25 12:30:10,396:INFO:_master_model_container: 14
2024-11-25 12:30:10,397:INFO:_display_container: 2
2024-11-25 12:30:10,397:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:30:10,397:INFO:compare_models() successfully completed......................................
2024-11-25 12:30:10,405:INFO:Initializing evaluate_model()
2024-11-25 12:30:10,405:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:30:10,422:INFO:Initializing plot_model()
2024-11-25 12:30:10,423:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:30:10,423:INFO:Checking exceptions
2024-11-25 12:30:10,425:INFO:Preloading libraries
2024-11-25 12:30:10,425:INFO:Copying training dataset
2024-11-25 12:30:10,426:INFO:Plot type: pipeline
2024-11-25 12:30:10,612:INFO:Visual Rendered Successfully
2024-11-25 12:30:10,690:INFO:plot_model() successfully completed......................................
2024-11-25 12:30:10,701:INFO:Initializing plot_model()
2024-11-25 12:30:10,701:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:30:10,701:INFO:Checking exceptions
2024-11-25 12:30:10,706:INFO:Preloading libraries
2024-11-25 12:30:10,707:INFO:Copying training dataset
2024-11-25 12:30:10,707:INFO:Plot type: confusion_matrix
2024-11-25 12:30:10,951:INFO:Fitting Model
2024-11-25 12:30:10,952:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:30:10,952:INFO:Scoring test/hold-out set
2024-11-25 12:30:11,091:INFO:Visual Rendered Successfully
2024-11-25 12:30:11,163:INFO:plot_model() successfully completed......................................
2024-11-25 12:30:11,170:INFO:Initializing predict_model()
2024-11-25 12:30:11,170:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001B93C9089A0>)
2024-11-25 12:30:11,170:INFO:Checking exceptions
2024-11-25 12:30:11,170:INFO:Preloading libraries
2024-11-25 12:30:11,173:INFO:Set up data.
2024-11-25 12:30:11,182:INFO:Set up index.
2024-11-25 12:30:11,286:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:30:11,291:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2363, in recall_score
    _, r, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:30:11,295:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 2182, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:30:11,299:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 93, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1271, in f1_score
    return fbeta_score(
           ^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1463, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1767, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 1542, in _check_set_wise_labels
    present_labels = unique_labels(y_true, y_pred).tolist()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:30:11,302:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 691, in cohen_kappa_score
    confusion = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 186, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 324, in confusion_matrix
    labels = unique_labels(y_true, y_pred)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\multiclass.py", line 116, in unique_labels
    raise ValueError("Mix of label input types (string and number)")
ValueError: Mix of label input types (string and number)

  warnings.warn(traceback.format_exc())

2024-11-25 12:30:11,305:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 174, in _unique_python
    uniques = sorted(uniques_set)
              ^^^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'str' and 'int'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py", line 987, in matthews_corrcoef
    lb.fit(np.hstack([y_true, y_pred]))
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\preprocessing\_label.py", line 98, in fit
    self.classes_ = _unique(y)
                    ^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 42, in _unique
    return _unique_python(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_encode.py", line 179, in _unique_python
    raise TypeError(
TypeError: Encoders require their input argument must be uniformly strings or numbers. Got ['int', 'str']

  warnings.warn(traceback.format_exc())

2024-11-25 12:31:51,585:INFO:Initializing predict_model()
2024-11-25 12:31:51,585:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B93C364210>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001B93C520680>)
2024-11-25 12:31:51,585:INFO:Checking exceptions
2024-11-25 12:31:51,585:INFO:Preloading libraries
2024-11-25 12:31:51,589:INFO:Set up data.
2024-11-25 12:31:51,597:INFO:Set up index.
2024-11-25 12:31:51,685:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_a70753aed438478e80f0bab47d71c6ff
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_45bd994950f14812acc757ef39bae5ef_8a5fe58a705349058c3db40b073b9c88
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_626342be7f71475295fc86a0d3464e0d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_fa9665fe76574f069cecada401a5238c_724a753b9c7c49859286bb6421059527
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_a8d6fb7132d64052bfc4b47003787c67
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f64caa9201544308ab2b8fce36eb783d_17076af08a1c4d6c8da718ce111d6ee7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_dcf37a324c2641ae82e344c221a6c403
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_d39817da326246e1a8af82be467d3333_9eed17c65b5e471db4720804b2e6868a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_29e17ffe9cb04f5d82ab72dd359305a3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_c6b6d7e33b254c8db904a14293233e3f_44202b7dca2444cb8669808397523c90
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_3fdbc7b43d9841349b456bbf73a7d234
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_870ef5a3dad94ff98086aa43e1b66a9e_5205437c98e448f9ad6321d991864ac0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_3f55692957a3420fab648063e6255f24
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_9805ae573a2f46ba9a9120595bb9f8c8_955ee0ac48ab4e028745afd4d7aa5e9f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_5c2931cff94c4a4680c4c199e4526e43
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_44de8c3c6ff3412293cce66929b187a4_3665d6512e104a33ab7e34d7a1b6bc64
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,232:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_75d973ca4b354a7393753e3516b5d16d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_99bd769235ce4d298585ef6087868169_9eb9e26a48324c55903f3386557c4734
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_207cf4eb23af4816816006167419d344
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_8945db818a0c4b838814821eda5ccd83_f5772fca92ca4ca48a06ca76f05e2960
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_5cae9fffb7b44ee2888f54b23022a45e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_2d5a77ca02d3420882ddafc37682fd4d_b97731df115a450bae84c48911fa44c2
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_cc6d6bc616804a9fb563a4231ca801e1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_c02af415ce81445b92b7809870e81539_4a05bd3a445e4d02945de7835ae4688a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_a2b44128b9d246c7b4b9ba22af652d1e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_ebc9a3225a0d4659a47e0f1e2791b963_6da15f9d6304408f8c0d985d11fda3fb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,233:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_344210b64d234f77b19de99758dbbe59
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_14606c6e7194492a9a3b9afcf5022169_b9a01a1d09da484090389b154f875b41
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_192517c7140c469f91527b6d5798a530
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_b913078bae6a420c8cdd3b1146088d62_097fddc3e5284d4da3e8c98872fc8097
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_ef353908469d46d2bb1b2b6e2d876e9d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_055269d131ca4670a574235c4e227ab2_84107c949ac8472687131c768ffd4a78
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_dccb80a931594213a81b8a5319912019
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_27bb0fa8a9c1487ebd31da6ea4a17b17_81d8aa2be47b43589c74053ae339aff3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_98c219f9f88b4b988e66b7b5c0dc4cbe
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_47ebca51179e4cf193630a1da6da8524_77ea19ee78e44ebab955644f9be85a46
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,234:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_4f9943f2e4664d8d8ad181d3ca7d2acd
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_a0e79288781040aeb0eb68b8349137c6_fd0b53cb01594259b58e8b171abf12ce
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_7bc196724aa2498c8102fca669a631bb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_ae13f6a06a5e4e4194f670319612c490_542f2d6f2f724f3aaf01faac2d0abf53
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_c7e0f3beba93408baa64a75c3238aa5b
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_71838cf46ae54daeaf37aad119518a35_5f7c480c9fe94acd9d522d8c696b0e8f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_d2cb64cb318343be9c1159df94d28ca3
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_00ddcdc6ca6945b384cdca0d19960949_a3af9ede710c4d9a9b3d6909a71f7a10
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_90b18a7edc294677a050c6bdc830c4f2
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_5d8d57cc97fe413fadeb63a4cbad657d_84c747044f7e430a896636d177324522
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_5eac99d1a80c48cb88959b8fa9e40c92
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,235:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_bd5343453d834f67a4c3fa0abffffcc8_224b8711286147e58da1571ef2dc88f5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_ff76ee14f8b2496e80f2721f222cc120
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_4f199aee25aa4916a31537e29360aa3f_02e22d4d711d435ea024f061377d46aa
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_04a010e1f4dc447ca9c49cb411e5fc17
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_18bffb9595fd4004ae60874ad6c824b4_29d291709fd44621a6142fdf79bed508
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_022f931577be4be698b4227c052234f6
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_62406739706448daa61767a1e909be1c_79a9438ce334456f958bd5ecf5185c05
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_ee65a55da2244cbf8d32200e1331d0e2
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:42,236:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_9440_f5a221a9aa0547c6a6af60e1aae1f0f0_da5a7b67e8cb44109d8290fbad68e9bb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:35:51,890:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:35:51,891:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:35:51,891:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:35:51,891:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:35:52,312:INFO:PyCaret ClassificationExperiment
2024-11-25 12:35:52,312:INFO:Logging name: clf-default-name
2024-11-25 12:35:52,312:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:35:52,312:INFO:version 3.3.2
2024-11-25 12:35:52,312:INFO:Initializing setup()
2024-11-25 12:35:52,312:INFO:self.USI: 5091
2024-11-25 12:35:52,312:INFO:self._variable_keys: {'html_param', '_ml_usecase', 'gpu_n_jobs_param', 'fold_groups_param', 'y_train', 'fix_imbalance', 'seed', 'fold_shuffle_param', 'X_train', 'log_plots_param', '_available_plots', 'data', 'memory', 'y_test', 'y', 'exp_name_log', 'target_param', 'idx', 'exp_id', 'pipeline', 'X', 'X_test', 'USI', 'gpu_param', 'is_multiclass', 'fold_generator', 'n_jobs_param', 'logging_param'}
2024-11-25 12:35:52,312:INFO:Checking environment
2024-11-25 12:35:52,312:INFO:python_version: 3.11.9
2024-11-25 12:35:52,313:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:35:52,313:INFO:machine: AMD64
2024-11-25 12:35:52,313:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:35:52,317:INFO:Memory: svmem(total=25525833728, available=14802022400, percent=42.0, used=10723811328, free=14802022400)
2024-11-25 12:35:52,317:INFO:Physical Core: 4
2024-11-25 12:35:52,317:INFO:Logical Core: 8
2024-11-25 12:35:52,317:INFO:Checking libraries
2024-11-25 12:35:52,317:INFO:System:
2024-11-25 12:35:52,317:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:35:52,317:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:35:52,317:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:35:52,317:INFO:PyCaret required dependencies:
2024-11-25 12:35:52,339:INFO:                 pip: 24.0
2024-11-25 12:35:52,340:INFO:          setuptools: 65.5.0
2024-11-25 12:35:52,340:INFO:             pycaret: 3.3.2
2024-11-25 12:35:52,340:INFO:             IPython: 8.29.0
2024-11-25 12:35:52,340:INFO:          ipywidgets: 8.1.5
2024-11-25 12:35:52,340:INFO:                tqdm: 4.66.6
2024-11-25 12:35:52,340:INFO:               numpy: 1.26.4
2024-11-25 12:35:52,340:INFO:              pandas: 2.1.4
2024-11-25 12:35:52,340:INFO:              jinja2: 3.1.4
2024-11-25 12:35:52,340:INFO:               scipy: 1.11.4
2024-11-25 12:35:52,340:INFO:              joblib: 1.3.2
2024-11-25 12:35:52,340:INFO:             sklearn: 1.4.2
2024-11-25 12:35:52,340:INFO:                pyod: 2.0.2
2024-11-25 12:35:52,340:INFO:            imblearn: 0.12.4
2024-11-25 12:35:52,340:INFO:   category_encoders: 2.6.4
2024-11-25 12:35:52,340:INFO:            lightgbm: 4.5.0
2024-11-25 12:35:52,340:INFO:               numba: 0.60.0
2024-11-25 12:35:52,340:INFO:            requests: 2.32.3
2024-11-25 12:35:52,340:INFO:          matplotlib: 3.7.5
2024-11-25 12:35:52,340:INFO:          scikitplot: 0.3.7
2024-11-25 12:35:52,340:INFO:         yellowbrick: 1.5
2024-11-25 12:35:52,340:INFO:              plotly: 5.24.1
2024-11-25 12:35:52,340:INFO:    plotly-resampler: Not installed
2024-11-25 12:35:52,341:INFO:             kaleido: 0.2.1
2024-11-25 12:35:52,341:INFO:           schemdraw: 0.15
2024-11-25 12:35:52,341:INFO:         statsmodels: 0.14.4
2024-11-25 12:35:52,341:INFO:              sktime: 0.26.0
2024-11-25 12:35:52,341:INFO:               tbats: 1.1.3
2024-11-25 12:35:52,341:INFO:            pmdarima: 2.0.4
2024-11-25 12:35:52,341:INFO:              psutil: 6.1.0
2024-11-25 12:35:52,341:INFO:          markupsafe: 3.0.2
2024-11-25 12:35:52,341:INFO:             pickle5: Not installed
2024-11-25 12:35:52,341:INFO:         cloudpickle: 3.1.0
2024-11-25 12:35:52,341:INFO:         deprecation: 2.1.0
2024-11-25 12:35:52,341:INFO:              xxhash: 3.5.0
2024-11-25 12:35:52,341:INFO:           wurlitzer: 3.1.1
2024-11-25 12:35:52,341:INFO:PyCaret optional dependencies:
2024-11-25 12:35:52,359:INFO:                shap: Not installed
2024-11-25 12:35:52,359:INFO:           interpret: Not installed
2024-11-25 12:35:52,359:INFO:                umap: Not installed
2024-11-25 12:35:52,359:INFO:     ydata_profiling: Not installed
2024-11-25 12:35:52,359:INFO:  explainerdashboard: Not installed
2024-11-25 12:35:52,359:INFO:             autoviz: Not installed
2024-11-25 12:35:52,359:INFO:           fairlearn: Not installed
2024-11-25 12:35:52,359:INFO:          deepchecks: Not installed
2024-11-25 12:35:52,359:INFO:             xgboost: Not installed
2024-11-25 12:35:52,360:INFO:            catboost: Not installed
2024-11-25 12:35:52,360:INFO:              kmodes: Not installed
2024-11-25 12:35:52,360:INFO:             mlxtend: Not installed
2024-11-25 12:35:52,360:INFO:       statsforecast: Not installed
2024-11-25 12:35:52,360:INFO:        tune_sklearn: Not installed
2024-11-25 12:35:52,360:INFO:                 ray: Not installed
2024-11-25 12:35:52,360:INFO:            hyperopt: Not installed
2024-11-25 12:35:52,360:INFO:              optuna: Not installed
2024-11-25 12:35:52,360:INFO:               skopt: Not installed
2024-11-25 12:35:52,360:INFO:              mlflow: Not installed
2024-11-25 12:35:52,360:INFO:              gradio: Not installed
2024-11-25 12:35:52,360:INFO:             fastapi: Not installed
2024-11-25 12:35:52,361:INFO:             uvicorn: Not installed
2024-11-25 12:35:52,361:INFO:              m2cgen: Not installed
2024-11-25 12:35:52,361:INFO:           evidently: Not installed
2024-11-25 12:35:52,361:INFO:               fugue: Not installed
2024-11-25 12:35:52,361:INFO:           streamlit: Not installed
2024-11-25 12:35:52,361:INFO:             prophet: Not installed
2024-11-25 12:35:52,361:INFO:None
2024-11-25 12:35:52,361:INFO:Set up data.
2024-11-25 12:35:52,367:INFO:Set up folding strategy.
2024-11-25 12:35:52,368:INFO:Set up train/test split.
2024-11-25 12:35:52,378:INFO:Set up index.
2024-11-25 12:35:52,378:INFO:Assigning column types.
2024-11-25 12:35:52,380:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:35:52,432:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,437:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,475:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,475:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,520:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,521:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,550:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,550:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:35:52,596:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,623:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,624:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,669:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:52,699:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,699:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,699:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:35:52,778:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,779:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,866:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,866:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:52,868:INFO:Preparing preprocessing pipeline...
2024-11-25 12:35:52,869:INFO:Set up simple imputation.
2024-11-25 12:35:52,872:INFO:Set up encoding of categorical features.
2024-11-25 12:35:52,872:INFO:Set up column name cleaning.
2024-11-25 12:35:53,018:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:35:53,026:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:35:53,026:INFO:Creating final display dataframe.
2024-11-25 12:35:53,232:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              5091
2024-11-25 12:35:53,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,393:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,395:INFO:setup() successfully completed in 1.09s...............
2024-11-25 12:35:53,410:INFO:PyCaret ClassificationExperiment
2024-11-25 12:35:53,410:INFO:Logging name: clf-default-name
2024-11-25 12:35:53,410:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:35:53,410:INFO:version 3.3.2
2024-11-25 12:35:53,410:INFO:Initializing setup()
2024-11-25 12:35:53,410:INFO:self.USI: 1d0d
2024-11-25 12:35:53,410:INFO:self._variable_keys: {'html_param', '_ml_usecase', 'gpu_n_jobs_param', 'fold_groups_param', 'y_train', 'fix_imbalance', 'seed', 'fold_shuffle_param', 'X_train', 'log_plots_param', '_available_plots', 'data', 'memory', 'y_test', 'y', 'exp_name_log', 'target_param', 'idx', 'exp_id', 'pipeline', 'X', 'X_test', 'USI', 'gpu_param', 'is_multiclass', 'fold_generator', 'n_jobs_param', 'logging_param'}
2024-11-25 12:35:53,411:INFO:Checking environment
2024-11-25 12:35:53,411:INFO:python_version: 3.11.9
2024-11-25 12:35:53,411:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:35:53,411:INFO:machine: AMD64
2024-11-25 12:35:53,411:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:35:53,413:INFO:Memory: svmem(total=25525833728, available=14809247744, percent=42.0, used=10716585984, free=14809247744)
2024-11-25 12:35:53,413:INFO:Physical Core: 4
2024-11-25 12:35:53,413:INFO:Logical Core: 8
2024-11-25 12:35:53,414:INFO:Checking libraries
2024-11-25 12:35:53,414:INFO:System:
2024-11-25 12:35:53,414:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:35:53,414:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:35:53,414:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:35:53,414:INFO:PyCaret required dependencies:
2024-11-25 12:35:53,414:INFO:                 pip: 24.0
2024-11-25 12:35:53,414:INFO:          setuptools: 65.5.0
2024-11-25 12:35:53,414:INFO:             pycaret: 3.3.2
2024-11-25 12:35:53,414:INFO:             IPython: 8.29.0
2024-11-25 12:35:53,414:INFO:          ipywidgets: 8.1.5
2024-11-25 12:35:53,414:INFO:                tqdm: 4.66.6
2024-11-25 12:35:53,414:INFO:               numpy: 1.26.4
2024-11-25 12:35:53,414:INFO:              pandas: 2.1.4
2024-11-25 12:35:53,414:INFO:              jinja2: 3.1.4
2024-11-25 12:35:53,414:INFO:               scipy: 1.11.4
2024-11-25 12:35:53,414:INFO:              joblib: 1.3.2
2024-11-25 12:35:53,414:INFO:             sklearn: 1.4.2
2024-11-25 12:35:53,414:INFO:                pyod: 2.0.2
2024-11-25 12:35:53,414:INFO:            imblearn: 0.12.4
2024-11-25 12:35:53,414:INFO:   category_encoders: 2.6.4
2024-11-25 12:35:53,414:INFO:            lightgbm: 4.5.0
2024-11-25 12:35:53,414:INFO:               numba: 0.60.0
2024-11-25 12:35:53,414:INFO:            requests: 2.32.3
2024-11-25 12:35:53,414:INFO:          matplotlib: 3.7.5
2024-11-25 12:35:53,414:INFO:          scikitplot: 0.3.7
2024-11-25 12:35:53,415:INFO:         yellowbrick: 1.5
2024-11-25 12:35:53,415:INFO:              plotly: 5.24.1
2024-11-25 12:35:53,415:INFO:    plotly-resampler: Not installed
2024-11-25 12:35:53,415:INFO:             kaleido: 0.2.1
2024-11-25 12:35:53,415:INFO:           schemdraw: 0.15
2024-11-25 12:35:53,415:INFO:         statsmodels: 0.14.4
2024-11-25 12:35:53,415:INFO:              sktime: 0.26.0
2024-11-25 12:35:53,415:INFO:               tbats: 1.1.3
2024-11-25 12:35:53,415:INFO:            pmdarima: 2.0.4
2024-11-25 12:35:53,415:INFO:              psutil: 6.1.0
2024-11-25 12:35:53,415:INFO:          markupsafe: 3.0.2
2024-11-25 12:35:53,415:INFO:             pickle5: Not installed
2024-11-25 12:35:53,415:INFO:         cloudpickle: 3.1.0
2024-11-25 12:35:53,415:INFO:         deprecation: 2.1.0
2024-11-25 12:35:53,415:INFO:              xxhash: 3.5.0
2024-11-25 12:35:53,415:INFO:           wurlitzer: 3.1.1
2024-11-25 12:35:53,415:INFO:PyCaret optional dependencies:
2024-11-25 12:35:53,415:INFO:                shap: Not installed
2024-11-25 12:35:53,415:INFO:           interpret: Not installed
2024-11-25 12:35:53,415:INFO:                umap: Not installed
2024-11-25 12:35:53,415:INFO:     ydata_profiling: Not installed
2024-11-25 12:35:53,415:INFO:  explainerdashboard: Not installed
2024-11-25 12:35:53,415:INFO:             autoviz: Not installed
2024-11-25 12:35:53,415:INFO:           fairlearn: Not installed
2024-11-25 12:35:53,415:INFO:          deepchecks: Not installed
2024-11-25 12:35:53,415:INFO:             xgboost: Not installed
2024-11-25 12:35:53,415:INFO:            catboost: Not installed
2024-11-25 12:35:53,415:INFO:              kmodes: Not installed
2024-11-25 12:35:53,415:INFO:             mlxtend: Not installed
2024-11-25 12:35:53,415:INFO:       statsforecast: Not installed
2024-11-25 12:35:53,416:INFO:        tune_sklearn: Not installed
2024-11-25 12:35:53,416:INFO:                 ray: Not installed
2024-11-25 12:35:53,416:INFO:            hyperopt: Not installed
2024-11-25 12:35:53,416:INFO:              optuna: Not installed
2024-11-25 12:35:53,416:INFO:               skopt: Not installed
2024-11-25 12:35:53,416:INFO:              mlflow: Not installed
2024-11-25 12:35:53,416:INFO:              gradio: Not installed
2024-11-25 12:35:53,416:INFO:             fastapi: Not installed
2024-11-25 12:35:53,416:INFO:             uvicorn: Not installed
2024-11-25 12:35:53,416:INFO:              m2cgen: Not installed
2024-11-25 12:35:53,416:INFO:           evidently: Not installed
2024-11-25 12:35:53,416:INFO:               fugue: Not installed
2024-11-25 12:35:53,416:INFO:           streamlit: Not installed
2024-11-25 12:35:53,416:INFO:             prophet: Not installed
2024-11-25 12:35:53,416:INFO:None
2024-11-25 12:35:53,416:INFO:Set up data.
2024-11-25 12:35:53,422:INFO:Set up folding strategy.
2024-11-25 12:35:53,422:INFO:Set up train/test split.
2024-11-25 12:35:53,427:INFO:Set up index.
2024-11-25 12:35:53,427:INFO:Assigning column types.
2024-11-25 12:35:53,430:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:35:53,473:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,474:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,499:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,499:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,542:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,542:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,568:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:35:53,611:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,636:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,637:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,681:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:35:53,708:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,708:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:35:53,779:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,780:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,852:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,852:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:53,853:INFO:Preparing preprocessing pipeline...
2024-11-25 12:35:53,854:INFO:Set up simple imputation.
2024-11-25 12:35:53,856:INFO:Set up encoding of categorical features.
2024-11-25 12:35:53,857:INFO:Set up column name cleaning.
2024-11-25 12:35:54,000:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:35:54,006:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:35:54,006:INFO:Creating final display dataframe.
2024-11-25 12:35:54,220:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              1d0d
2024-11-25 12:35:54,308:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:54,308:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:54,377:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:54,377:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:35:54,379:INFO:setup() successfully completed in 0.97s...............
2024-11-25 12:35:54,387:INFO:Initializing compare_models()
2024-11-25 12:35:54,387:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:35:54,387:INFO:Checking exceptions
2024-11-25 12:35:54,392:INFO:Preparing display monitor
2024-11-25 12:35:54,423:INFO:Initializing Logistic Regression
2024-11-25 12:35:54,423:INFO:Total runtime is 0.0 minutes
2024-11-25 12:35:54,429:INFO:SubProcess create_model() called ==================================
2024-11-25 12:35:54,429:INFO:Initializing create_model()
2024-11-25 12:35:54,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:35:54,430:INFO:Checking exceptions
2024-11-25 12:35:54,430:INFO:Importing libraries
2024-11-25 12:35:54,430:INFO:Copying training dataset
2024-11-25 12:35:54,434:INFO:Defining folds
2024-11-25 12:35:54,434:INFO:Declaring metric variables
2024-11-25 12:35:54,438:INFO:Importing untrained model
2024-11-25 12:35:54,445:INFO:Logistic Regression Imported successfully
2024-11-25 12:35:54,452:INFO:Starting cross validation
2024-11-25 12:35:54,457:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:35:57,670:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:35:57,709:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:35:57,711:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:35:57,731:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:35:57,748:INFO:Calculating mean and std
2024-11-25 12:35:57,750:INFO:Creating metrics dataframe
2024-11-25 12:35:57,752:INFO:Uploading results into container
2024-11-25 12:35:57,752:INFO:Uploading model into container now
2024-11-25 12:35:57,752:INFO:_master_model_container: 1
2024-11-25 12:35:57,752:INFO:_display_container: 2
2024-11-25 12:35:57,752:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:35:57,752:INFO:create_model() successfully completed......................................
2024-11-25 12:35:57,834:INFO:SubProcess create_model() end ==================================
2024-11-25 12:35:57,834:INFO:Creating metrics dataframe
2024-11-25 12:35:57,842:INFO:Initializing K Neighbors Classifier
2024-11-25 12:35:57,842:INFO:Total runtime is 0.05698837041854858 minutes
2024-11-25 12:35:57,842:INFO:SubProcess create_model() called ==================================
2024-11-25 12:35:57,852:INFO:Initializing create_model()
2024-11-25 12:35:57,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:35:57,852:INFO:Checking exceptions
2024-11-25 12:35:57,852:INFO:Importing libraries
2024-11-25 12:35:57,852:INFO:Copying training dataset
2024-11-25 12:35:57,854:INFO:Defining folds
2024-11-25 12:35:57,854:INFO:Declaring metric variables
2024-11-25 12:35:57,862:INFO:Importing untrained model
2024-11-25 12:35:57,864:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:35:57,874:INFO:Starting cross validation
2024-11-25 12:35:57,874:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:00,793:INFO:Calculating mean and std
2024-11-25 12:36:00,795:INFO:Creating metrics dataframe
2024-11-25 12:36:00,798:INFO:Uploading results into container
2024-11-25 12:36:00,800:INFO:Uploading model into container now
2024-11-25 12:36:00,801:INFO:_master_model_container: 2
2024-11-25 12:36:00,802:INFO:_display_container: 2
2024-11-25 12:36:00,802:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:36:00,803:INFO:create_model() successfully completed......................................
2024-11-25 12:36:00,905:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:00,905:INFO:Creating metrics dataframe
2024-11-25 12:36:00,911:INFO:Initializing Naive Bayes
2024-11-25 12:36:00,911:INFO:Total runtime is 0.10813584725062053 minutes
2024-11-25 12:36:00,914:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:00,914:INFO:Initializing create_model()
2024-11-25 12:36:00,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:00,915:INFO:Checking exceptions
2024-11-25 12:36:00,915:INFO:Importing libraries
2024-11-25 12:36:00,915:INFO:Copying training dataset
2024-11-25 12:36:00,920:INFO:Defining folds
2024-11-25 12:36:00,920:INFO:Declaring metric variables
2024-11-25 12:36:00,924:INFO:Importing untrained model
2024-11-25 12:36:00,928:INFO:Naive Bayes Imported successfully
2024-11-25 12:36:00,936:INFO:Starting cross validation
2024-11-25 12:36:00,942:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:01,224:INFO:Calculating mean and std
2024-11-25 12:36:01,225:INFO:Creating metrics dataframe
2024-11-25 12:36:01,227:INFO:Uploading results into container
2024-11-25 12:36:01,227:INFO:Uploading model into container now
2024-11-25 12:36:01,228:INFO:_master_model_container: 3
2024-11-25 12:36:01,228:INFO:_display_container: 2
2024-11-25 12:36:01,228:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:36:01,228:INFO:create_model() successfully completed......................................
2024-11-25 12:36:01,303:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:01,304:INFO:Creating metrics dataframe
2024-11-25 12:36:01,313:INFO:Initializing Decision Tree Classifier
2024-11-25 12:36:01,313:INFO:Total runtime is 0.11483920415242514 minutes
2024-11-25 12:36:01,317:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:01,317:INFO:Initializing create_model()
2024-11-25 12:36:01,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:01,318:INFO:Checking exceptions
2024-11-25 12:36:01,318:INFO:Importing libraries
2024-11-25 12:36:01,318:INFO:Copying training dataset
2024-11-25 12:36:01,323:INFO:Defining folds
2024-11-25 12:36:01,323:INFO:Declaring metric variables
2024-11-25 12:36:01,326:INFO:Importing untrained model
2024-11-25 12:36:01,332:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:36:01,344:INFO:Starting cross validation
2024-11-25 12:36:01,348:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:01,598:INFO:Calculating mean and std
2024-11-25 12:36:01,598:INFO:Creating metrics dataframe
2024-11-25 12:36:01,601:INFO:Uploading results into container
2024-11-25 12:36:01,602:INFO:Uploading model into container now
2024-11-25 12:36:01,603:INFO:_master_model_container: 4
2024-11-25 12:36:01,603:INFO:_display_container: 2
2024-11-25 12:36:01,604:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:36:01,604:INFO:create_model() successfully completed......................................
2024-11-25 12:36:01,675:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:01,676:INFO:Creating metrics dataframe
2024-11-25 12:36:01,683:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:36:01,683:INFO:Total runtime is 0.12100587288538617 minutes
2024-11-25 12:36:01,686:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:01,686:INFO:Initializing create_model()
2024-11-25 12:36:01,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:01,687:INFO:Checking exceptions
2024-11-25 12:36:01,687:INFO:Importing libraries
2024-11-25 12:36:01,688:INFO:Copying training dataset
2024-11-25 12:36:01,692:INFO:Defining folds
2024-11-25 12:36:01,692:INFO:Declaring metric variables
2024-11-25 12:36:01,695:INFO:Importing untrained model
2024-11-25 12:36:01,700:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:36:01,711:INFO:Starting cross validation
2024-11-25 12:36:01,715:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:01,924:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:01,931:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:01,946:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:01,957:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:01,967:INFO:Calculating mean and std
2024-11-25 12:36:01,967:INFO:Creating metrics dataframe
2024-11-25 12:36:01,970:INFO:Uploading results into container
2024-11-25 12:36:01,970:INFO:Uploading model into container now
2024-11-25 12:36:01,971:INFO:_master_model_container: 5
2024-11-25 12:36:01,971:INFO:_display_container: 2
2024-11-25 12:36:01,971:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:36:01,971:INFO:create_model() successfully completed......................................
2024-11-25 12:36:02,042:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:02,042:INFO:Creating metrics dataframe
2024-11-25 12:36:02,052:INFO:Initializing Ridge Classifier
2024-11-25 12:36:02,052:INFO:Total runtime is 0.1271558960278829 minutes
2024-11-25 12:36:02,056:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:02,057:INFO:Initializing create_model()
2024-11-25 12:36:02,057:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:02,057:INFO:Checking exceptions
2024-11-25 12:36:02,057:INFO:Importing libraries
2024-11-25 12:36:02,057:INFO:Copying training dataset
2024-11-25 12:36:02,061:INFO:Defining folds
2024-11-25 12:36:02,061:INFO:Declaring metric variables
2024-11-25 12:36:02,064:INFO:Importing untrained model
2024-11-25 12:36:02,068:INFO:Ridge Classifier Imported successfully
2024-11-25 12:36:02,076:INFO:Starting cross validation
2024-11-25 12:36:02,080:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:02,292:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:02,307:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:02,310:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:02,318:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:02,339:INFO:Calculating mean and std
2024-11-25 12:36:02,340:INFO:Creating metrics dataframe
2024-11-25 12:36:02,342:INFO:Uploading results into container
2024-11-25 12:36:02,343:INFO:Uploading model into container now
2024-11-25 12:36:02,343:INFO:_master_model_container: 6
2024-11-25 12:36:02,344:INFO:_display_container: 2
2024-11-25 12:36:02,344:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:02,344:INFO:create_model() successfully completed......................................
2024-11-25 12:36:02,422:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:02,422:INFO:Creating metrics dataframe
2024-11-25 12:36:02,430:INFO:Initializing Random Forest Classifier
2024-11-25 12:36:02,430:INFO:Total runtime is 0.1334558924039205 minutes
2024-11-25 12:36:02,434:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:02,435:INFO:Initializing create_model()
2024-11-25 12:36:02,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:02,435:INFO:Checking exceptions
2024-11-25 12:36:02,435:INFO:Importing libraries
2024-11-25 12:36:02,435:INFO:Copying training dataset
2024-11-25 12:36:02,440:INFO:Defining folds
2024-11-25 12:36:02,440:INFO:Declaring metric variables
2024-11-25 12:36:02,444:INFO:Importing untrained model
2024-11-25 12:36:02,448:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:36:02,457:INFO:Starting cross validation
2024-11-25 12:36:02,460:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:02,946:INFO:Calculating mean and std
2024-11-25 12:36:02,947:INFO:Creating metrics dataframe
2024-11-25 12:36:02,949:INFO:Uploading results into container
2024-11-25 12:36:02,950:INFO:Uploading model into container now
2024-11-25 12:36:02,950:INFO:_master_model_container: 7
2024-11-25 12:36:02,950:INFO:_display_container: 2
2024-11-25 12:36:02,951:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:36:02,951:INFO:create_model() successfully completed......................................
2024-11-25 12:36:03,029:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:03,029:INFO:Creating metrics dataframe
2024-11-25 12:36:03,041:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:36:03,041:INFO:Total runtime is 0.14363559484481814 minutes
2024-11-25 12:36:03,044:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:03,044:INFO:Initializing create_model()
2024-11-25 12:36:03,045:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:03,045:INFO:Checking exceptions
2024-11-25 12:36:03,045:INFO:Importing libraries
2024-11-25 12:36:03,045:INFO:Copying training dataset
2024-11-25 12:36:03,050:INFO:Defining folds
2024-11-25 12:36:03,050:INFO:Declaring metric variables
2024-11-25 12:36:03,053:INFO:Importing untrained model
2024-11-25 12:36:03,058:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:36:03,065:INFO:Starting cross validation
2024-11-25 12:36:03,068:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:03,231:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:03,237:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:03,237:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:03,240:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:03,289:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,293:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,300:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,316:INFO:Calculating mean and std
2024-11-25 12:36:03,317:INFO:Creating metrics dataframe
2024-11-25 12:36:03,319:INFO:Uploading results into container
2024-11-25 12:36:03,320:INFO:Uploading model into container now
2024-11-25 12:36:03,320:INFO:_master_model_container: 8
2024-11-25 12:36:03,320:INFO:_display_container: 2
2024-11-25 12:36:03,320:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:36:03,321:INFO:create_model() successfully completed......................................
2024-11-25 12:36:03,394:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:03,395:INFO:Creating metrics dataframe
2024-11-25 12:36:03,402:INFO:Initializing Ada Boost Classifier
2024-11-25 12:36:03,403:INFO:Total runtime is 0.14966119527816774 minutes
2024-11-25 12:36:03,407:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:03,407:INFO:Initializing create_model()
2024-11-25 12:36:03,407:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:03,408:INFO:Checking exceptions
2024-11-25 12:36:03,408:INFO:Importing libraries
2024-11-25 12:36:03,408:INFO:Copying training dataset
2024-11-25 12:36:03,412:INFO:Defining folds
2024-11-25 12:36:03,412:INFO:Declaring metric variables
2024-11-25 12:36:03,416:INFO:Importing untrained model
2024-11-25 12:36:03,419:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:36:03,427:INFO:Starting cross validation
2024-11-25 12:36:03,432:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:03,578:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:03,585:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:03,585:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:03,587:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:03,724:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,727:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,735:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,737:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:03,757:INFO:Calculating mean and std
2024-11-25 12:36:03,757:INFO:Creating metrics dataframe
2024-11-25 12:36:03,760:INFO:Uploading results into container
2024-11-25 12:36:03,760:INFO:Uploading model into container now
2024-11-25 12:36:03,760:INFO:_master_model_container: 9
2024-11-25 12:36:03,761:INFO:_display_container: 2
2024-11-25 12:36:03,761:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:36:03,761:INFO:create_model() successfully completed......................................
2024-11-25 12:36:03,832:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:03,832:INFO:Creating metrics dataframe
2024-11-25 12:36:03,841:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:36:03,841:INFO:Total runtime is 0.15698081652323406 minutes
2024-11-25 12:36:03,844:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:03,844:INFO:Initializing create_model()
2024-11-25 12:36:03,845:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:03,845:INFO:Checking exceptions
2024-11-25 12:36:03,845:INFO:Importing libraries
2024-11-25 12:36:03,845:INFO:Copying training dataset
2024-11-25 12:36:03,849:INFO:Defining folds
2024-11-25 12:36:03,849:INFO:Declaring metric variables
2024-11-25 12:36:03,853:INFO:Importing untrained model
2024-11-25 12:36:03,857:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:36:03,865:INFO:Starting cross validation
2024-11-25 12:36:03,867:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:04,401:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,403:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,410:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,457:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,471:INFO:Calculating mean and std
2024-11-25 12:36:04,472:INFO:Creating metrics dataframe
2024-11-25 12:36:04,474:INFO:Uploading results into container
2024-11-25 12:36:04,475:INFO:Uploading model into container now
2024-11-25 12:36:04,476:INFO:_master_model_container: 10
2024-11-25 12:36:04,476:INFO:_display_container: 2
2024-11-25 12:36:04,477:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:36:04,477:INFO:create_model() successfully completed......................................
2024-11-25 12:36:04,546:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:04,546:INFO:Creating metrics dataframe
2024-11-25 12:36:04,553:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:36:04,553:INFO:Total runtime is 0.16883366107940675 minutes
2024-11-25 12:36:04,556:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:04,556:INFO:Initializing create_model()
2024-11-25 12:36:04,556:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:04,557:INFO:Checking exceptions
2024-11-25 12:36:04,557:INFO:Importing libraries
2024-11-25 12:36:04,557:INFO:Copying training dataset
2024-11-25 12:36:04,562:INFO:Defining folds
2024-11-25 12:36:04,563:INFO:Declaring metric variables
2024-11-25 12:36:04,566:INFO:Importing untrained model
2024-11-25 12:36:04,570:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:36:04,579:INFO:Starting cross validation
2024-11-25 12:36:04,583:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:04,776:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,783:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,787:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,794:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:04,809:INFO:Calculating mean and std
2024-11-25 12:36:04,810:INFO:Creating metrics dataframe
2024-11-25 12:36:04,812:INFO:Uploading results into container
2024-11-25 12:36:04,812:INFO:Uploading model into container now
2024-11-25 12:36:04,812:INFO:_master_model_container: 11
2024-11-25 12:36:04,813:INFO:_display_container: 2
2024-11-25 12:36:04,813:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:36:04,813:INFO:create_model() successfully completed......................................
2024-11-25 12:36:04,882:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:04,883:INFO:Creating metrics dataframe
2024-11-25 12:36:04,894:INFO:Initializing Extra Trees Classifier
2024-11-25 12:36:04,894:INFO:Total runtime is 0.17451632420221966 minutes
2024-11-25 12:36:04,897:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:04,898:INFO:Initializing create_model()
2024-11-25 12:36:04,898:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:04,899:INFO:Checking exceptions
2024-11-25 12:36:04,899:INFO:Importing libraries
2024-11-25 12:36:04,899:INFO:Copying training dataset
2024-11-25 12:36:04,903:INFO:Defining folds
2024-11-25 12:36:04,903:INFO:Declaring metric variables
2024-11-25 12:36:04,907:INFO:Importing untrained model
2024-11-25 12:36:04,911:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:36:04,919:INFO:Starting cross validation
2024-11-25 12:36:04,922:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:05,461:INFO:Calculating mean and std
2024-11-25 12:36:05,461:INFO:Creating metrics dataframe
2024-11-25 12:36:05,463:INFO:Uploading results into container
2024-11-25 12:36:05,464:INFO:Uploading model into container now
2024-11-25 12:36:05,464:INFO:_master_model_container: 12
2024-11-25 12:36:05,465:INFO:_display_container: 2
2024-11-25 12:36:05,465:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:36:05,465:INFO:create_model() successfully completed......................................
2024-11-25 12:36:05,535:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:05,535:INFO:Creating metrics dataframe
2024-11-25 12:36:05,545:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:36:05,545:INFO:Total runtime is 0.1853742043177287 minutes
2024-11-25 12:36:05,549:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:05,550:INFO:Initializing create_model()
2024-11-25 12:36:05,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:05,550:INFO:Checking exceptions
2024-11-25 12:36:05,550:INFO:Importing libraries
2024-11-25 12:36:05,550:INFO:Copying training dataset
2024-11-25 12:36:05,555:INFO:Defining folds
2024-11-25 12:36:05,555:INFO:Declaring metric variables
2024-11-25 12:36:05,559:INFO:Importing untrained model
2024-11-25 12:36:05,564:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:36:05,572:INFO:Starting cross validation
2024-11-25 12:36:05,577:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:06,328:INFO:Calculating mean and std
2024-11-25 12:36:06,329:INFO:Creating metrics dataframe
2024-11-25 12:36:06,332:INFO:Uploading results into container
2024-11-25 12:36:06,332:INFO:Uploading model into container now
2024-11-25 12:36:06,333:INFO:_master_model_container: 13
2024-11-25 12:36:06,333:INFO:_display_container: 2
2024-11-25 12:36:06,334:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:36:06,335:INFO:create_model() successfully completed......................................
2024-11-25 12:36:06,430:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:06,430:INFO:Creating metrics dataframe
2024-11-25 12:36:06,439:INFO:Initializing Dummy Classifier
2024-11-25 12:36:06,439:INFO:Total runtime is 0.20027796824773153 minutes
2024-11-25 12:36:06,442:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:06,442:INFO:Initializing create_model()
2024-11-25 12:36:06,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4035DF250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:06,443:INFO:Checking exceptions
2024-11-25 12:36:06,443:INFO:Importing libraries
2024-11-25 12:36:06,443:INFO:Copying training dataset
2024-11-25 12:36:06,448:INFO:Defining folds
2024-11-25 12:36:06,448:INFO:Declaring metric variables
2024-11-25 12:36:06,451:INFO:Importing untrained model
2024-11-25 12:36:06,455:INFO:Dummy Classifier Imported successfully
2024-11-25 12:36:06,463:INFO:Starting cross validation
2024-11-25 12:36:06,467:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:06,672:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:06,678:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:06,680:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:06,681:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:06,698:INFO:Calculating mean and std
2024-11-25 12:36:06,698:INFO:Creating metrics dataframe
2024-11-25 12:36:06,700:INFO:Uploading results into container
2024-11-25 12:36:06,701:INFO:Uploading model into container now
2024-11-25 12:36:06,701:INFO:_master_model_container: 14
2024-11-25 12:36:06,701:INFO:_display_container: 2
2024-11-25 12:36:06,701:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:36:06,701:INFO:create_model() successfully completed......................................
2024-11-25 12:36:06,777:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:06,777:INFO:Creating metrics dataframe
2024-11-25 12:36:06,792:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:36:06,799:INFO:Initializing create_model()
2024-11-25 12:36:06,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:06,799:INFO:Checking exceptions
2024-11-25 12:36:06,801:INFO:Importing libraries
2024-11-25 12:36:06,801:INFO:Copying training dataset
2024-11-25 12:36:06,809:INFO:Defining folds
2024-11-25 12:36:06,809:INFO:Declaring metric variables
2024-11-25 12:36:06,809:INFO:Importing untrained model
2024-11-25 12:36:06,809:INFO:Declaring custom model
2024-11-25 12:36:06,809:INFO:Ridge Classifier Imported successfully
2024-11-25 12:36:06,812:INFO:Cross validation set to False
2024-11-25 12:36:06,812:INFO:Fitting Model
2024-11-25 12:36:06,918:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:06,918:INFO:create_model() successfully completed......................................
2024-11-25 12:36:07,008:INFO:_master_model_container: 14
2024-11-25 12:36:07,008:INFO:_display_container: 2
2024-11-25 12:36:07,008:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:07,008:INFO:compare_models() successfully completed......................................
2024-11-25 12:36:07,013:INFO:Initializing compare_models()
2024-11-25 12:36:07,014:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:36:07,014:INFO:Checking exceptions
2024-11-25 12:36:07,018:INFO:Preparing display monitor
2024-11-25 12:36:07,049:INFO:Initializing Logistic Regression
2024-11-25 12:36:07,050:INFO:Total runtime is 1.651446024576823e-05 minutes
2024-11-25 12:36:07,055:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:07,055:INFO:Initializing create_model()
2024-11-25 12:36:07,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:07,056:INFO:Checking exceptions
2024-11-25 12:36:07,056:INFO:Importing libraries
2024-11-25 12:36:07,057:INFO:Copying training dataset
2024-11-25 12:36:07,061:INFO:Defining folds
2024-11-25 12:36:07,061:INFO:Declaring metric variables
2024-11-25 12:36:07,065:INFO:Importing untrained model
2024-11-25 12:36:07,068:INFO:Logistic Regression Imported successfully
2024-11-25 12:36:07,076:INFO:Starting cross validation
2024-11-25 12:36:07,079:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:07,476:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,487:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,500:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,505:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,506:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,512:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,515:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,544:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,693:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,696:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:07,712:INFO:Calculating mean and std
2024-11-25 12:36:07,712:INFO:Creating metrics dataframe
2024-11-25 12:36:07,714:INFO:Uploading results into container
2024-11-25 12:36:07,714:INFO:Uploading model into container now
2024-11-25 12:36:07,715:INFO:_master_model_container: 1
2024-11-25 12:36:07,715:INFO:_display_container: 2
2024-11-25 12:36:07,715:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:36:07,715:INFO:create_model() successfully completed......................................
2024-11-25 12:36:07,779:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:07,779:INFO:Creating metrics dataframe
2024-11-25 12:36:07,784:INFO:Initializing K Neighbors Classifier
2024-11-25 12:36:07,784:INFO:Total runtime is 0.012249986330668133 minutes
2024-11-25 12:36:07,787:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:07,788:INFO:Initializing create_model()
2024-11-25 12:36:07,788:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:07,788:INFO:Checking exceptions
2024-11-25 12:36:07,788:INFO:Importing libraries
2024-11-25 12:36:07,788:INFO:Copying training dataset
2024-11-25 12:36:07,792:INFO:Defining folds
2024-11-25 12:36:07,792:INFO:Declaring metric variables
2024-11-25 12:36:07,796:INFO:Importing untrained model
2024-11-25 12:36:07,801:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:36:07,813:INFO:Starting cross validation
2024-11-25 12:36:07,816:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:08,480:INFO:Calculating mean and std
2024-11-25 12:36:08,481:INFO:Creating metrics dataframe
2024-11-25 12:36:08,482:INFO:Uploading results into container
2024-11-25 12:36:08,483:INFO:Uploading model into container now
2024-11-25 12:36:08,483:INFO:_master_model_container: 2
2024-11-25 12:36:08,483:INFO:_display_container: 2
2024-11-25 12:36:08,483:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:36:08,484:INFO:create_model() successfully completed......................................
2024-11-25 12:36:08,560:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:08,560:INFO:Creating metrics dataframe
2024-11-25 12:36:08,569:INFO:Initializing Naive Bayes
2024-11-25 12:36:08,569:INFO:Total runtime is 0.025332117080688478 minutes
2024-11-25 12:36:08,575:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:08,576:INFO:Initializing create_model()
2024-11-25 12:36:08,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:08,576:INFO:Checking exceptions
2024-11-25 12:36:08,576:INFO:Importing libraries
2024-11-25 12:36:08,576:INFO:Copying training dataset
2024-11-25 12:36:08,583:INFO:Defining folds
2024-11-25 12:36:08,584:INFO:Declaring metric variables
2024-11-25 12:36:08,588:INFO:Importing untrained model
2024-11-25 12:36:08,592:INFO:Naive Bayes Imported successfully
2024-11-25 12:36:08,600:INFO:Starting cross validation
2024-11-25 12:36:08,604:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:09,180:INFO:Calculating mean and std
2024-11-25 12:36:09,181:INFO:Creating metrics dataframe
2024-11-25 12:36:09,183:INFO:Uploading results into container
2024-11-25 12:36:09,184:INFO:Uploading model into container now
2024-11-25 12:36:09,184:INFO:_master_model_container: 3
2024-11-25 12:36:09,184:INFO:_display_container: 2
2024-11-25 12:36:09,185:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:36:09,185:INFO:create_model() successfully completed......................................
2024-11-25 12:36:09,255:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:09,255:INFO:Creating metrics dataframe
2024-11-25 12:36:09,261:INFO:Initializing Decision Tree Classifier
2024-11-25 12:36:09,261:INFO:Total runtime is 0.03686556418736776 minutes
2024-11-25 12:36:09,265:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:09,265:INFO:Initializing create_model()
2024-11-25 12:36:09,265:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:09,265:INFO:Checking exceptions
2024-11-25 12:36:09,265:INFO:Importing libraries
2024-11-25 12:36:09,265:INFO:Copying training dataset
2024-11-25 12:36:09,271:INFO:Defining folds
2024-11-25 12:36:09,271:INFO:Declaring metric variables
2024-11-25 12:36:09,275:INFO:Importing untrained model
2024-11-25 12:36:09,279:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:36:09,286:INFO:Starting cross validation
2024-11-25 12:36:09,291:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:09,900:INFO:Calculating mean and std
2024-11-25 12:36:09,900:INFO:Creating metrics dataframe
2024-11-25 12:36:09,900:INFO:Uploading results into container
2024-11-25 12:36:09,900:INFO:Uploading model into container now
2024-11-25 12:36:09,906:INFO:_master_model_container: 4
2024-11-25 12:36:09,906:INFO:_display_container: 2
2024-11-25 12:36:09,906:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:36:09,906:INFO:create_model() successfully completed......................................
2024-11-25 12:36:09,985:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:09,985:INFO:Creating metrics dataframe
2024-11-25 12:36:09,985:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:36:09,985:INFO:Total runtime is 0.04891977310180664 minutes
2024-11-25 12:36:09,985:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:09,985:INFO:Initializing create_model()
2024-11-25 12:36:09,985:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:09,985:INFO:Checking exceptions
2024-11-25 12:36:09,985:INFO:Importing libraries
2024-11-25 12:36:09,985:INFO:Copying training dataset
2024-11-25 12:36:10,000:INFO:Defining folds
2024-11-25 12:36:10,000:INFO:Declaring metric variables
2024-11-25 12:36:10,006:INFO:Importing untrained model
2024-11-25 12:36:10,006:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:36:10,006:INFO:Starting cross validation
2024-11-25 12:36:10,021:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:10,412:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,423:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,423:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,423:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,432:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,434:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,434:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,462:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,628:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,628:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:10,648:INFO:Calculating mean and std
2024-11-25 12:36:10,648:INFO:Creating metrics dataframe
2024-11-25 12:36:10,648:INFO:Uploading results into container
2024-11-25 12:36:10,648:INFO:Uploading model into container now
2024-11-25 12:36:10,648:INFO:_master_model_container: 5
2024-11-25 12:36:10,648:INFO:_display_container: 2
2024-11-25 12:36:10,648:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:36:10,648:INFO:create_model() successfully completed......................................
2024-11-25 12:36:10,713:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:10,713:INFO:Creating metrics dataframe
2024-11-25 12:36:10,713:INFO:Initializing Ridge Classifier
2024-11-25 12:36:10,713:INFO:Total runtime is 0.061065173149108885 minutes
2024-11-25 12:36:10,729:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:10,729:INFO:Initializing create_model()
2024-11-25 12:36:10,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:10,729:INFO:Checking exceptions
2024-11-25 12:36:10,729:INFO:Importing libraries
2024-11-25 12:36:10,729:INFO:Copying training dataset
2024-11-25 12:36:10,729:INFO:Defining folds
2024-11-25 12:36:10,729:INFO:Declaring metric variables
2024-11-25 12:36:10,729:INFO:Importing untrained model
2024-11-25 12:36:10,729:INFO:Ridge Classifier Imported successfully
2024-11-25 12:36:10,745:INFO:Starting cross validation
2024-11-25 12:36:10,745:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:11,155:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,163:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,163:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,165:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,176:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,186:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,196:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,206:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,338:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,338:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:11,365:INFO:Calculating mean and std
2024-11-25 12:36:11,367:INFO:Creating metrics dataframe
2024-11-25 12:36:11,367:INFO:Uploading results into container
2024-11-25 12:36:11,367:INFO:Uploading model into container now
2024-11-25 12:36:11,367:INFO:_master_model_container: 6
2024-11-25 12:36:11,367:INFO:_display_container: 2
2024-11-25 12:36:11,367:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:11,367:INFO:create_model() successfully completed......................................
2024-11-25 12:36:11,429:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:11,429:INFO:Creating metrics dataframe
2024-11-25 12:36:11,429:INFO:Initializing Random Forest Classifier
2024-11-25 12:36:11,429:INFO:Total runtime is 0.07299619913101196 minutes
2024-11-25 12:36:11,445:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:11,445:INFO:Initializing create_model()
2024-11-25 12:36:11,445:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:11,445:INFO:Checking exceptions
2024-11-25 12:36:11,445:INFO:Importing libraries
2024-11-25 12:36:11,445:INFO:Copying training dataset
2024-11-25 12:36:11,445:INFO:Defining folds
2024-11-25 12:36:11,445:INFO:Declaring metric variables
2024-11-25 12:36:11,445:INFO:Importing untrained model
2024-11-25 12:36:11,445:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:36:11,467:INFO:Starting cross validation
2024-11-25 12:36:11,467:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:12,677:INFO:Calculating mean and std
2024-11-25 12:36:12,677:INFO:Creating metrics dataframe
2024-11-25 12:36:12,677:INFO:Uploading results into container
2024-11-25 12:36:12,677:INFO:Uploading model into container now
2024-11-25 12:36:12,677:INFO:_master_model_container: 7
2024-11-25 12:36:12,677:INFO:_display_container: 2
2024-11-25 12:36:12,677:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:36:12,677:INFO:create_model() successfully completed......................................
2024-11-25 12:36:12,739:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:12,739:INFO:Creating metrics dataframe
2024-11-25 12:36:12,755:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:36:12,755:INFO:Total runtime is 0.09509005149205525 minutes
2024-11-25 12:36:12,759:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:12,759:INFO:Initializing create_model()
2024-11-25 12:36:12,759:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:12,759:INFO:Checking exceptions
2024-11-25 12:36:12,759:INFO:Importing libraries
2024-11-25 12:36:12,759:INFO:Copying training dataset
2024-11-25 12:36:12,759:INFO:Defining folds
2024-11-25 12:36:12,759:INFO:Declaring metric variables
2024-11-25 12:36:12,759:INFO:Importing untrained model
2024-11-25 12:36:12,759:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:36:12,774:INFO:Starting cross validation
2024-11-25 12:36:12,774:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:13,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,071:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,073:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,073:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,084:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,094:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,102:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,102:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,165:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,172:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,175:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,175:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,196:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,316:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,316:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:36:13,356:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,358:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:13,368:INFO:Calculating mean and std
2024-11-25 12:36:13,369:INFO:Creating metrics dataframe
2024-11-25 12:36:13,370:INFO:Uploading results into container
2024-11-25 12:36:13,371:INFO:Uploading model into container now
2024-11-25 12:36:13,371:INFO:_master_model_container: 8
2024-11-25 12:36:13,371:INFO:_display_container: 2
2024-11-25 12:36:13,371:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:36:13,371:INFO:create_model() successfully completed......................................
2024-11-25 12:36:13,439:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:13,440:INFO:Creating metrics dataframe
2024-11-25 12:36:13,448:INFO:Initializing Ada Boost Classifier
2024-11-25 12:36:13,448:INFO:Total runtime is 0.10664857625961303 minutes
2024-11-25 12:36:13,452:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:13,453:INFO:Initializing create_model()
2024-11-25 12:36:13,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:13,453:INFO:Checking exceptions
2024-11-25 12:36:13,453:INFO:Importing libraries
2024-11-25 12:36:13,453:INFO:Copying training dataset
2024-11-25 12:36:13,458:INFO:Defining folds
2024-11-25 12:36:13,458:INFO:Declaring metric variables
2024-11-25 12:36:13,461:INFO:Importing untrained model
2024-11-25 12:36:13,464:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:36:13,475:INFO:Starting cross validation
2024-11-25 12:36:13,479:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:13,783:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,792:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,799:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,816:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,819:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,824:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,825:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:13,847:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:14,107:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,109:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,121:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,130:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,149:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,162:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,167:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,175:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,268:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:14,274:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:36:14,396:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,409:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:14,428:INFO:Calculating mean and std
2024-11-25 12:36:14,429:INFO:Creating metrics dataframe
2024-11-25 12:36:14,431:INFO:Uploading results into container
2024-11-25 12:36:14,431:INFO:Uploading model into container now
2024-11-25 12:36:14,432:INFO:_master_model_container: 9
2024-11-25 12:36:14,432:INFO:_display_container: 2
2024-11-25 12:36:14,433:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:36:14,433:INFO:create_model() successfully completed......................................
2024-11-25 12:36:14,501:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:14,501:INFO:Creating metrics dataframe
2024-11-25 12:36:14,509:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:36:14,509:INFO:Total runtime is 0.1243343989054362 minutes
2024-11-25 12:36:14,513:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:14,514:INFO:Initializing create_model()
2024-11-25 12:36:14,514:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:14,514:INFO:Checking exceptions
2024-11-25 12:36:14,514:INFO:Importing libraries
2024-11-25 12:36:14,514:INFO:Copying training dataset
2024-11-25 12:36:14,518:INFO:Defining folds
2024-11-25 12:36:14,519:INFO:Declaring metric variables
2024-11-25 12:36:14,523:INFO:Importing untrained model
2024-11-25 12:36:14,528:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:36:14,537:INFO:Starting cross validation
2024-11-25 12:36:14,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:15,596:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,610:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,632:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,635:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,649:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,652:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,654:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:15,664:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,114:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,117:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,130:INFO:Calculating mean and std
2024-11-25 12:36:16,130:INFO:Creating metrics dataframe
2024-11-25 12:36:16,133:INFO:Uploading results into container
2024-11-25 12:36:16,133:INFO:Uploading model into container now
2024-11-25 12:36:16,133:INFO:_master_model_container: 10
2024-11-25 12:36:16,133:INFO:_display_container: 2
2024-11-25 12:36:16,134:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:36:16,134:INFO:create_model() successfully completed......................................
2024-11-25 12:36:16,206:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:16,206:INFO:Creating metrics dataframe
2024-11-25 12:36:16,214:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:36:16,214:INFO:Total runtime is 0.15274993975957235 minutes
2024-11-25 12:36:16,218:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:16,219:INFO:Initializing create_model()
2024-11-25 12:36:16,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:16,219:INFO:Checking exceptions
2024-11-25 12:36:16,219:INFO:Importing libraries
2024-11-25 12:36:16,219:INFO:Copying training dataset
2024-11-25 12:36:16,225:INFO:Defining folds
2024-11-25 12:36:16,225:INFO:Declaring metric variables
2024-11-25 12:36:16,229:INFO:Importing untrained model
2024-11-25 12:36:16,234:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:36:16,245:INFO:Starting cross validation
2024-11-25 12:36:16,248:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:16,629:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,637:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,638:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,648:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,658:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,660:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,661:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,679:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,874:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,877:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:36:16,895:INFO:Calculating mean and std
2024-11-25 12:36:16,895:INFO:Creating metrics dataframe
2024-11-25 12:36:16,897:INFO:Uploading results into container
2024-11-25 12:36:16,898:INFO:Uploading model into container now
2024-11-25 12:36:16,899:INFO:_master_model_container: 11
2024-11-25 12:36:16,899:INFO:_display_container: 2
2024-11-25 12:36:16,899:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:36:16,899:INFO:create_model() successfully completed......................................
2024-11-25 12:36:16,972:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:16,972:INFO:Creating metrics dataframe
2024-11-25 12:36:16,980:INFO:Initializing Extra Trees Classifier
2024-11-25 12:36:16,981:INFO:Total runtime is 0.1655177434285482 minutes
2024-11-25 12:36:16,984:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:16,984:INFO:Initializing create_model()
2024-11-25 12:36:16,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:16,984:INFO:Checking exceptions
2024-11-25 12:36:16,984:INFO:Importing libraries
2024-11-25 12:36:16,984:INFO:Copying training dataset
2024-11-25 12:36:16,988:INFO:Defining folds
2024-11-25 12:36:16,989:INFO:Declaring metric variables
2024-11-25 12:36:16,992:INFO:Importing untrained model
2024-11-25 12:36:16,997:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:36:17,006:INFO:Starting cross validation
2024-11-25 12:36:17,009:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:18,182:INFO:Calculating mean and std
2024-11-25 12:36:18,183:INFO:Creating metrics dataframe
2024-11-25 12:36:18,185:INFO:Uploading results into container
2024-11-25 12:36:18,186:INFO:Uploading model into container now
2024-11-25 12:36:18,186:INFO:_master_model_container: 12
2024-11-25 12:36:18,186:INFO:_display_container: 2
2024-11-25 12:36:18,187:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:36:18,187:INFO:create_model() successfully completed......................................
2024-11-25 12:36:18,256:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:18,256:INFO:Creating metrics dataframe
2024-11-25 12:36:18,264:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:36:18,265:INFO:Total runtime is 0.18692883650461833 minutes
2024-11-25 12:36:18,268:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:18,268:INFO:Initializing create_model()
2024-11-25 12:36:18,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:18,269:INFO:Checking exceptions
2024-11-25 12:36:18,269:INFO:Importing libraries
2024-11-25 12:36:18,269:INFO:Copying training dataset
2024-11-25 12:36:18,273:INFO:Defining folds
2024-11-25 12:36:18,273:INFO:Declaring metric variables
2024-11-25 12:36:18,277:INFO:Importing untrained model
2024-11-25 12:36:18,281:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:36:18,290:INFO:Starting cross validation
2024-11-25 12:36:18,295:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:20,444:INFO:Calculating mean and std
2024-11-25 12:36:20,446:INFO:Creating metrics dataframe
2024-11-25 12:36:20,449:INFO:Uploading results into container
2024-11-25 12:36:20,450:INFO:Uploading model into container now
2024-11-25 12:36:20,451:INFO:_master_model_container: 13
2024-11-25 12:36:20,451:INFO:_display_container: 2
2024-11-25 12:36:20,452:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:36:20,452:INFO:create_model() successfully completed......................................
2024-11-25 12:36:20,551:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:20,551:INFO:Creating metrics dataframe
2024-11-25 12:36:20,560:INFO:Initializing Dummy Classifier
2024-11-25 12:36:20,560:INFO:Total runtime is 0.2251702388127645 minutes
2024-11-25 12:36:20,563:INFO:SubProcess create_model() called ==================================
2024-11-25 12:36:20,563:INFO:Initializing create_model()
2024-11-25 12:36:20,563:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A4036030D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:20,563:INFO:Checking exceptions
2024-11-25 12:36:20,563:INFO:Importing libraries
2024-11-25 12:36:20,563:INFO:Copying training dataset
2024-11-25 12:36:20,568:INFO:Defining folds
2024-11-25 12:36:20,569:INFO:Declaring metric variables
2024-11-25 12:36:20,573:INFO:Importing untrained model
2024-11-25 12:36:20,576:INFO:Dummy Classifier Imported successfully
2024-11-25 12:36:20,584:INFO:Starting cross validation
2024-11-25 12:36:20,587:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:36:20,985:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:20,990:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:20,994:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,002:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,003:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,006:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,016:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,030:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,194:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:36:21,204:INFO:Calculating mean and std
2024-11-25 12:36:21,205:INFO:Creating metrics dataframe
2024-11-25 12:36:21,207:INFO:Uploading results into container
2024-11-25 12:36:21,208:INFO:Uploading model into container now
2024-11-25 12:36:21,209:INFO:_master_model_container: 14
2024-11-25 12:36:21,209:INFO:_display_container: 2
2024-11-25 12:36:21,209:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:36:21,210:INFO:create_model() successfully completed......................................
2024-11-25 12:36:21,284:INFO:SubProcess create_model() end ==================================
2024-11-25 12:36:21,284:INFO:Creating metrics dataframe
2024-11-25 12:36:21,294:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:36:21,302:INFO:Initializing create_model()
2024-11-25 12:36:21,302:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A40360B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:21,302:INFO:Checking exceptions
2024-11-25 12:36:21,305:INFO:Importing libraries
2024-11-25 12:36:21,305:INFO:Copying training dataset
2024-11-25 12:36:21,309:INFO:Defining folds
2024-11-25 12:36:21,310:INFO:Declaring metric variables
2024-11-25 12:36:21,310:INFO:Importing untrained model
2024-11-25 12:36:21,310:INFO:Declaring custom model
2024-11-25 12:36:21,311:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:36:21,313:INFO:Cross validation set to False
2024-11-25 12:36:21,313:INFO:Fitting Model
2024-11-25 12:36:21,433:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:36:21,434:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.
2024-11-25 12:36:21,434:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-25 12:36:21,434:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:36:21,435:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:36:21,435:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:36:21,435:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:36:21,435:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:36:21,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:36:21,531:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:36:21,531:INFO:create_model() successfully completed......................................
2024-11-25 12:36:21,672:INFO:_master_model_container: 14
2024-11-25 12:36:21,672:INFO:_display_container: 2
2024-11-25 12:36:21,673:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:36:21,673:INFO:compare_models() successfully completed......................................
2024-11-25 12:36:21,679:INFO:Initializing evaluate_model()
2024-11-25 12:36:21,679:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:36:21,694:INFO:Initializing plot_model()
2024-11-25 12:36:21,694:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:36:21,694:INFO:Checking exceptions
2024-11-25 12:36:21,696:INFO:Preloading libraries
2024-11-25 12:36:21,698:INFO:Copying training dataset
2024-11-25 12:36:21,698:INFO:Plot type: pipeline
2024-11-25 12:36:21,868:INFO:Visual Rendered Successfully
2024-11-25 12:36:21,936:INFO:plot_model() successfully completed......................................
2024-11-25 12:36:21,945:INFO:Initializing plot_model()
2024-11-25 12:36:21,945:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:36:21,945:INFO:Checking exceptions
2024-11-25 12:36:21,951:INFO:Preloading libraries
2024-11-25 12:36:21,951:INFO:Copying training dataset
2024-11-25 12:36:21,951:INFO:Plot type: confusion_matrix
2024-11-25 12:36:22,178:INFO:Fitting Model
2024-11-25 12:36:22,179:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:36:22,179:INFO:Scoring test/hold-out set
2024-11-25 12:36:22,302:INFO:Visual Rendered Successfully
2024-11-25 12:36:22,374:INFO:plot_model() successfully completed......................................
2024-11-25 12:36:22,379:INFO:Initializing predict_model()
2024-11-25 12:36:22,379:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A403711080>)
2024-11-25 12:36:22,380:INFO:Checking exceptions
2024-11-25 12:36:22,380:INFO:Preloading libraries
2024-11-25 12:36:22,383:INFO:Set up data.
2024-11-25 12:36:22,391:INFO:Set up index.
2024-11-25 12:36:22,473:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:36:22,570:INFO:Initializing finalize_model()
2024-11-25 12:36:22,571:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:36:22,571:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:22,575:INFO:Initializing create_model()
2024-11-25 12:36:22,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:22,575:INFO:Checking exceptions
2024-11-25 12:36:22,577:INFO:Importing libraries
2024-11-25 12:36:22,577:INFO:Copying training dataset
2024-11-25 12:36:22,577:INFO:Defining folds
2024-11-25 12:36:22,578:INFO:Declaring metric variables
2024-11-25 12:36:22,578:INFO:Importing untrained model
2024-11-25 12:36:22,578:INFO:Declaring custom model
2024-11-25 12:36:22,578:INFO:Ridge Classifier Imported successfully
2024-11-25 12:36:22,581:INFO:Cross validation set to False
2024-11-25 12:36:22,581:INFO:Fitting Model
2024-11-25 12:36:22,695:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:36:22,695:INFO:create_model() successfully completed......................................
2024-11-25 12:36:22,769:INFO:_master_model_container: 14
2024-11-25 12:36:22,770:INFO:_display_container: 3
2024-11-25 12:36:22,775:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:36:22,775:INFO:finalize_model() successfully completed......................................
2024-11-25 12:36:22,847:INFO:Initializing predict_model()
2024-11-25 12:36:22,847:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A4037134C0>)
2024-11-25 12:36:22,847:INFO:Checking exceptions
2024-11-25 12:36:22,847:INFO:Preloading libraries
2024-11-25 12:36:22,848:INFO:Set up data.
2024-11-25 12:36:22,855:INFO:Set up index.
2024-11-25 12:36:22,947:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:36:23,045:INFO:Initializing save_model()
2024-11-25 12:36:23,045:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-25 12:36:23,046:INFO:Adding model into prep_pipe
2024-11-25 12:36:23,064:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-25 12:36:23,073:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:36:23,073:INFO:save_model() successfully completed......................................
2024-11-25 12:36:47,052:INFO:Initializing finalize_model()
2024-11-25 12:36:47,052:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:36:47,052:INFO:Finalizing RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:36:47,056:INFO:Initializing create_model()
2024-11-25 12:36:47,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:36:47,056:INFO:Checking exceptions
2024-11-25 12:36:47,060:INFO:Importing libraries
2024-11-25 12:36:47,060:INFO:Copying training dataset
2024-11-25 12:36:47,060:INFO:Defining folds
2024-11-25 12:36:47,060:INFO:Declaring metric variables
2024-11-25 12:36:47,060:INFO:Importing untrained model
2024-11-25 12:36:47,060:INFO:Declaring custom model
2024-11-25 12:36:47,060:INFO:Ridge Classifier Imported successfully
2024-11-25 12:36:47,060:INFO:Cross validation set to False
2024-11-25 12:36:47,060:INFO:Fitting Model
2024-11-25 12:36:47,184:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:36:47,184:INFO:create_model() successfully completed......................................
2024-11-25 12:36:47,256:INFO:_master_model_container: 14
2024-11-25 12:36:47,256:INFO:_display_container: 4
2024-11-25 12:36:47,260:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:36:47,260:INFO:finalize_model() successfully completed......................................
2024-11-25 12:36:47,336:INFO:Initializing predict_model()
2024-11-25 12:36:47,336:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A4032E7BA0>)
2024-11-25 12:36:47,336:INFO:Checking exceptions
2024-11-25 12:36:47,336:INFO:Preloading libraries
2024-11-25 12:36:47,336:INFO:Set up data.
2024-11-25 12:36:47,344:INFO:Set up index.
2024-11-25 12:36:47,384:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:41:24,925:INFO:Initializing tune_model()
2024-11-25 12:41:24,925:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:41:24,925:INFO:Checking exceptions
2024-11-25 12:41:39,161:INFO:Initializing tune_model()
2024-11-25 12:41:39,161:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:41:39,165:INFO:Checking exceptions
2024-11-25 12:44:13,603:INFO:Initializing create_model()
2024-11-25 12:44:13,603:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:44:13,603:INFO:Checking exceptions
2024-11-25 12:44:13,620:INFO:Importing libraries
2024-11-25 12:44:13,620:INFO:Copying training dataset
2024-11-25 12:44:13,627:INFO:Defining folds
2024-11-25 12:44:13,628:INFO:Declaring metric variables
2024-11-25 12:44:13,632:INFO:Importing untrained model
2024-11-25 12:44:13,637:INFO:Ridge Classifier Imported successfully
2024-11-25 12:44:13,649:INFO:Starting cross validation
2024-11-25 12:44:13,652:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:44:18,124:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:18,146:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:18,230:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:18,258:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:18,280:INFO:Calculating mean and std
2024-11-25 12:44:18,280:INFO:Creating metrics dataframe
2024-11-25 12:44:18,286:INFO:Finalizing model
2024-11-25 12:44:18,392:INFO:Uploading results into container
2024-11-25 12:44:18,392:INFO:Uploading model into container now
2024-11-25 12:44:18,400:INFO:_master_model_container: 15
2024-11-25 12:44:18,400:INFO:_display_container: 6
2024-11-25 12:44:18,400:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:44:18,404:INFO:create_model() successfully completed......................................
2024-11-25 12:44:18,504:INFO:Initializing tune_model()
2024-11-25 12:44:18,504:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:44:18,504:INFO:Checking exceptions
2024-11-25 12:44:18,520:INFO:Copying training dataset
2024-11-25 12:44:18,528:INFO:Checking base model
2024-11-25 12:44:18,528:INFO:Base model : Ridge Classifier
2024-11-25 12:44:18,532:INFO:Declaring metric variables
2024-11-25 12:44:18,536:INFO:Defining Hyperparameters
2024-11-25 12:44:18,646:INFO:Tuning with n_jobs=-1
2024-11-25 12:44:18,646:INFO:Initializing RandomizedSearchCV
2024-11-25 12:44:22,589:INFO:best_params: {'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 5.62}
2024-11-25 12:44:22,593:INFO:Hyperparameter search completed
2024-11-25 12:44:22,593:INFO:SubProcess create_model() called ==================================
2024-11-25 12:44:22,593:INFO:Initializing create_model()
2024-11-25 12:44:22,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A45EB9D990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'fit_intercept': True, 'alpha': 5.62})
2024-11-25 12:44:22,593:INFO:Checking exceptions
2024-11-25 12:44:22,593:INFO:Importing libraries
2024-11-25 12:44:22,593:INFO:Copying training dataset
2024-11-25 12:44:22,601:INFO:Defining folds
2024-11-25 12:44:22,601:INFO:Declaring metric variables
2024-11-25 12:44:22,609:INFO:Importing untrained model
2024-11-25 12:44:22,609:INFO:Declaring custom model
2024-11-25 12:44:22,617:INFO:Ridge Classifier Imported successfully
2024-11-25 12:44:22,625:INFO:Starting cross validation
2024-11-25 12:44:22,629:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:44:22,855:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:22,859:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:22,859:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:22,867:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:22,887:INFO:Calculating mean and std
2024-11-25 12:44:22,887:INFO:Creating metrics dataframe
2024-11-25 12:44:22,891:INFO:Finalizing model
2024-11-25 12:44:23,007:INFO:Uploading results into container
2024-11-25 12:44:23,007:INFO:Uploading model into container now
2024-11-25 12:44:23,007:INFO:_master_model_container: 16
2024-11-25 12:44:23,007:INFO:_display_container: 7
2024-11-25 12:44:23,011:INFO:RidgeClassifier(alpha=5.62, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:44:23,011:INFO:create_model() successfully completed......................................
2024-11-25 12:44:23,103:INFO:SubProcess create_model() end ==================================
2024-11-25 12:44:23,103:INFO:choose_better activated
2024-11-25 12:44:23,103:INFO:SubProcess create_model() called ==================================
2024-11-25 12:44:23,107:INFO:Initializing create_model()
2024-11-25 12:44:23,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:44:23,107:INFO:Checking exceptions
2024-11-25 12:44:23,107:INFO:Importing libraries
2024-11-25 12:44:23,107:INFO:Copying training dataset
2024-11-25 12:44:23,111:INFO:Defining folds
2024-11-25 12:44:23,111:INFO:Declaring metric variables
2024-11-25 12:44:23,111:INFO:Importing untrained model
2024-11-25 12:44:23,111:INFO:Declaring custom model
2024-11-25 12:44:23,111:INFO:Ridge Classifier Imported successfully
2024-11-25 12:44:23,111:INFO:Starting cross validation
2024-11-25 12:44:23,115:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:44:23,299:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:23,307:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:23,311:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:23,323:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:44:23,335:INFO:Calculating mean and std
2024-11-25 12:44:23,335:INFO:Creating metrics dataframe
2024-11-25 12:44:23,335:INFO:Finalizing model
2024-11-25 12:44:23,427:INFO:Uploading results into container
2024-11-25 12:44:23,427:INFO:Uploading model into container now
2024-11-25 12:44:23,427:INFO:_master_model_container: 17
2024-11-25 12:44:23,427:INFO:_display_container: 8
2024-11-25 12:44:23,427:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:44:23,427:INFO:create_model() successfully completed......................................
2024-11-25 12:44:23,524:INFO:SubProcess create_model() end ==================================
2024-11-25 12:44:23,524:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6577
2024-11-25 12:44:23,524:INFO:RidgeClassifier(alpha=5.62, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6615
2024-11-25 12:44:23,524:INFO:RidgeClassifier(alpha=5.62, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) is best model
2024-11-25 12:44:23,524:INFO:choose_better completed
2024-11-25 12:44:23,532:INFO:_master_model_container: 17
2024-11-25 12:44:23,536:INFO:_display_container: 7
2024-11-25 12:44:23,536:INFO:RidgeClassifier(alpha=5.62, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:44:23,536:INFO:tune_model() successfully completed......................................
2024-11-25 12:46:13,318:INFO:Initializing create_model()
2024-11-25 12:46:13,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:46:13,318:INFO:Checking exceptions
2024-11-25 12:46:13,341:INFO:Importing libraries
2024-11-25 12:46:13,341:INFO:Copying training dataset
2024-11-25 12:46:13,356:INFO:Defining folds
2024-11-25 12:46:13,356:INFO:Declaring metric variables
2024-11-25 12:46:13,356:INFO:Importing untrained model
2024-11-25 12:46:13,356:INFO:Ridge Classifier Imported successfully
2024-11-25 12:46:13,372:INFO:Starting cross validation
2024-11-25 12:46:13,372:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:46:13,628:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:13,633:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:13,635:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:13,639:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:13,655:INFO:Calculating mean and std
2024-11-25 12:46:13,655:INFO:Creating metrics dataframe
2024-11-25 12:46:13,659:INFO:Finalizing model
2024-11-25 12:46:13,761:INFO:Uploading results into container
2024-11-25 12:46:13,762:INFO:Uploading model into container now
2024-11-25 12:46:13,771:INFO:_master_model_container: 18
2024-11-25 12:46:13,771:INFO:_display_container: 8
2024-11-25 12:46:13,772:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:46:13,772:INFO:create_model() successfully completed......................................
2024-11-25 12:46:13,879:INFO:Initializing tune_model()
2024-11-25 12:46:13,879:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, n_iter=20, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:46:13,879:INFO:Checking exceptions
2024-11-25 12:46:13,899:INFO:Copying training dataset
2024-11-25 12:46:13,903:INFO:Checking base model
2024-11-25 12:46:13,903:INFO:Base model : Ridge Classifier
2024-11-25 12:46:13,907:INFO:Declaring metric variables
2024-11-25 12:46:13,914:INFO:Defining Hyperparameters
2024-11-25 12:46:14,019:INFO:Tuning with n_jobs=-1
2024-11-25 12:46:14,019:INFO:Initializing RandomizedSearchCV
2024-11-25 12:46:17,436:INFO:best_params: {'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 3.2}
2024-11-25 12:46:17,436:INFO:Hyperparameter search completed
2024-11-25 12:46:17,436:INFO:SubProcess create_model() called ==================================
2024-11-25 12:46:17,436:INFO:Initializing create_model()
2024-11-25 12:46:17,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A408F24C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'fit_intercept': True, 'alpha': 3.2})
2024-11-25 12:46:17,436:INFO:Checking exceptions
2024-11-25 12:46:17,436:INFO:Importing libraries
2024-11-25 12:46:17,436:INFO:Copying training dataset
2024-11-25 12:46:17,444:INFO:Defining folds
2024-11-25 12:46:17,444:INFO:Declaring metric variables
2024-11-25 12:46:17,448:INFO:Importing untrained model
2024-11-25 12:46:17,448:INFO:Declaring custom model
2024-11-25 12:46:17,452:INFO:Ridge Classifier Imported successfully
2024-11-25 12:46:17,460:INFO:Starting cross validation
2024-11-25 12:46:17,460:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:46:17,643:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:17,647:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:17,647:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:17,651:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:17,669:INFO:Calculating mean and std
2024-11-25 12:46:17,670:INFO:Creating metrics dataframe
2024-11-25 12:46:17,676:INFO:Finalizing model
2024-11-25 12:46:17,774:INFO:Uploading results into container
2024-11-25 12:46:17,775:INFO:Uploading model into container now
2024-11-25 12:46:17,775:INFO:_master_model_container: 19
2024-11-25 12:46:17,776:INFO:_display_container: 9
2024-11-25 12:46:17,776:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:46:17,776:INFO:create_model() successfully completed......................................
2024-11-25 12:46:17,874:INFO:SubProcess create_model() end ==================================
2024-11-25 12:46:17,874:INFO:choose_better activated
2024-11-25 12:46:17,876:INFO:SubProcess create_model() called ==================================
2024-11-25 12:46:17,877:INFO:Initializing create_model()
2024-11-25 12:46:17,877:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:46:17,877:INFO:Checking exceptions
2024-11-25 12:46:17,878:INFO:Importing libraries
2024-11-25 12:46:17,878:INFO:Copying training dataset
2024-11-25 12:46:17,884:INFO:Defining folds
2024-11-25 12:46:17,884:INFO:Declaring metric variables
2024-11-25 12:46:17,885:INFO:Importing untrained model
2024-11-25 12:46:17,885:INFO:Declaring custom model
2024-11-25 12:46:17,885:INFO:Ridge Classifier Imported successfully
2024-11-25 12:46:17,885:INFO:Starting cross validation
2024-11-25 12:46:17,887:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:46:18,061:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:18,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:18,067:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:18,067:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:18,081:INFO:Calculating mean and std
2024-11-25 12:46:18,081:INFO:Creating metrics dataframe
2024-11-25 12:46:18,083:INFO:Finalizing model
2024-11-25 12:46:18,170:INFO:Uploading results into container
2024-11-25 12:46:18,170:INFO:Uploading model into container now
2024-11-25 12:46:18,171:INFO:_master_model_container: 20
2024-11-25 12:46:18,171:INFO:_display_container: 10
2024-11-25 12:46:18,171:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:46:18,171:INFO:create_model() successfully completed......................................
2024-11-25 12:46:18,252:INFO:SubProcess create_model() end ==================================
2024-11-25 12:46:18,253:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6577
2024-11-25 12:46:18,253:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6654
2024-11-25 12:46:18,253:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) is best model
2024-11-25 12:46:18,253:INFO:choose_better completed
2024-11-25 12:46:18,262:INFO:_master_model_container: 20
2024-11-25 12:46:18,262:INFO:_display_container: 9
2024-11-25 12:46:18,262:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:46:18,262:INFO:tune_model() successfully completed......................................
2024-11-25 12:46:28,590:INFO:Initializing create_model()
2024-11-25 12:46:28,590:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:46:28,590:INFO:Checking exceptions
2024-11-25 12:46:28,609:INFO:Importing libraries
2024-11-25 12:46:28,609:INFO:Copying training dataset
2024-11-25 12:46:28,615:INFO:Defining folds
2024-11-25 12:46:28,615:INFO:Declaring metric variables
2024-11-25 12:46:28,620:INFO:Importing untrained model
2024-11-25 12:46:28,628:INFO:Ridge Classifier Imported successfully
2024-11-25 12:46:28,637:INFO:Starting cross validation
2024-11-25 12:46:28,644:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:46:28,860:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:28,872:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:28,881:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:28,884:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:46:28,904:INFO:Calculating mean and std
2024-11-25 12:46:28,904:INFO:Creating metrics dataframe
2024-11-25 12:46:28,908:INFO:Finalizing model
2024-11-25 12:46:29,007:INFO:Uploading results into container
2024-11-25 12:46:29,008:INFO:Uploading model into container now
2024-11-25 12:46:29,018:INFO:_master_model_container: 21
2024-11-25 12:46:29,019:INFO:_display_container: 10
2024-11-25 12:46:29,019:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:46:29,019:INFO:create_model() successfully completed......................................
2024-11-25 12:46:29,122:INFO:Initializing tune_model()
2024-11-25 12:46:29,122:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, n_iter=200, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:46:29,122:INFO:Checking exceptions
2024-11-25 12:46:29,138:INFO:Copying training dataset
2024-11-25 12:46:29,142:INFO:Checking base model
2024-11-25 12:46:29,142:INFO:Base model : Ridge Classifier
2024-11-25 12:46:29,147:INFO:Declaring metric variables
2024-11-25 12:46:29,150:INFO:Defining Hyperparameters
2024-11-25 12:46:29,254:INFO:Tuning with n_jobs=-1
2024-11-25 12:46:29,254:INFO:Initializing RandomizedSearchCV
2024-11-25 12:47:06,910:INFO:best_params: {'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 3.2}
2024-11-25 12:47:06,911:INFO:Hyperparameter search completed
2024-11-25 12:47:06,911:INFO:SubProcess create_model() called ==================================
2024-11-25 12:47:06,912:INFO:Initializing create_model()
2024-11-25 12:47:06,912:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A46F47AB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'fit_intercept': True, 'alpha': 3.2})
2024-11-25 12:47:06,912:INFO:Checking exceptions
2024-11-25 12:47:06,912:INFO:Importing libraries
2024-11-25 12:47:06,912:INFO:Copying training dataset
2024-11-25 12:47:06,917:INFO:Defining folds
2024-11-25 12:47:06,917:INFO:Declaring metric variables
2024-11-25 12:47:06,922:INFO:Importing untrained model
2024-11-25 12:47:06,923:INFO:Declaring custom model
2024-11-25 12:47:06,929:INFO:Ridge Classifier Imported successfully
2024-11-25 12:47:06,940:INFO:Starting cross validation
2024-11-25 12:47:06,944:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:47:07,248:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,248:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,267:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,293:INFO:Calculating mean and std
2024-11-25 12:47:07,294:INFO:Creating metrics dataframe
2024-11-25 12:47:07,300:INFO:Finalizing model
2024-11-25 12:47:07,424:INFO:Uploading results into container
2024-11-25 12:47:07,425:INFO:Uploading model into container now
2024-11-25 12:47:07,425:INFO:_master_model_container: 22
2024-11-25 12:47:07,425:INFO:_display_container: 11
2024-11-25 12:47:07,426:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:47:07,426:INFO:create_model() successfully completed......................................
2024-11-25 12:47:07,523:INFO:SubProcess create_model() end ==================================
2024-11-25 12:47:07,524:INFO:choose_better activated
2024-11-25 12:47:07,527:INFO:SubProcess create_model() called ==================================
2024-11-25 12:47:07,527:INFO:Initializing create_model()
2024-11-25 12:47:07,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A403144110>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:47:07,527:INFO:Checking exceptions
2024-11-25 12:47:07,529:INFO:Importing libraries
2024-11-25 12:47:07,530:INFO:Copying training dataset
2024-11-25 12:47:07,533:INFO:Defining folds
2024-11-25 12:47:07,533:INFO:Declaring metric variables
2024-11-25 12:47:07,533:INFO:Importing untrained model
2024-11-25 12:47:07,533:INFO:Declaring custom model
2024-11-25 12:47:07,534:INFO:Ridge Classifier Imported successfully
2024-11-25 12:47:07,534:INFO:Starting cross validation
2024-11-25 12:47:07,536:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:47:07,739:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,744:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,747:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,748:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:47:07,767:INFO:Calculating mean and std
2024-11-25 12:47:07,768:INFO:Creating metrics dataframe
2024-11-25 12:47:07,769:INFO:Finalizing model
2024-11-25 12:47:07,873:INFO:Uploading results into container
2024-11-25 12:47:07,873:INFO:Uploading model into container now
2024-11-25 12:47:07,874:INFO:_master_model_container: 23
2024-11-25 12:47:07,874:INFO:_display_container: 12
2024-11-25 12:47:07,874:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:47:07,874:INFO:create_model() successfully completed......................................
2024-11-25 12:47:07,957:INFO:SubProcess create_model() end ==================================
2024-11-25 12:47:07,957:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6577
2024-11-25 12:47:07,958:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6654
2024-11-25 12:47:07,958:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) is best model
2024-11-25 12:47:07,958:INFO:choose_better completed
2024-11-25 12:47:07,966:INFO:_master_model_container: 23
2024-11-25 12:47:07,966:INFO:_display_container: 11
2024-11-25 12:47:07,967:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:47:07,967:INFO:tune_model() successfully completed......................................
2024-11-25 12:50:06,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_907df8a14a1a4c358f4f720ce40a6d78
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_494a1a3d3d9c4e6fa9aae5a7a9fc9cfc_e8502945b87841d4bfd6e399e3a01512
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_bf85294f36674e26be8eaa8d83bee3eb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_7cf62ff5f56148d99bc30d0a8b153863_274b59981808477485371027f5a72b31
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,197:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_cd6e19c92ba149a3ac074e0345479c6f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_34ef20a1e2134c51ac41f1b2fd3e7ea6_1e104df9eefc4ea1ac62abcd53bfa24c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_617fc0b9f65a4e77a74b941226aba469
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_a22a87b143b34f97bb6c8ab960222fcb_8bd0bee353604f41854dc6897f296f80
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_37f126bacea54e4e8c49d4980dfbbe35
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_9e958fbc34224c4d9fe05fb1c07ebac1_af663833ddd7482883701a5c0e1841e2
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_41d8e2585aef44698de2b79c6b5c7daf
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_51d70c3e6a224e87ba6b004e284b694b_bb1b5ac40c404bf087ceb5dcbd6e986a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_265895df47e348faaa2b903e37a4e297
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_188758cff43e43818084321a88eff60f_9e28da8c679e4561a0794c211563a21d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,198:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_c5c21be98a534841bd930597d94b9e49
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_3d96355e42c04e1d8a6e004497da8f16_d3d90bef7af54809ab98b182387d7437
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_c69fdbd6b2c14d10adf13bdb38892107
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_eb32e3fb04e2412fbcae852629a0d4d6_ef7bbc96d4a74bdb89fed3e2d819b3b7
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_2c16f9c35d6d43949a03b4e0e0655eaf
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_dd3c9bd6427d452bbb1ad03a5e369f6c_e3458b3aaced4608bb8061c0cc530a25
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_f714fa65ab34473b8371f4d17b0f31cf
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_b1f92a837c004cc2a72aa28367789706_3e4e127b7948420a8562b5d68144b397
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_6f1d9feb8d0844d39ca505ab62d44fbb
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,199:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_696afd386d95447b9498776f48846671_85f99482337947a6a3642cf62a53d921
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_e41cb571cdbd471c956da3dcdab1e337
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_8fbc50f515ec48bd9ab510a3a27df110_8b654620ae7246afaf692deda239ddf1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_d0e7356760c4457ba3f3d2634073db5a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_276a10a92bbb4aa6b018cd3c8a0b4ea6_e5b5d7c7ecab40979eabfd850f5cc187
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_4d5bc5368f9f4224a33d204a838bd322
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_bdc10b52d0394b24a88f253e43209f09_f85184352d60433dbd63440621eb72a1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_e180b13d9e2547b6af27da579011cec9
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_f0cd3f6d9b26448c81d48b5988e6aca1_bd47d6f791fc4819a790f3fe6f726f96
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_a4c78543bcb14eb29f5cdf32cfcd9119
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_1167586805f84f6da0435eb535e97f83_6cf111bc22dc41f8b99150d2b104032c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_94737f11998443b4aac5fef531935a2e
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_081952710ebc48e0b7203ac45f9cd71b_95d9737f9fd9435f9dc66079c14c095d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_237b7e5ef2fb4a1c956db8fe733f12ed
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_f4d1906b3ddf477e95939811b46af33b_aacca557c60f4a97852865b348318a7a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_e31a7b4a3a234b6a988428cc53b28a94
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_727774ea9ae24141841adef8cc1b0591_6a017bf8a8a2461d99fc691f2b3cdac8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_68a1b7ded2584e9ebe0e19fc19fdd38f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,201:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_9748defcf9044dd3929d826f5643fcc0_355730764d1b4b7dbeb459176db8c46d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_1feeebe3c2814aceb6f6f150a494643f
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_9fffd5ecc61a4b0dace34cf3a6e38156_24f789f7ab314d36b90340fca7402b72
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_90bf2b3edafc4c799fa225d3baf453cc
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_e09e6a7de97e4c16b0917f28d7b83fe1_25d7ddc9c62b4f24a47e4ec17e691eba
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_74175fc7a3844c0da33dc4968a8bbbef
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_e45cd518ecab4546a07c815dae5a37b1_c09f9b083d864ec69680b29c7678a709
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_685347db75de41268f01019cb6dac4b5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_8c464685b9c14000b0ddd225ea5e4707_299ae3d8608646e6a72fb6496361ab45
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_ddece235caa54b198c23316643c44586
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_f43fd13886844816b5a488c4e4c4f9bd_2dca0942d89a4c77bc93712095249fee
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_e3f1fc61f3a649d195e1430e2cdb4a57
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_2f24807f4b1c49d0a0540a00f5fe8366_16e434e9c15b425b915dc1a9bc9da781
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_53cdcd866edf4c82955bc497e7bc4828
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_d6676c621aaf40a9be0a7a9ac54d8954_72f281d779ae4a88a4f9908dde887677
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_eed4c1ba20ca49178b66b52a2c605408
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_299c3ace217b4f62a0bd8c9099ea791b_c6638ab662ad4aa1b42f40897b9d9c47
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_468cb574e20144e8a8645d86785d9251
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_9e453b16ee4d44b481bb0c5f8391cdff_03136fad1398424a9cebe8d878948c7c
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_354e026b0c0c4771b90c503a7c04909a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,203:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_8733c1efc17242abb63a72fd2f0c53ee_f11a15a7b4ff404baf72d91112307ee0
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_ad13aed5ec754f558d58dbcd2f09aa51
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_10ae5a1f25934e14a429812a6962ee75_4932e11b7cc34ed7942f909cadc74b67
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_14c45cc72da149cf84f521a6e7a30c04
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_966cefddb2134f28b5993443c91f33f1_0013975b7ddb4cde83c3582acc9e4848
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_cd1dcd88efe54516a509bf75c614ff4a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_3d809d716d45474996580833fde25775_ab164ab1abd246b59396c1b639a123de
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_c8feb632c3644365813694360bd00c64
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_9b5ec31854854ba7bb6d8b182e7d8378_63781676079b454c9a9c15be4ec5d3e5
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_4f13eadbbc324958bf2783dfd6cf7e0d
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_6bcbc4bbcf9d4f73b52d3e3f0bc0388d_6aacd7769ed547668e4ea52bd777ce18
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_1f4202582aa9429d86eee0eac2fcf980
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_c235e8d1ffb7477fb96b60ba2d7e674c_13bf9a33362e40228fabb5be8e804151
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_d29e109096824ee59b9c4eae66865777
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_a08506499efe4256b7f5eaa6a6553c4f_4c780026b7f94da2a0c396e913ce1cd1
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,205:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_60a9a42b2d9648839eb89c05b87cf7f8
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,206:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_054c3b7d66ee4d15ab9141730f6af575_0805ec2ecd6e44d88b7357c8d9c5f258
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,206:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_060c7a3567924908991b26be1ae92b81
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:06,206:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\joblib\_memmapping_reducer.py:598: UserWarning: Failed to delete temporary folder: C:\Users\KURTZ~1\AppData\Local\Temp\joblib_memmapping_folder_3400_938b8b1d695849c8af728fd6a912813b_c801916c61f14e5290f8af0318a5f22a
  warnings.warn("Failed to delete temporary folder: {}"

2024-11-25 12:50:16,016:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:50:16,017:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:50:16,017:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:50:16,017:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-25 12:50:16,394:INFO:PyCaret ClassificationExperiment
2024-11-25 12:50:16,395:INFO:Logging name: clf-default-name
2024-11-25 12:50:16,395:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:50:16,395:INFO:version 3.3.2
2024-11-25 12:50:16,395:INFO:Initializing setup()
2024-11-25 12:50:16,395:INFO:self.USI: a797
2024-11-25 12:50:16,395:INFO:self._variable_keys: {'exp_id', 'memory', '_available_plots', 'is_multiclass', 'data', 'pipeline', 'exp_name_log', 'logging_param', 'y', 'gpu_param', 'y_test', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'gpu_n_jobs_param', 'idx', 'X_train', 'fold_shuffle_param', 'target_param', 'X_test', 'USI', 'n_jobs_param', 'fold_groups_param', 'y_train', 'X', 'fold_generator', 'html_param', 'seed'}
2024-11-25 12:50:16,395:INFO:Checking environment
2024-11-25 12:50:16,395:INFO:python_version: 3.11.9
2024-11-25 12:50:16,395:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:50:16,396:INFO:machine: AMD64
2024-11-25 12:50:16,396:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:50:16,399:INFO:Memory: svmem(total=25525833728, available=14349852672, percent=43.8, used=11175981056, free=14349852672)
2024-11-25 12:50:16,399:INFO:Physical Core: 4
2024-11-25 12:50:16,399:INFO:Logical Core: 8
2024-11-25 12:50:16,399:INFO:Checking libraries
2024-11-25 12:50:16,399:INFO:System:
2024-11-25 12:50:16,399:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:50:16,399:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:50:16,399:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:50:16,400:INFO:PyCaret required dependencies:
2024-11-25 12:50:16,422:INFO:                 pip: 24.0
2024-11-25 12:50:16,422:INFO:          setuptools: 65.5.0
2024-11-25 12:50:16,422:INFO:             pycaret: 3.3.2
2024-11-25 12:50:16,422:INFO:             IPython: 8.29.0
2024-11-25 12:50:16,422:INFO:          ipywidgets: 8.1.5
2024-11-25 12:50:16,422:INFO:                tqdm: 4.66.6
2024-11-25 12:50:16,422:INFO:               numpy: 1.26.4
2024-11-25 12:50:16,422:INFO:              pandas: 2.1.4
2024-11-25 12:50:16,422:INFO:              jinja2: 3.1.4
2024-11-25 12:50:16,422:INFO:               scipy: 1.11.4
2024-11-25 12:50:16,422:INFO:              joblib: 1.3.2
2024-11-25 12:50:16,422:INFO:             sklearn: 1.4.2
2024-11-25 12:50:16,422:INFO:                pyod: 2.0.2
2024-11-25 12:50:16,422:INFO:            imblearn: 0.12.4
2024-11-25 12:50:16,422:INFO:   category_encoders: 2.6.4
2024-11-25 12:50:16,422:INFO:            lightgbm: 4.5.0
2024-11-25 12:50:16,422:INFO:               numba: 0.60.0
2024-11-25 12:50:16,422:INFO:            requests: 2.32.3
2024-11-25 12:50:16,422:INFO:          matplotlib: 3.7.5
2024-11-25 12:50:16,422:INFO:          scikitplot: 0.3.7
2024-11-25 12:50:16,422:INFO:         yellowbrick: 1.5
2024-11-25 12:50:16,422:INFO:              plotly: 5.24.1
2024-11-25 12:50:16,422:INFO:    plotly-resampler: Not installed
2024-11-25 12:50:16,422:INFO:             kaleido: 0.2.1
2024-11-25 12:50:16,422:INFO:           schemdraw: 0.15
2024-11-25 12:50:16,422:INFO:         statsmodels: 0.14.4
2024-11-25 12:50:16,423:INFO:              sktime: 0.26.0
2024-11-25 12:50:16,423:INFO:               tbats: 1.1.3
2024-11-25 12:50:16,423:INFO:            pmdarima: 2.0.4
2024-11-25 12:50:16,423:INFO:              psutil: 6.1.0
2024-11-25 12:50:16,423:INFO:          markupsafe: 3.0.2
2024-11-25 12:50:16,423:INFO:             pickle5: Not installed
2024-11-25 12:50:16,423:INFO:         cloudpickle: 3.1.0
2024-11-25 12:50:16,423:INFO:         deprecation: 2.1.0
2024-11-25 12:50:16,423:INFO:              xxhash: 3.5.0
2024-11-25 12:50:16,423:INFO:           wurlitzer: 3.1.1
2024-11-25 12:50:16,423:INFO:PyCaret optional dependencies:
2024-11-25 12:50:16,434:INFO:                shap: Not installed
2024-11-25 12:50:16,434:INFO:           interpret: Not installed
2024-11-25 12:50:16,434:INFO:                umap: Not installed
2024-11-25 12:50:16,434:INFO:     ydata_profiling: Not installed
2024-11-25 12:50:16,434:INFO:  explainerdashboard: Not installed
2024-11-25 12:50:16,434:INFO:             autoviz: Not installed
2024-11-25 12:50:16,434:INFO:           fairlearn: Not installed
2024-11-25 12:50:16,434:INFO:          deepchecks: Not installed
2024-11-25 12:50:16,434:INFO:             xgboost: Not installed
2024-11-25 12:50:16,434:INFO:            catboost: Not installed
2024-11-25 12:50:16,434:INFO:              kmodes: Not installed
2024-11-25 12:50:16,434:INFO:             mlxtend: Not installed
2024-11-25 12:50:16,434:INFO:       statsforecast: Not installed
2024-11-25 12:50:16,434:INFO:        tune_sklearn: Not installed
2024-11-25 12:50:16,434:INFO:                 ray: Not installed
2024-11-25 12:50:16,434:INFO:            hyperopt: Not installed
2024-11-25 12:50:16,434:INFO:              optuna: Not installed
2024-11-25 12:50:16,434:INFO:               skopt: Not installed
2024-11-25 12:50:16,434:INFO:              mlflow: Not installed
2024-11-25 12:50:16,434:INFO:              gradio: Not installed
2024-11-25 12:50:16,435:INFO:             fastapi: Not installed
2024-11-25 12:50:16,435:INFO:             uvicorn: Not installed
2024-11-25 12:50:16,435:INFO:              m2cgen: Not installed
2024-11-25 12:50:16,435:INFO:           evidently: Not installed
2024-11-25 12:50:16,435:INFO:               fugue: Not installed
2024-11-25 12:50:16,435:INFO:           streamlit: Not installed
2024-11-25 12:50:16,435:INFO:             prophet: Not installed
2024-11-25 12:50:16,435:INFO:None
2024-11-25 12:50:16,435:INFO:Set up data.
2024-11-25 12:50:16,440:INFO:Set up folding strategy.
2024-11-25 12:50:16,441:INFO:Set up train/test split.
2024-11-25 12:50:16,446:INFO:Set up index.
2024-11-25 12:50:16,446:INFO:Assigning column types.
2024-11-25 12:50:16,449:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:50:16,489:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,492:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,524:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,524:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,566:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,566:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,592:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,592:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,593:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:50:16,635:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,660:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,661:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,707:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:16,735:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,735:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,735:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:50:16,808:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,808:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,877:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,878:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:16,879:INFO:Preparing preprocessing pipeline...
2024-11-25 12:50:16,880:INFO:Set up simple imputation.
2024-11-25 12:50:16,882:INFO:Set up encoding of categorical features.
2024-11-25 12:50:16,883:INFO:Set up column name cleaning.
2024-11-25 12:50:17,006:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:50:17,011:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:50:17,012:INFO:Creating final display dataframe.
2024-11-25 12:50:17,194:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                 4
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              a797
2024-11-25 12:50:17,279:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,280:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,348:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,350:INFO:setup() successfully completed in 0.96s...............
2024-11-25 12:50:17,366:INFO:PyCaret ClassificationExperiment
2024-11-25 12:50:17,366:INFO:Logging name: clf-default-name
2024-11-25 12:50:17,366:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-25 12:50:17,366:INFO:version 3.3.2
2024-11-25 12:50:17,366:INFO:Initializing setup()
2024-11-25 12:50:17,366:INFO:self.USI: c13b
2024-11-25 12:50:17,366:INFO:self._variable_keys: {'exp_id', 'memory', '_available_plots', 'is_multiclass', 'data', 'pipeline', 'exp_name_log', 'logging_param', 'y', 'gpu_param', 'y_test', '_ml_usecase', 'log_plots_param', 'fix_imbalance', 'gpu_n_jobs_param', 'idx', 'X_train', 'fold_shuffle_param', 'target_param', 'X_test', 'USI', 'n_jobs_param', 'fold_groups_param', 'y_train', 'X', 'fold_generator', 'html_param', 'seed'}
2024-11-25 12:50:17,366:INFO:Checking environment
2024-11-25 12:50:17,366:INFO:python_version: 3.11.9
2024-11-25 12:50:17,366:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2024-11-25 12:50:17,366:INFO:machine: AMD64
2024-11-25 12:50:17,366:INFO:platform: Windows-10-10.0.19045-SP0
2024-11-25 12:50:17,369:INFO:Memory: svmem(total=25525833728, available=14363885568, percent=43.7, used=11161948160, free=14363885568)
2024-11-25 12:50:17,370:INFO:Physical Core: 4
2024-11-25 12:50:17,370:INFO:Logical Core: 8
2024-11-25 12:50:17,370:INFO:Checking libraries
2024-11-25 12:50:17,370:INFO:System:
2024-11-25 12:50:17,370:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2024-11-25 12:50:17,370:INFO:executable: D:\coding\Predict_ECE_from_SA45\backend\.venv\Scripts\python.exe
2024-11-25 12:50:17,370:INFO:   machine: Windows-10-10.0.19045-SP0
2024-11-25 12:50:17,370:INFO:PyCaret required dependencies:
2024-11-25 12:50:17,370:INFO:                 pip: 24.0
2024-11-25 12:50:17,370:INFO:          setuptools: 65.5.0
2024-11-25 12:50:17,370:INFO:             pycaret: 3.3.2
2024-11-25 12:50:17,370:INFO:             IPython: 8.29.0
2024-11-25 12:50:17,370:INFO:          ipywidgets: 8.1.5
2024-11-25 12:50:17,370:INFO:                tqdm: 4.66.6
2024-11-25 12:50:17,370:INFO:               numpy: 1.26.4
2024-11-25 12:50:17,370:INFO:              pandas: 2.1.4
2024-11-25 12:50:17,370:INFO:              jinja2: 3.1.4
2024-11-25 12:50:17,370:INFO:               scipy: 1.11.4
2024-11-25 12:50:17,370:INFO:              joblib: 1.3.2
2024-11-25 12:50:17,371:INFO:             sklearn: 1.4.2
2024-11-25 12:50:17,371:INFO:                pyod: 2.0.2
2024-11-25 12:50:17,371:INFO:            imblearn: 0.12.4
2024-11-25 12:50:17,371:INFO:   category_encoders: 2.6.4
2024-11-25 12:50:17,371:INFO:            lightgbm: 4.5.0
2024-11-25 12:50:17,371:INFO:               numba: 0.60.0
2024-11-25 12:50:17,371:INFO:            requests: 2.32.3
2024-11-25 12:50:17,371:INFO:          matplotlib: 3.7.5
2024-11-25 12:50:17,371:INFO:          scikitplot: 0.3.7
2024-11-25 12:50:17,371:INFO:         yellowbrick: 1.5
2024-11-25 12:50:17,371:INFO:              plotly: 5.24.1
2024-11-25 12:50:17,371:INFO:    plotly-resampler: Not installed
2024-11-25 12:50:17,371:INFO:             kaleido: 0.2.1
2024-11-25 12:50:17,371:INFO:           schemdraw: 0.15
2024-11-25 12:50:17,371:INFO:         statsmodels: 0.14.4
2024-11-25 12:50:17,371:INFO:              sktime: 0.26.0
2024-11-25 12:50:17,371:INFO:               tbats: 1.1.3
2024-11-25 12:50:17,371:INFO:            pmdarima: 2.0.4
2024-11-25 12:50:17,371:INFO:              psutil: 6.1.0
2024-11-25 12:50:17,371:INFO:          markupsafe: 3.0.2
2024-11-25 12:50:17,371:INFO:             pickle5: Not installed
2024-11-25 12:50:17,371:INFO:         cloudpickle: 3.1.0
2024-11-25 12:50:17,371:INFO:         deprecation: 2.1.0
2024-11-25 12:50:17,371:INFO:              xxhash: 3.5.0
2024-11-25 12:50:17,371:INFO:           wurlitzer: 3.1.1
2024-11-25 12:50:17,371:INFO:PyCaret optional dependencies:
2024-11-25 12:50:17,371:INFO:                shap: Not installed
2024-11-25 12:50:17,371:INFO:           interpret: Not installed
2024-11-25 12:50:17,371:INFO:                umap: Not installed
2024-11-25 12:50:17,371:INFO:     ydata_profiling: Not installed
2024-11-25 12:50:17,371:INFO:  explainerdashboard: Not installed
2024-11-25 12:50:17,371:INFO:             autoviz: Not installed
2024-11-25 12:50:17,372:INFO:           fairlearn: Not installed
2024-11-25 12:50:17,372:INFO:          deepchecks: Not installed
2024-11-25 12:50:17,372:INFO:             xgboost: Not installed
2024-11-25 12:50:17,372:INFO:            catboost: Not installed
2024-11-25 12:50:17,372:INFO:              kmodes: Not installed
2024-11-25 12:50:17,372:INFO:             mlxtend: Not installed
2024-11-25 12:50:17,372:INFO:       statsforecast: Not installed
2024-11-25 12:50:17,372:INFO:        tune_sklearn: Not installed
2024-11-25 12:50:17,372:INFO:                 ray: Not installed
2024-11-25 12:50:17,372:INFO:            hyperopt: Not installed
2024-11-25 12:50:17,372:INFO:              optuna: Not installed
2024-11-25 12:50:17,372:INFO:               skopt: Not installed
2024-11-25 12:50:17,372:INFO:              mlflow: Not installed
2024-11-25 12:50:17,372:INFO:              gradio: Not installed
2024-11-25 12:50:17,372:INFO:             fastapi: Not installed
2024-11-25 12:50:17,372:INFO:             uvicorn: Not installed
2024-11-25 12:50:17,372:INFO:              m2cgen: Not installed
2024-11-25 12:50:17,372:INFO:           evidently: Not installed
2024-11-25 12:50:17,372:INFO:               fugue: Not installed
2024-11-25 12:50:17,372:INFO:           streamlit: Not installed
2024-11-25 12:50:17,372:INFO:             prophet: Not installed
2024-11-25 12:50:17,372:INFO:None
2024-11-25 12:50:17,372:INFO:Set up data.
2024-11-25 12:50:17,381:INFO:Set up folding strategy.
2024-11-25 12:50:17,381:INFO:Set up train/test split.
2024-11-25 12:50:17,384:INFO:Set up index.
2024-11-25 12:50:17,385:INFO:Assigning column types.
2024-11-25 12:50:17,387:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-25 12:50:17,437:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,438:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,465:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,465:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,511:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,537:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,537:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,537:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-25 12:50:17,582:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,609:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,654:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-25 12:50:17,678:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,678:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,679:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-25 12:50:17,744:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,744:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,813:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:17,814:INFO:Preparing preprocessing pipeline...
2024-11-25 12:50:17,815:INFO:Set up simple imputation.
2024-11-25 12:50:17,816:INFO:Set up encoding of categorical features.
2024-11-25 12:50:17,817:INFO:Set up column name cleaning.
2024-11-25 12:50:17,933:INFO:Finished creating preprocessing pipeline.
2024-11-25 12:50:17,937:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-11-25 12:50:17,937:INFO:Creating final display dataframe.
2024-11-25 12:50:18,117:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target       Agotamiento
2                   Target type        Multiclass
3           Original data shape         (372, 12)
4        Transformed data shape         (372, 32)
5   Transformed train set shape         (260, 32)
6    Transformed test set shape         (112, 32)
7              Numeric features                 1
8          Categorical features                10
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              c13b
2024-11-25 12:50:18,187:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:18,188:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:18,257:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:18,257:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-25 12:50:18,259:INFO:setup() successfully completed in 0.9s...............
2024-11-25 12:50:18,266:INFO:Initializing compare_models()
2024-11-25 12:50:18,266:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:50:18,266:INFO:Checking exceptions
2024-11-25 12:50:18,271:INFO:Preparing display monitor
2024-11-25 12:50:18,299:INFO:Initializing Logistic Regression
2024-11-25 12:50:18,299:INFO:Total runtime is 0.0 minutes
2024-11-25 12:50:18,303:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:18,304:INFO:Initializing create_model()
2024-11-25 12:50:18,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=lr, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:18,304:INFO:Checking exceptions
2024-11-25 12:50:18,304:INFO:Importing libraries
2024-11-25 12:50:18,304:INFO:Copying training dataset
2024-11-25 12:50:18,311:INFO:Defining folds
2024-11-25 12:50:18,311:INFO:Declaring metric variables
2024-11-25 12:50:18,315:INFO:Importing untrained model
2024-11-25 12:50:18,319:INFO:Logistic Regression Imported successfully
2024-11-25 12:50:18,329:INFO:Starting cross validation
2024-11-25 12:50:18,332:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:21,717:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:21,729:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:21,739:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:21,782:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:21,804:INFO:Calculating mean and std
2024-11-25 12:50:21,806:INFO:Creating metrics dataframe
2024-11-25 12:50:21,809:INFO:Uploading results into container
2024-11-25 12:50:21,810:INFO:Uploading model into container now
2024-11-25 12:50:21,811:INFO:_master_model_container: 1
2024-11-25 12:50:21,811:INFO:_display_container: 2
2024-11-25 12:50:21,811:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:50:21,812:INFO:create_model() successfully completed......................................
2024-11-25 12:50:21,897:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:21,898:INFO:Creating metrics dataframe
2024-11-25 12:50:21,904:INFO:Initializing K Neighbors Classifier
2024-11-25 12:50:21,904:INFO:Total runtime is 0.06008475224177043 minutes
2024-11-25 12:50:21,907:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:21,907:INFO:Initializing create_model()
2024-11-25 12:50:21,908:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=knn, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:21,908:INFO:Checking exceptions
2024-11-25 12:50:21,908:INFO:Importing libraries
2024-11-25 12:50:21,908:INFO:Copying training dataset
2024-11-25 12:50:21,913:INFO:Defining folds
2024-11-25 12:50:21,913:INFO:Declaring metric variables
2024-11-25 12:50:21,916:INFO:Importing untrained model
2024-11-25 12:50:21,921:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:50:21,928:INFO:Starting cross validation
2024-11-25 12:50:21,931:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:24,765:INFO:Calculating mean and std
2024-11-25 12:50:24,768:INFO:Creating metrics dataframe
2024-11-25 12:50:24,773:INFO:Uploading results into container
2024-11-25 12:50:24,776:INFO:Uploading model into container now
2024-11-25 12:50:24,777:INFO:_master_model_container: 2
2024-11-25 12:50:24,778:INFO:_display_container: 2
2024-11-25 12:50:24,778:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:50:24,779:INFO:create_model() successfully completed......................................
2024-11-25 12:50:24,891:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:24,891:INFO:Creating metrics dataframe
2024-11-25 12:50:24,897:INFO:Initializing Naive Bayes
2024-11-25 12:50:24,897:INFO:Total runtime is 0.10996890068054199 minutes
2024-11-25 12:50:24,900:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:24,900:INFO:Initializing create_model()
2024-11-25 12:50:24,900:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=nb, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:24,900:INFO:Checking exceptions
2024-11-25 12:50:24,901:INFO:Importing libraries
2024-11-25 12:50:24,901:INFO:Copying training dataset
2024-11-25 12:50:24,905:INFO:Defining folds
2024-11-25 12:50:24,906:INFO:Declaring metric variables
2024-11-25 12:50:24,910:INFO:Importing untrained model
2024-11-25 12:50:24,914:INFO:Naive Bayes Imported successfully
2024-11-25 12:50:24,921:INFO:Starting cross validation
2024-11-25 12:50:24,924:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:25,184:INFO:Calculating mean and std
2024-11-25 12:50:25,185:INFO:Creating metrics dataframe
2024-11-25 12:50:25,187:INFO:Uploading results into container
2024-11-25 12:50:25,188:INFO:Uploading model into container now
2024-11-25 12:50:25,189:INFO:_master_model_container: 3
2024-11-25 12:50:25,189:INFO:_display_container: 2
2024-11-25 12:50:25,189:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:50:25,189:INFO:create_model() successfully completed......................................
2024-11-25 12:50:25,259:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:25,259:INFO:Creating metrics dataframe
2024-11-25 12:50:25,265:INFO:Initializing Decision Tree Classifier
2024-11-25 12:50:25,266:INFO:Total runtime is 0.11609510978062948 minutes
2024-11-25 12:50:25,269:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:25,269:INFO:Initializing create_model()
2024-11-25 12:50:25,269:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=dt, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:25,269:INFO:Checking exceptions
2024-11-25 12:50:25,269:INFO:Importing libraries
2024-11-25 12:50:25,269:INFO:Copying training dataset
2024-11-25 12:50:25,274:INFO:Defining folds
2024-11-25 12:50:25,274:INFO:Declaring metric variables
2024-11-25 12:50:25,278:INFO:Importing untrained model
2024-11-25 12:50:25,281:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:50:25,288:INFO:Starting cross validation
2024-11-25 12:50:25,291:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:25,516:INFO:Calculating mean and std
2024-11-25 12:50:25,517:INFO:Creating metrics dataframe
2024-11-25 12:50:25,519:INFO:Uploading results into container
2024-11-25 12:50:25,520:INFO:Uploading model into container now
2024-11-25 12:50:25,520:INFO:_master_model_container: 4
2024-11-25 12:50:25,520:INFO:_display_container: 2
2024-11-25 12:50:25,521:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:50:25,521:INFO:create_model() successfully completed......................................
2024-11-25 12:50:25,587:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:25,587:INFO:Creating metrics dataframe
2024-11-25 12:50:25,594:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:50:25,594:INFO:Total runtime is 0.12157411177953084 minutes
2024-11-25 12:50:25,598:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:25,598:INFO:Initializing create_model()
2024-11-25 12:50:25,598:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=svm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:25,598:INFO:Checking exceptions
2024-11-25 12:50:25,598:INFO:Importing libraries
2024-11-25 12:50:25,598:INFO:Copying training dataset
2024-11-25 12:50:25,606:INFO:Defining folds
2024-11-25 12:50:25,606:INFO:Declaring metric variables
2024-11-25 12:50:25,611:INFO:Importing untrained model
2024-11-25 12:50:25,616:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:50:25,623:INFO:Starting cross validation
2024-11-25 12:50:25,627:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:25,837:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:25,840:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:25,848:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:25,856:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:25,866:INFO:Calculating mean and std
2024-11-25 12:50:25,867:INFO:Creating metrics dataframe
2024-11-25 12:50:25,869:INFO:Uploading results into container
2024-11-25 12:50:25,870:INFO:Uploading model into container now
2024-11-25 12:50:25,870:INFO:_master_model_container: 5
2024-11-25 12:50:25,871:INFO:_display_container: 2
2024-11-25 12:50:25,871:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:50:25,871:INFO:create_model() successfully completed......................................
2024-11-25 12:50:25,941:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:25,941:INFO:Creating metrics dataframe
2024-11-25 12:50:25,948:INFO:Initializing Ridge Classifier
2024-11-25 12:50:25,948:INFO:Total runtime is 0.12747511466344197 minutes
2024-11-25 12:50:25,951:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:25,952:INFO:Initializing create_model()
2024-11-25 12:50:25,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=ridge, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:25,952:INFO:Checking exceptions
2024-11-25 12:50:25,952:INFO:Importing libraries
2024-11-25 12:50:25,952:INFO:Copying training dataset
2024-11-25 12:50:25,956:INFO:Defining folds
2024-11-25 12:50:25,956:INFO:Declaring metric variables
2024-11-25 12:50:25,959:INFO:Importing untrained model
2024-11-25 12:50:25,964:INFO:Ridge Classifier Imported successfully
2024-11-25 12:50:25,971:INFO:Starting cross validation
2024-11-25 12:50:25,974:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:26,178:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:26,187:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:26,193:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:26,202:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:26,223:INFO:Calculating mean and std
2024-11-25 12:50:26,224:INFO:Creating metrics dataframe
2024-11-25 12:50:26,225:INFO:Uploading results into container
2024-11-25 12:50:26,226:INFO:Uploading model into container now
2024-11-25 12:50:26,226:INFO:_master_model_container: 6
2024-11-25 12:50:26,226:INFO:_display_container: 2
2024-11-25 12:50:26,226:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:50:26,227:INFO:create_model() successfully completed......................................
2024-11-25 12:50:26,304:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:26,304:INFO:Creating metrics dataframe
2024-11-25 12:50:26,315:INFO:Initializing Random Forest Classifier
2024-11-25 12:50:26,315:INFO:Total runtime is 0.13360178073247272 minutes
2024-11-25 12:50:26,321:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:26,321:INFO:Initializing create_model()
2024-11-25 12:50:26,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=rf, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:26,321:INFO:Checking exceptions
2024-11-25 12:50:26,321:INFO:Importing libraries
2024-11-25 12:50:26,321:INFO:Copying training dataset
2024-11-25 12:50:26,329:INFO:Defining folds
2024-11-25 12:50:26,330:INFO:Declaring metric variables
2024-11-25 12:50:26,334:INFO:Importing untrained model
2024-11-25 12:50:26,338:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:50:26,348:INFO:Starting cross validation
2024-11-25 12:50:26,352:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:26,859:INFO:Calculating mean and std
2024-11-25 12:50:26,860:INFO:Creating metrics dataframe
2024-11-25 12:50:26,862:INFO:Uploading results into container
2024-11-25 12:50:26,863:INFO:Uploading model into container now
2024-11-25 12:50:26,863:INFO:_master_model_container: 7
2024-11-25 12:50:26,863:INFO:_display_container: 2
2024-11-25 12:50:26,864:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:50:26,864:INFO:create_model() successfully completed......................................
2024-11-25 12:50:26,933:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:26,933:INFO:Creating metrics dataframe
2024-11-25 12:50:26,941:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:50:26,941:INFO:Total runtime is 0.1440253973007202 minutes
2024-11-25 12:50:26,944:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:26,944:INFO:Initializing create_model()
2024-11-25 12:50:26,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=qda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:26,945:INFO:Checking exceptions
2024-11-25 12:50:26,945:INFO:Importing libraries
2024-11-25 12:50:26,945:INFO:Copying training dataset
2024-11-25 12:50:26,948:INFO:Defining folds
2024-11-25 12:50:26,948:INFO:Declaring metric variables
2024-11-25 12:50:26,951:INFO:Importing untrained model
2024-11-25 12:50:26,955:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:50:26,964:INFO:Starting cross validation
2024-11-25 12:50:26,968:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:27,122:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:27,127:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:27,138:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:27,144:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:27,178:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,190:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,200:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,213:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,233:INFO:Calculating mean and std
2024-11-25 12:50:27,234:INFO:Creating metrics dataframe
2024-11-25 12:50:27,236:INFO:Uploading results into container
2024-11-25 12:50:27,237:INFO:Uploading model into container now
2024-11-25 12:50:27,238:INFO:_master_model_container: 8
2024-11-25 12:50:27,238:INFO:_display_container: 2
2024-11-25 12:50:27,238:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:50:27,238:INFO:create_model() successfully completed......................................
2024-11-25 12:50:27,309:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:27,309:INFO:Creating metrics dataframe
2024-11-25 12:50:27,317:INFO:Initializing Ada Boost Classifier
2024-11-25 12:50:27,318:INFO:Total runtime is 0.1503132104873657 minutes
2024-11-25 12:50:27,321:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:27,321:INFO:Initializing create_model()
2024-11-25 12:50:27,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=ada, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:27,321:INFO:Checking exceptions
2024-11-25 12:50:27,321:INFO:Importing libraries
2024-11-25 12:50:27,322:INFO:Copying training dataset
2024-11-25 12:50:27,328:INFO:Defining folds
2024-11-25 12:50:27,329:INFO:Declaring metric variables
2024-11-25 12:50:27,332:INFO:Importing untrained model
2024-11-25 12:50:27,337:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:50:27,347:INFO:Starting cross validation
2024-11-25 12:50:27,350:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:27,494:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:27,508:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:27,509:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:27,513:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:27,635:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,652:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,655:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,660:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:27,674:INFO:Calculating mean and std
2024-11-25 12:50:27,674:INFO:Creating metrics dataframe
2024-11-25 12:50:27,676:INFO:Uploading results into container
2024-11-25 12:50:27,677:INFO:Uploading model into container now
2024-11-25 12:50:27,678:INFO:_master_model_container: 9
2024-11-25 12:50:27,678:INFO:_display_container: 2
2024-11-25 12:50:27,678:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:50:27,678:INFO:create_model() successfully completed......................................
2024-11-25 12:50:27,752:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:27,752:INFO:Creating metrics dataframe
2024-11-25 12:50:27,759:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:50:27,760:INFO:Total runtime is 0.1576797564824422 minutes
2024-11-25 12:50:27,763:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:27,764:INFO:Initializing create_model()
2024-11-25 12:50:27,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=gbc, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:27,764:INFO:Checking exceptions
2024-11-25 12:50:27,764:INFO:Importing libraries
2024-11-25 12:50:27,765:INFO:Copying training dataset
2024-11-25 12:50:27,769:INFO:Defining folds
2024-11-25 12:50:27,769:INFO:Declaring metric variables
2024-11-25 12:50:27,772:INFO:Importing untrained model
2024-11-25 12:50:27,778:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:50:27,787:INFO:Starting cross validation
2024-11-25 12:50:27,789:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:28,337:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,339:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,372:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,403:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,424:INFO:Calculating mean and std
2024-11-25 12:50:28,424:INFO:Creating metrics dataframe
2024-11-25 12:50:28,426:INFO:Uploading results into container
2024-11-25 12:50:28,427:INFO:Uploading model into container now
2024-11-25 12:50:28,428:INFO:_master_model_container: 10
2024-11-25 12:50:28,428:INFO:_display_container: 2
2024-11-25 12:50:28,428:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:50:28,428:INFO:create_model() successfully completed......................................
2024-11-25 12:50:28,505:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:28,505:INFO:Creating metrics dataframe
2024-11-25 12:50:28,519:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:50:28,520:INFO:Total runtime is 0.1703373074531555 minutes
2024-11-25 12:50:28,523:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:28,523:INFO:Initializing create_model()
2024-11-25 12:50:28,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=lda, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:28,523:INFO:Checking exceptions
2024-11-25 12:50:28,523:INFO:Importing libraries
2024-11-25 12:50:28,524:INFO:Copying training dataset
2024-11-25 12:50:28,527:INFO:Defining folds
2024-11-25 12:50:28,527:INFO:Declaring metric variables
2024-11-25 12:50:28,532:INFO:Importing untrained model
2024-11-25 12:50:28,536:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:50:28,548:INFO:Starting cross validation
2024-11-25 12:50:28,553:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:28,756:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,758:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:28,779:INFO:Calculating mean and std
2024-11-25 12:50:28,779:INFO:Creating metrics dataframe
2024-11-25 12:50:28,781:INFO:Uploading results into container
2024-11-25 12:50:28,782:INFO:Uploading model into container now
2024-11-25 12:50:28,782:INFO:_master_model_container: 11
2024-11-25 12:50:28,782:INFO:_display_container: 2
2024-11-25 12:50:28,782:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:50:28,783:INFO:create_model() successfully completed......................................
2024-11-25 12:50:28,849:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:28,849:INFO:Creating metrics dataframe
2024-11-25 12:50:28,857:INFO:Initializing Extra Trees Classifier
2024-11-25 12:50:28,857:INFO:Total runtime is 0.17596420447031655 minutes
2024-11-25 12:50:28,861:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:28,861:INFO:Initializing create_model()
2024-11-25 12:50:28,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=et, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:28,862:INFO:Checking exceptions
2024-11-25 12:50:28,862:INFO:Importing libraries
2024-11-25 12:50:28,862:INFO:Copying training dataset
2024-11-25 12:50:28,867:INFO:Defining folds
2024-11-25 12:50:28,867:INFO:Declaring metric variables
2024-11-25 12:50:28,871:INFO:Importing untrained model
2024-11-25 12:50:28,877:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:50:28,884:INFO:Starting cross validation
2024-11-25 12:50:28,887:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:29,343:INFO:Calculating mean and std
2024-11-25 12:50:29,344:INFO:Creating metrics dataframe
2024-11-25 12:50:29,346:INFO:Uploading results into container
2024-11-25 12:50:29,346:INFO:Uploading model into container now
2024-11-25 12:50:29,347:INFO:_master_model_container: 12
2024-11-25 12:50:29,347:INFO:_display_container: 2
2024-11-25 12:50:29,348:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:50:29,348:INFO:create_model() successfully completed......................................
2024-11-25 12:50:29,419:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:29,420:INFO:Creating metrics dataframe
2024-11-25 12:50:29,428:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:50:29,428:INFO:Total runtime is 0.18548278013865152 minutes
2024-11-25 12:50:29,431:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:29,432:INFO:Initializing create_model()
2024-11-25 12:50:29,432:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:29,432:INFO:Checking exceptions
2024-11-25 12:50:29,432:INFO:Importing libraries
2024-11-25 12:50:29,432:INFO:Copying training dataset
2024-11-25 12:50:29,438:INFO:Defining folds
2024-11-25 12:50:29,438:INFO:Declaring metric variables
2024-11-25 12:50:29,442:INFO:Importing untrained model
2024-11-25 12:50:29,446:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:50:29,455:INFO:Starting cross validation
2024-11-25 12:50:29,458:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:30,148:INFO:Calculating mean and std
2024-11-25 12:50:30,149:INFO:Creating metrics dataframe
2024-11-25 12:50:30,152:INFO:Uploading results into container
2024-11-25 12:50:30,152:INFO:Uploading model into container now
2024-11-25 12:50:30,153:INFO:_master_model_container: 13
2024-11-25 12:50:30,153:INFO:_display_container: 2
2024-11-25 12:50:30,154:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:50:30,154:INFO:create_model() successfully completed......................................
2024-11-25 12:50:30,250:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:30,250:INFO:Creating metrics dataframe
2024-11-25 12:50:30,269:INFO:Initializing Dummy Classifier
2024-11-25 12:50:30,269:INFO:Total runtime is 0.1994921843210856 minutes
2024-11-25 12:50:30,275:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:30,276:INFO:Initializing create_model()
2024-11-25 12:50:30,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=dummy, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157157250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:30,276:INFO:Checking exceptions
2024-11-25 12:50:30,276:INFO:Importing libraries
2024-11-25 12:50:30,276:INFO:Copying training dataset
2024-11-25 12:50:30,283:INFO:Defining folds
2024-11-25 12:50:30,284:INFO:Declaring metric variables
2024-11-25 12:50:30,288:INFO:Importing untrained model
2024-11-25 12:50:30,295:INFO:Dummy Classifier Imported successfully
2024-11-25 12:50:30,304:INFO:Starting cross validation
2024-11-25 12:50:30,307:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:30,514:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:30,519:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:30,527:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:30,536:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:30,547:INFO:Calculating mean and std
2024-11-25 12:50:30,547:INFO:Creating metrics dataframe
2024-11-25 12:50:30,549:INFO:Uploading results into container
2024-11-25 12:50:30,550:INFO:Uploading model into container now
2024-11-25 12:50:30,550:INFO:_master_model_container: 14
2024-11-25 12:50:30,550:INFO:_display_container: 2
2024-11-25 12:50:30,550:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:50:30,550:INFO:create_model() successfully completed......................................
2024-11-25 12:50:30,622:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:30,623:INFO:Creating metrics dataframe
2024-11-25 12:50:30,634:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:50:30,644:INFO:Initializing create_model()
2024-11-25 12:50:30,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:30,644:INFO:Checking exceptions
2024-11-25 12:50:30,646:INFO:Importing libraries
2024-11-25 12:50:30,646:INFO:Copying training dataset
2024-11-25 12:50:30,650:INFO:Defining folds
2024-11-25 12:50:30,650:INFO:Declaring metric variables
2024-11-25 12:50:30,650:INFO:Importing untrained model
2024-11-25 12:50:30,651:INFO:Declaring custom model
2024-11-25 12:50:30,651:INFO:Ridge Classifier Imported successfully
2024-11-25 12:50:30,653:INFO:Cross validation set to False
2024-11-25 12:50:30,653:INFO:Fitting Model
2024-11-25 12:50:30,772:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:50:30,772:INFO:create_model() successfully completed......................................
2024-11-25 12:50:30,865:INFO:_master_model_container: 14
2024-11-25 12:50:30,865:INFO:_display_container: 2
2024-11-25 12:50:30,865:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:50:30,866:INFO:compare_models() successfully completed......................................
2024-11-25 12:50:30,871:INFO:Initializing compare_models()
2024-11-25 12:50:30,871:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-11-25 12:50:30,871:INFO:Checking exceptions
2024-11-25 12:50:30,878:INFO:Preparing display monitor
2024-11-25 12:50:30,907:INFO:Initializing Logistic Regression
2024-11-25 12:50:30,908:INFO:Total runtime is 1.7102559407552084e-05 minutes
2024-11-25 12:50:30,914:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:30,914:INFO:Initializing create_model()
2024-11-25 12:50:30,914:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:30,914:INFO:Checking exceptions
2024-11-25 12:50:30,915:INFO:Importing libraries
2024-11-25 12:50:30,915:INFO:Copying training dataset
2024-11-25 12:50:30,921:INFO:Defining folds
2024-11-25 12:50:30,921:INFO:Declaring metric variables
2024-11-25 12:50:30,926:INFO:Importing untrained model
2024-11-25 12:50:30,930:INFO:Logistic Regression Imported successfully
2024-11-25 12:50:30,937:INFO:Starting cross validation
2024-11-25 12:50:30,941:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:31,341:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,345:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,356:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,362:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,396:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,403:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,410:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,420:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,541:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,547:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:31,561:INFO:Calculating mean and std
2024-11-25 12:50:31,562:INFO:Creating metrics dataframe
2024-11-25 12:50:31,563:INFO:Uploading results into container
2024-11-25 12:50:31,564:INFO:Uploading model into container now
2024-11-25 12:50:31,564:INFO:_master_model_container: 1
2024-11-25 12:50:31,564:INFO:_display_container: 2
2024-11-25 12:50:31,564:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-25 12:50:31,564:INFO:create_model() successfully completed......................................
2024-11-25 12:50:31,629:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:31,629:INFO:Creating metrics dataframe
2024-11-25 12:50:31,635:INFO:Initializing K Neighbors Classifier
2024-11-25 12:50:31,635:INFO:Total runtime is 0.012135509649912515 minutes
2024-11-25 12:50:31,638:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:31,638:INFO:Initializing create_model()
2024-11-25 12:50:31,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:31,638:INFO:Checking exceptions
2024-11-25 12:50:31,639:INFO:Importing libraries
2024-11-25 12:50:31,639:INFO:Copying training dataset
2024-11-25 12:50:31,643:INFO:Defining folds
2024-11-25 12:50:31,643:INFO:Declaring metric variables
2024-11-25 12:50:31,647:INFO:Importing untrained model
2024-11-25 12:50:31,652:INFO:K Neighbors Classifier Imported successfully
2024-11-25 12:50:31,663:INFO:Starting cross validation
2024-11-25 12:50:31,666:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:32,293:INFO:Calculating mean and std
2024-11-25 12:50:32,294:INFO:Creating metrics dataframe
2024-11-25 12:50:32,296:INFO:Uploading results into container
2024-11-25 12:50:32,297:INFO:Uploading model into container now
2024-11-25 12:50:32,297:INFO:_master_model_container: 2
2024-11-25 12:50:32,297:INFO:_display_container: 2
2024-11-25 12:50:32,297:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-25 12:50:32,298:INFO:create_model() successfully completed......................................
2024-11-25 12:50:32,363:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:32,363:INFO:Creating metrics dataframe
2024-11-25 12:50:32,371:INFO:Initializing Naive Bayes
2024-11-25 12:50:32,371:INFO:Total runtime is 0.02440683046976725 minutes
2024-11-25 12:50:32,375:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:32,375:INFO:Initializing create_model()
2024-11-25 12:50:32,375:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:32,375:INFO:Checking exceptions
2024-11-25 12:50:32,375:INFO:Importing libraries
2024-11-25 12:50:32,375:INFO:Copying training dataset
2024-11-25 12:50:32,381:INFO:Defining folds
2024-11-25 12:50:32,381:INFO:Declaring metric variables
2024-11-25 12:50:32,384:INFO:Importing untrained model
2024-11-25 12:50:32,387:INFO:Naive Bayes Imported successfully
2024-11-25 12:50:32,395:INFO:Starting cross validation
2024-11-25 12:50:32,399:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:32,967:INFO:Calculating mean and std
2024-11-25 12:50:32,968:INFO:Creating metrics dataframe
2024-11-25 12:50:32,970:INFO:Uploading results into container
2024-11-25 12:50:32,970:INFO:Uploading model into container now
2024-11-25 12:50:32,971:INFO:_master_model_container: 3
2024-11-25 12:50:32,971:INFO:_display_container: 2
2024-11-25 12:50:32,971:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-25 12:50:32,971:INFO:create_model() successfully completed......................................
2024-11-25 12:50:33,044:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:33,044:INFO:Creating metrics dataframe
2024-11-25 12:50:33,051:INFO:Initializing Decision Tree Classifier
2024-11-25 12:50:33,051:INFO:Total runtime is 0.03573362827301025 minutes
2024-11-25 12:50:33,055:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:33,055:INFO:Initializing create_model()
2024-11-25 12:50:33,055:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:33,056:INFO:Checking exceptions
2024-11-25 12:50:33,056:INFO:Importing libraries
2024-11-25 12:50:33,056:INFO:Copying training dataset
2024-11-25 12:50:33,062:INFO:Defining folds
2024-11-25 12:50:33,062:INFO:Declaring metric variables
2024-11-25 12:50:33,065:INFO:Importing untrained model
2024-11-25 12:50:33,069:INFO:Decision Tree Classifier Imported successfully
2024-11-25 12:50:33,082:INFO:Starting cross validation
2024-11-25 12:50:33,087:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:33,710:INFO:Calculating mean and std
2024-11-25 12:50:33,710:INFO:Creating metrics dataframe
2024-11-25 12:50:33,713:INFO:Uploading results into container
2024-11-25 12:50:33,714:INFO:Uploading model into container now
2024-11-25 12:50:33,714:INFO:_master_model_container: 4
2024-11-25 12:50:33,714:INFO:_display_container: 2
2024-11-25 12:50:33,714:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-25 12:50:33,715:INFO:create_model() successfully completed......................................
2024-11-25 12:50:33,795:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:33,795:INFO:Creating metrics dataframe
2024-11-25 12:50:33,805:INFO:Initializing SVM - Linear Kernel
2024-11-25 12:50:33,805:INFO:Total runtime is 0.04830458958943684 minutes
2024-11-25 12:50:33,811:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:33,812:INFO:Initializing create_model()
2024-11-25 12:50:33,812:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:33,812:INFO:Checking exceptions
2024-11-25 12:50:33,812:INFO:Importing libraries
2024-11-25 12:50:33,812:INFO:Copying training dataset
2024-11-25 12:50:33,818:INFO:Defining folds
2024-11-25 12:50:33,818:INFO:Declaring metric variables
2024-11-25 12:50:33,822:INFO:Importing untrained model
2024-11-25 12:50:33,828:INFO:SVM - Linear Kernel Imported successfully
2024-11-25 12:50:33,837:INFO:Starting cross validation
2024-11-25 12:50:33,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:34,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,279:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,287:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,291:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,299:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,319:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,331:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,343:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,475:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,487:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,499:INFO:Calculating mean and std
2024-11-25 12:50:34,499:INFO:Creating metrics dataframe
2024-11-25 12:50:34,499:INFO:Uploading results into container
2024-11-25 12:50:34,499:INFO:Uploading model into container now
2024-11-25 12:50:34,503:INFO:_master_model_container: 5
2024-11-25 12:50:34,503:INFO:_display_container: 2
2024-11-25 12:50:34,503:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-25 12:50:34,503:INFO:create_model() successfully completed......................................
2024-11-25 12:50:34,567:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:34,567:INFO:Creating metrics dataframe
2024-11-25 12:50:34,579:INFO:Initializing Ridge Classifier
2024-11-25 12:50:34,579:INFO:Total runtime is 0.06120161215464273 minutes
2024-11-25 12:50:34,583:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:34,583:INFO:Initializing create_model()
2024-11-25 12:50:34,583:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:34,583:INFO:Checking exceptions
2024-11-25 12:50:34,583:INFO:Importing libraries
2024-11-25 12:50:34,583:INFO:Copying training dataset
2024-11-25 12:50:34,587:INFO:Defining folds
2024-11-25 12:50:34,587:INFO:Declaring metric variables
2024-11-25 12:50:34,591:INFO:Importing untrained model
2024-11-25 12:50:34,595:INFO:Ridge Classifier Imported successfully
2024-11-25 12:50:34,599:INFO:Starting cross validation
2024-11-25 12:50:34,603:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:34,973:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,977:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,977:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,977:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,981:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,981:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:34,989:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:35,009:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:35,159:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:35,164:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:35,181:INFO:Calculating mean and std
2024-11-25 12:50:35,181:INFO:Creating metrics dataframe
2024-11-25 12:50:35,181:INFO:Uploading results into container
2024-11-25 12:50:35,185:INFO:Uploading model into container now
2024-11-25 12:50:35,185:INFO:_master_model_container: 6
2024-11-25 12:50:35,185:INFO:_display_container: 2
2024-11-25 12:50:35,185:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:50:35,185:INFO:create_model() successfully completed......................................
2024-11-25 12:50:35,253:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:35,253:INFO:Creating metrics dataframe
2024-11-25 12:50:35,266:INFO:Initializing Random Forest Classifier
2024-11-25 12:50:35,266:INFO:Total runtime is 0.07265727917353311 minutes
2024-11-25 12:50:35,274:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:35,274:INFO:Initializing create_model()
2024-11-25 12:50:35,274:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:35,274:INFO:Checking exceptions
2024-11-25 12:50:35,274:INFO:Importing libraries
2024-11-25 12:50:35,274:INFO:Copying training dataset
2024-11-25 12:50:35,278:INFO:Defining folds
2024-11-25 12:50:35,278:INFO:Declaring metric variables
2024-11-25 12:50:35,282:INFO:Importing untrained model
2024-11-25 12:50:35,286:INFO:Random Forest Classifier Imported successfully
2024-11-25 12:50:35,298:INFO:Starting cross validation
2024-11-25 12:50:35,298:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:36,600:INFO:Calculating mean and std
2024-11-25 12:50:36,600:INFO:Creating metrics dataframe
2024-11-25 12:50:36,600:INFO:Uploading results into container
2024-11-25 12:50:36,600:INFO:Uploading model into container now
2024-11-25 12:50:36,600:INFO:_master_model_container: 7
2024-11-25 12:50:36,600:INFO:_display_container: 2
2024-11-25 12:50:36,604:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-25 12:50:36,604:INFO:create_model() successfully completed......................................
2024-11-25 12:50:36,668:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:36,668:INFO:Creating metrics dataframe
2024-11-25 12:50:36,676:INFO:Initializing Quadratic Discriminant Analysis
2024-11-25 12:50:36,676:INFO:Total runtime is 0.09615809122721353 minutes
2024-11-25 12:50:36,680:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:36,680:INFO:Initializing create_model()
2024-11-25 12:50:36,680:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:36,680:INFO:Checking exceptions
2024-11-25 12:50:36,680:INFO:Importing libraries
2024-11-25 12:50:36,680:INFO:Copying training dataset
2024-11-25 12:50:36,688:INFO:Defining folds
2024-11-25 12:50:36,688:INFO:Declaring metric variables
2024-11-25 12:50:36,692:INFO:Importing untrained model
2024-11-25 12:50:36,696:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-25 12:50:36,704:INFO:Starting cross validation
2024-11-25 12:50:36,708:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:37,014:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,018:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,018:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,018:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,022:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,028:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,032:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,070:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,121:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,121:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,125:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,133:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,143:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,155:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,169:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,266:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,270:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-25 12:50:37,322:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,326:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:37,342:INFO:Calculating mean and std
2024-11-25 12:50:37,342:INFO:Creating metrics dataframe
2024-11-25 12:50:37,346:INFO:Uploading results into container
2024-11-25 12:50:37,346:INFO:Uploading model into container now
2024-11-25 12:50:37,346:INFO:_master_model_container: 8
2024-11-25 12:50:37,346:INFO:_display_container: 2
2024-11-25 12:50:37,346:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-25 12:50:37,346:INFO:create_model() successfully completed......................................
2024-11-25 12:50:37,414:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:37,414:INFO:Creating metrics dataframe
2024-11-25 12:50:37,422:INFO:Initializing Ada Boost Classifier
2024-11-25 12:50:37,422:INFO:Total runtime is 0.108590030670166 minutes
2024-11-25 12:50:37,426:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:37,426:INFO:Initializing create_model()
2024-11-25 12:50:37,426:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:37,426:INFO:Checking exceptions
2024-11-25 12:50:37,426:INFO:Importing libraries
2024-11-25 12:50:37,426:INFO:Copying training dataset
2024-11-25 12:50:37,430:INFO:Defining folds
2024-11-25 12:50:37,430:INFO:Declaring metric variables
2024-11-25 12:50:37,430:INFO:Importing untrained model
2024-11-25 12:50:37,434:INFO:Ada Boost Classifier Imported successfully
2024-11-25 12:50:37,442:INFO:Starting cross validation
2024-11-25 12:50:37,446:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:37,723:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,727:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,743:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,751:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,751:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,763:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,767:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:37,779:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:38,027:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,031:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,059:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,067:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,071:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,079:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,083:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,095:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:38,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-25 12:50:38,322:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,327:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:38,341:INFO:Calculating mean and std
2024-11-25 12:50:38,341:INFO:Creating metrics dataframe
2024-11-25 12:50:38,341:INFO:Uploading results into container
2024-11-25 12:50:38,345:INFO:Uploading model into container now
2024-11-25 12:50:38,345:INFO:_master_model_container: 9
2024-11-25 12:50:38,345:INFO:_display_container: 2
2024-11-25 12:50:38,345:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-25 12:50:38,345:INFO:create_model() successfully completed......................................
2024-11-25 12:50:38,409:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:38,413:INFO:Creating metrics dataframe
2024-11-25 12:50:38,421:INFO:Initializing Gradient Boosting Classifier
2024-11-25 12:50:38,421:INFO:Total runtime is 0.12523537476857502 minutes
2024-11-25 12:50:38,421:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:38,425:INFO:Initializing create_model()
2024-11-25 12:50:38,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:38,425:INFO:Checking exceptions
2024-11-25 12:50:38,425:INFO:Importing libraries
2024-11-25 12:50:38,425:INFO:Copying training dataset
2024-11-25 12:50:38,429:INFO:Defining folds
2024-11-25 12:50:38,429:INFO:Declaring metric variables
2024-11-25 12:50:38,433:INFO:Importing untrained model
2024-11-25 12:50:38,437:INFO:Gradient Boosting Classifier Imported successfully
2024-11-25 12:50:38,445:INFO:Starting cross validation
2024-11-25 12:50:38,449:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:39,537:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,569:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,573:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,589:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,641:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,665:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,669:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:39,681:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,096:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,106:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,122:INFO:Calculating mean and std
2024-11-25 12:50:40,122:INFO:Creating metrics dataframe
2024-11-25 12:50:40,122:INFO:Uploading results into container
2024-11-25 12:50:40,122:INFO:Uploading model into container now
2024-11-25 12:50:40,122:INFO:_master_model_container: 10
2024-11-25 12:50:40,122:INFO:_display_container: 2
2024-11-25 12:50:40,126:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-25 12:50:40,126:INFO:create_model() successfully completed......................................
2024-11-25 12:50:40,190:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:40,190:INFO:Creating metrics dataframe
2024-11-25 12:50:40,198:INFO:Initializing Linear Discriminant Analysis
2024-11-25 12:50:40,198:INFO:Total runtime is 0.15486201047897338 minutes
2024-11-25 12:50:40,202:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:40,202:INFO:Initializing create_model()
2024-11-25 12:50:40,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:40,202:INFO:Checking exceptions
2024-11-25 12:50:40,202:INFO:Importing libraries
2024-11-25 12:50:40,202:INFO:Copying training dataset
2024-11-25 12:50:40,206:INFO:Defining folds
2024-11-25 12:50:40,206:INFO:Declaring metric variables
2024-11-25 12:50:40,210:INFO:Importing untrained model
2024-11-25 12:50:40,214:INFO:Linear Discriminant Analysis Imported successfully
2024-11-25 12:50:40,226:INFO:Starting cross validation
2024-11-25 12:50:40,230:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:40,644:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,656:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,679:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,679:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,695:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,699:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,715:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,895:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,905:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:40,921:INFO:Calculating mean and std
2024-11-25 12:50:40,922:INFO:Creating metrics dataframe
2024-11-25 12:50:40,923:INFO:Uploading results into container
2024-11-25 12:50:40,924:INFO:Uploading model into container now
2024-11-25 12:50:40,924:INFO:_master_model_container: 11
2024-11-25 12:50:40,924:INFO:_display_container: 2
2024-11-25 12:50:40,925:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-25 12:50:40,925:INFO:create_model() successfully completed......................................
2024-11-25 12:50:40,992:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:40,992:INFO:Creating metrics dataframe
2024-11-25 12:50:41,002:INFO:Initializing Extra Trees Classifier
2024-11-25 12:50:41,002:INFO:Total runtime is 0.16825397809346515 minutes
2024-11-25 12:50:41,006:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:41,006:INFO:Initializing create_model()
2024-11-25 12:50:41,006:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:41,006:INFO:Checking exceptions
2024-11-25 12:50:41,006:INFO:Importing libraries
2024-11-25 12:50:41,007:INFO:Copying training dataset
2024-11-25 12:50:41,012:INFO:Defining folds
2024-11-25 12:50:41,013:INFO:Declaring metric variables
2024-11-25 12:50:41,016:INFO:Importing untrained model
2024-11-25 12:50:41,019:INFO:Extra Trees Classifier Imported successfully
2024-11-25 12:50:41,027:INFO:Starting cross validation
2024-11-25 12:50:41,030:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:42,198:INFO:Calculating mean and std
2024-11-25 12:50:42,199:INFO:Creating metrics dataframe
2024-11-25 12:50:42,201:INFO:Uploading results into container
2024-11-25 12:50:42,201:INFO:Uploading model into container now
2024-11-25 12:50:42,201:INFO:_master_model_container: 12
2024-11-25 12:50:42,201:INFO:_display_container: 2
2024-11-25 12:50:42,202:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-25 12:50:42,202:INFO:create_model() successfully completed......................................
2024-11-25 12:50:42,271:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:42,271:INFO:Creating metrics dataframe
2024-11-25 12:50:42,279:INFO:Initializing Light Gradient Boosting Machine
2024-11-25 12:50:42,279:INFO:Total runtime is 0.18953766822814938 minutes
2024-11-25 12:50:42,283:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:42,284:INFO:Initializing create_model()
2024-11-25 12:50:42,284:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:42,284:INFO:Checking exceptions
2024-11-25 12:50:42,284:INFO:Importing libraries
2024-11-25 12:50:42,284:INFO:Copying training dataset
2024-11-25 12:50:42,287:INFO:Defining folds
2024-11-25 12:50:42,287:INFO:Declaring metric variables
2024-11-25 12:50:42,292:INFO:Importing untrained model
2024-11-25 12:50:42,296:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:50:42,303:INFO:Starting cross validation
2024-11-25 12:50:42,308:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:44,283:INFO:Calculating mean and std
2024-11-25 12:50:44,285:INFO:Creating metrics dataframe
2024-11-25 12:50:44,288:INFO:Uploading results into container
2024-11-25 12:50:44,289:INFO:Uploading model into container now
2024-11-25 12:50:44,289:INFO:_master_model_container: 13
2024-11-25 12:50:44,290:INFO:_display_container: 2
2024-11-25 12:50:44,291:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:50:44,291:INFO:create_model() successfully completed......................................
2024-11-25 12:50:44,391:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:44,391:INFO:Creating metrics dataframe
2024-11-25 12:50:44,401:INFO:Initializing Dummy Classifier
2024-11-25 12:50:44,401:INFO:Total runtime is 0.2249081214269002 minutes
2024-11-25 12:50:44,404:INFO:SubProcess create_model() called ==================================
2024-11-25 12:50:44,404:INFO:Initializing create_model()
2024-11-25 12:50:44,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019157351450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:44,405:INFO:Checking exceptions
2024-11-25 12:50:44,405:INFO:Importing libraries
2024-11-25 12:50:44,405:INFO:Copying training dataset
2024-11-25 12:50:44,409:INFO:Defining folds
2024-11-25 12:50:44,409:INFO:Declaring metric variables
2024-11-25 12:50:44,413:INFO:Importing untrained model
2024-11-25 12:50:44,417:INFO:Dummy Classifier Imported successfully
2024-11-25 12:50:44,426:INFO:Starting cross validation
2024-11-25 12:50:44,429:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:44,829:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,835:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,841:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,848:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,854:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,863:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,865:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:44,881:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:45,063:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:45,064:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-25 12:50:45,071:INFO:Calculating mean and std
2024-11-25 12:50:45,072:INFO:Creating metrics dataframe
2024-11-25 12:50:45,074:INFO:Uploading results into container
2024-11-25 12:50:45,074:INFO:Uploading model into container now
2024-11-25 12:50:45,075:INFO:_master_model_container: 14
2024-11-25 12:50:45,075:INFO:_display_container: 2
2024-11-25 12:50:45,075:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-25 12:50:45,075:INFO:create_model() successfully completed......................................
2024-11-25 12:50:45,143:INFO:SubProcess create_model() end ==================================
2024-11-25 12:50:45,143:INFO:Creating metrics dataframe
2024-11-25 12:50:45,155:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2024-11-25 12:50:45,165:INFO:Initializing create_model()
2024-11-25 12:50:45,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001915716B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:45,166:INFO:Checking exceptions
2024-11-25 12:50:45,167:INFO:Importing libraries
2024-11-25 12:50:45,167:INFO:Copying training dataset
2024-11-25 12:50:45,170:INFO:Defining folds
2024-11-25 12:50:45,171:INFO:Declaring metric variables
2024-11-25 12:50:45,171:INFO:Importing untrained model
2024-11-25 12:50:45,171:INFO:Declaring custom model
2024-11-25 12:50:45,172:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-25 12:50:45,175:INFO:Cross validation set to False
2024-11-25 12:50:45,175:INFO:Fitting Model
2024-11-25 12:50:45,287:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-25 12:50:45,288:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000059 seconds.
2024-11-25 12:50:45,288:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-25 12:50:45,288:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-25 12:50:45,288:INFO:[LightGBM] [Info] Total Bins 63
2024-11-25 12:50:45,288:INFO:[LightGBM] [Info] Number of data points in the train set: 260, number of used features: 31
2024-11-25 12:50:45,289:INFO:[LightGBM] [Info] Start training from score -1.535330
2024-11-25 12:50:45,289:INFO:[LightGBM] [Info] Start training from score -1.072045
2024-11-25 12:50:45,289:INFO:[LightGBM] [Info] Start training from score -0.815750
2024-11-25 12:50:45,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-25 12:50:45,392:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:50:45,392:INFO:create_model() successfully completed......................................
2024-11-25 12:50:45,531:INFO:_master_model_container: 14
2024-11-25 12:50:45,531:INFO:_display_container: 2
2024-11-25 12:50:45,531:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-25 12:50:45,531:INFO:compare_models() successfully completed......................................
2024-11-25 12:50:45,536:INFO:Initializing evaluate_model()
2024-11-25 12:50:45,536:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-25 12:50:45,550:INFO:Initializing plot_model()
2024-11-25 12:50:45,550:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-11-25 12:50:45,550:INFO:Checking exceptions
2024-11-25 12:50:45,552:INFO:Preloading libraries
2024-11-25 12:50:45,553:INFO:Copying training dataset
2024-11-25 12:50:45,553:INFO:Plot type: pipeline
2024-11-25 12:50:45,707:INFO:Visual Rendered Successfully
2024-11-25 12:50:45,779:INFO:plot_model() successfully completed......................................
2024-11-25 12:50:45,787:INFO:Initializing plot_model()
2024-11-25 12:50:45,787:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), plot=confusion_matrix, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2024-11-25 12:50:45,787:INFO:Checking exceptions
2024-11-25 12:50:45,791:INFO:Preloading libraries
2024-11-25 12:50:45,791:INFO:Copying training dataset
2024-11-25 12:50:45,791:INFO:Plot type: confusion_matrix
2024-11-25 12:50:46,011:INFO:Fitting Model
2024-11-25 12:50:46,011:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RidgeClassifier was fitted with feature names
  warnings.warn(

2024-11-25 12:50:46,011:INFO:Scoring test/hold-out set
2024-11-25 12:50:46,138:INFO:Visual Rendered Successfully
2024-11-25 12:50:46,210:INFO:plot_model() successfully completed......................................
2024-11-25 12:50:46,219:INFO:Initializing create_model()
2024-11-25 12:50:46,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=ridge, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:50:46,219:INFO:Checking exceptions
2024-11-25 12:50:46,235:INFO:Importing libraries
2024-11-25 12:50:46,235:INFO:Copying training dataset
2024-11-25 12:50:46,243:INFO:Defining folds
2024-11-25 12:50:46,243:INFO:Declaring metric variables
2024-11-25 12:50:46,247:INFO:Importing untrained model
2024-11-25 12:50:46,255:INFO:Ridge Classifier Imported successfully
2024-11-25 12:50:46,263:INFO:Starting cross validation
2024-11-25 12:50:46,267:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:50:46,503:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:46,507:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:46,525:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:46,525:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:50:46,553:INFO:Calculating mean and std
2024-11-25 12:50:46,553:INFO:Creating metrics dataframe
2024-11-25 12:50:46,557:INFO:Finalizing model
2024-11-25 12:50:46,665:INFO:Uploading results into container
2024-11-25 12:50:46,669:INFO:Uploading model into container now
2024-11-25 12:50:46,677:INFO:_master_model_container: 15
2024-11-25 12:50:46,677:INFO:_display_container: 3
2024-11-25 12:50:46,677:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:50:46,677:INFO:create_model() successfully completed......................................
2024-11-25 12:50:46,745:INFO:Initializing tune_model()
2024-11-25 12:50:46,745:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, n_iter=200, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-11-25 12:50:46,745:INFO:Checking exceptions
2024-11-25 12:50:46,761:INFO:Copying training dataset
2024-11-25 12:50:46,765:INFO:Checking base model
2024-11-25 12:50:46,765:INFO:Base model : Ridge Classifier
2024-11-25 12:50:46,769:INFO:Declaring metric variables
2024-11-25 12:50:46,773:INFO:Defining Hyperparameters
2024-11-25 12:50:46,849:INFO:Tuning with n_jobs=-1
2024-11-25 12:50:46,849:INFO:Initializing RandomizedSearchCV
2024-11-25 12:51:30,742:INFO:best_params: {'actual_estimator__fit_intercept': True, 'actual_estimator__alpha': 3.2}
2024-11-25 12:51:30,743:INFO:Hyperparameter search completed
2024-11-25 12:51:30,744:INFO:SubProcess create_model() called ==================================
2024-11-25 12:51:30,745:INFO:Initializing create_model()
2024-11-25 12:51:30,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000191571F0B50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'fit_intercept': True, 'alpha': 3.2})
2024-11-25 12:51:30,745:INFO:Checking exceptions
2024-11-25 12:51:30,746:INFO:Importing libraries
2024-11-25 12:51:30,746:INFO:Copying training dataset
2024-11-25 12:51:30,753:INFO:Defining folds
2024-11-25 12:51:30,754:INFO:Declaring metric variables
2024-11-25 12:51:30,759:INFO:Importing untrained model
2024-11-25 12:51:30,760:INFO:Declaring custom model
2024-11-25 12:51:30,768:INFO:Ridge Classifier Imported successfully
2024-11-25 12:51:30,780:INFO:Starting cross validation
2024-11-25 12:51:30,786:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:51:31,164:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,194:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,194:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,204:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,230:INFO:Calculating mean and std
2024-11-25 12:51:31,230:INFO:Creating metrics dataframe
2024-11-25 12:51:31,238:INFO:Finalizing model
2024-11-25 12:51:31,398:INFO:Uploading results into container
2024-11-25 12:51:31,398:INFO:Uploading model into container now
2024-11-25 12:51:31,398:INFO:_master_model_container: 16
2024-11-25 12:51:31,398:INFO:_display_container: 4
2024-11-25 12:51:31,398:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:51:31,398:INFO:create_model() successfully completed......................................
2024-11-25 12:51:31,490:INFO:SubProcess create_model() end ==================================
2024-11-25 12:51:31,490:INFO:choose_better activated
2024-11-25 12:51:31,494:INFO:SubProcess create_model() called ==================================
2024-11-25 12:51:31,494:INFO:Initializing create_model()
2024-11-25 12:51:31,494:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=StratifiedKFold(n_splits=4, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:51:31,494:INFO:Checking exceptions
2024-11-25 12:51:31,498:INFO:Importing libraries
2024-11-25 12:51:31,498:INFO:Copying training dataset
2024-11-25 12:51:31,502:INFO:Defining folds
2024-11-25 12:51:31,502:INFO:Declaring metric variables
2024-11-25 12:51:31,502:INFO:Importing untrained model
2024-11-25 12:51:31,502:INFO:Declaring custom model
2024-11-25 12:51:31,502:INFO:Ridge Classifier Imported successfully
2024-11-25 12:51:31,502:INFO:Starting cross validation
2024-11-25 12:51:31,506:INFO:Cross validating with StratifiedKFold(n_splits=4, random_state=None, shuffle=False), n_jobs=-1
2024-11-25 12:51:31,761:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,769:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,779:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,791:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-25 12:51:31,806:INFO:Calculating mean and std
2024-11-25 12:51:31,806:INFO:Creating metrics dataframe
2024-11-25 12:51:31,806:INFO:Finalizing model
2024-11-25 12:51:31,919:INFO:Uploading results into container
2024-11-25 12:51:31,923:INFO:Uploading model into container now
2024-11-25 12:51:31,923:INFO:_master_model_container: 17
2024-11-25 12:51:31,923:INFO:_display_container: 5
2024-11-25 12:51:31,923:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:51:31,923:INFO:create_model() successfully completed......................................
2024-11-25 12:51:31,995:INFO:SubProcess create_model() end ==================================
2024-11-25 12:51:31,995:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6577
2024-11-25 12:51:31,995:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) result for Accuracy is 0.6654
2024-11-25 12:51:31,995:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001) is best model
2024-11-25 12:51:31,995:INFO:choose_better completed
2024-11-25 12:51:32,003:INFO:_master_model_container: 17
2024-11-25 12:51:32,003:INFO:_display_container: 4
2024-11-25 12:51:32,007:INFO:RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:51:32,007:INFO:tune_model() successfully completed......................................
2024-11-25 12:51:32,091:INFO:Initializing finalize_model()
2024-11-25 12:51:32,091:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-25 12:51:32,091:INFO:Finalizing RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-25 12:51:32,095:INFO:Initializing create_model()
2024-11-25 12:51:32,095:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-25 12:51:32,095:INFO:Checking exceptions
2024-11-25 12:51:32,095:INFO:Importing libraries
2024-11-25 12:51:32,095:INFO:Copying training dataset
2024-11-25 12:51:32,095:INFO:Defining folds
2024-11-25 12:51:32,095:INFO:Declaring metric variables
2024-11-25 12:51:32,095:INFO:Importing untrained model
2024-11-25 12:51:32,095:INFO:Declaring custom model
2024-11-25 12:51:32,095:INFO:Ridge Classifier Imported successfully
2024-11-25 12:51:32,099:INFO:Cross validation set to False
2024-11-25 12:51:32,099:INFO:Fitting Model
2024-11-25 12:51:32,211:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:51:32,211:INFO:create_model() successfully completed......................................
2024-11-25 12:51:32,283:INFO:_master_model_container: 17
2024-11-25 12:51:32,283:INFO:_display_container: 4
2024-11-25 12:51:32,287:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:51:32,287:INFO:finalize_model() successfully completed......................................
2024-11-25 12:51:32,367:INFO:Initializing predict_model()
2024-11-25 12:51:32,367:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019156C97D90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 RidgeClassifier(alpha=3.2, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000191570CB560>)
2024-11-25 12:51:32,367:INFO:Checking exceptions
2024-11-25 12:51:32,367:INFO:Preloading libraries
2024-11-25 12:51:32,371:INFO:Set up data.
2024-11-25 12:51:32,375:INFO:Set up index.
2024-11-25 12:51:32,423:WARNING:D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 706, in _multiclass_roc_auc_score
    if not np.allclose(1, y_score.sum(axis=1)):
                          ^^^^^^^^^^^^^^^^^^^
  File "D:\coding\Predict_ECE_from_SA45\backend\.venv\Lib\site-packages\numpy\core\_methods.py", line 49, in _sum
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy.exceptions.AxisError: axis 1 is out of bounds for array of dimension 1

  warnings.warn(traceback.format_exc())

2024-11-25 12:51:32,542:INFO:Initializing save_model()
2024-11-25 12:51:32,542:INFO:save_model(model=RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001), model_name=modelo-SA45-ECE_Sexo_SA45-total, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\KURTZ~1\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    in...
                                                                    'Depresin',
                                                                    'Ansiedad',
                                                                    'Hostilidad',
                                                                    'Ans. '
                                                                    'Fbica',
                                                                    'Ideacin '
                                                                    'Paran.',
                                                                    'Psicoticismo',
                                                                    'SA45'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-25 12:51:32,542:INFO:Adding model into prep_pipe
2024-11-25 12:51:32,562:INFO:modelo-SA45-ECE_Sexo_SA45-total.pkl saved in current working directory
2024-11-25 12:51:32,570:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None, include=['Sexo'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Somatizacin', 'Obsesin/com',
                                             'Sen....
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,
                                 fit_intercept=True, max_iter=None,
                                 positive=False, random_state=123,
                                 solver='auto', tol=0.0001))],
         verbose=False)
2024-11-25 12:51:32,570:INFO:save_model() successfully completed......................................
